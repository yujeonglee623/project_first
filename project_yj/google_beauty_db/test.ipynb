{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b79d78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ¥ YouTube ì˜ìƒ ë° ëŒ“ê¸€ ìˆ˜ì§‘ ì‹œì‘\n",
      "============================================================\n",
      "ğŸ” 'K-beauty' í‚¤ì›Œë“œë¡œ ì˜ìƒ ê²€ìƒ‰ ì¤‘...\n",
      "âœ… 10ê°œì˜ ì˜ìƒì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n",
      "ğŸ“Š 10ê°œ ì˜ìƒì˜ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘...\n",
      "âœ… 10ê°œ ì˜ìƒ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ\n",
      "\n",
      "ğŸ’¬ 10ê°œ ì˜ìƒì˜ ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘...\n",
      "  [1/10] ì˜ìƒ uSCUNfwMOJs ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘...\n",
      "    âœ… 126ê°œ ëŒ“ê¸€ ìˆ˜ì§‘\n",
      "  [2/10] ì˜ìƒ gGeRJWDtgY0 ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘...\n",
      "    âœ… 28ê°œ ëŒ“ê¸€ ìˆ˜ì§‘\n",
      "  [3/10] ì˜ìƒ iO8oW6CRiRk ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘...\n",
      "    âœ… 82ê°œ ëŒ“ê¸€ ìˆ˜ì§‘\n",
      "  [4/10] ì˜ìƒ i1o1XILTSxU ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘...\n",
      "    âœ… 73ê°œ ëŒ“ê¸€ ìˆ˜ì§‘\n",
      "  [5/10] ì˜ìƒ 48QReWnEPuc ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘...\n",
      "    âœ… 122ê°œ ëŒ“ê¸€ ìˆ˜ì§‘\n",
      "  [6/10] ì˜ìƒ wsB0opxgIFI ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘...\n",
      "    âœ… 87ê°œ ëŒ“ê¸€ ìˆ˜ì§‘\n",
      "  [7/10] ì˜ìƒ vUblA8O08oQ ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘...\n",
      "    âœ… 55ê°œ ëŒ“ê¸€ ìˆ˜ì§‘\n",
      "  [8/10] ì˜ìƒ Bi_8gjx0QB0 ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘...\n",
      "    âœ… 95ê°œ ëŒ“ê¸€ ìˆ˜ì§‘\n",
      "  [9/10] ì˜ìƒ 3J1j3JmSAgs ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘...\n",
      "    âœ… 49ê°œ ëŒ“ê¸€ ìˆ˜ì§‘\n",
      "  [10/10] ì˜ìƒ UyBMKPlEw0U ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘...\n",
      "    âœ… 72ê°œ ëŒ“ê¸€ ìˆ˜ì§‘\n",
      "\n",
      "âœ… ì´ 789ê°œì˜ ëŒ“ê¸€ ìˆ˜ì§‘ ì™„ë£Œ\n",
      "\n",
      "============================================================\n",
      "ğŸ’¾ ë°ì´í„° ì €ì¥ ì¤‘...\n",
      "============================================================\n",
      "ğŸ’¾ ë°ì´í„° ì €ì¥ ì™„ë£Œ: youtube_videos_K-beauty_20251103_113247.csv\n",
      "ğŸ’¾ ë°ì´í„° ì €ì¥ ì™„ë£Œ: youtube_comments_K-beauty_20251103_113247.csv\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½\n",
      "============================================================\n",
      "âœ… ìˆ˜ì§‘ëœ ì˜ìƒ: 10ê°œ\n",
      "âœ… ìˆ˜ì§‘ëœ ëŒ“ê¸€: 789ê°œ\n",
      "âœ… í‰ê·  ì¡°íšŒìˆ˜: 3,565,390\n",
      "âœ… í‰ê·  ì¢‹ì•„ìš”: 167,167\n",
      "âœ… ì´ ì¡°íšŒìˆ˜: 35,653,897\n",
      "\n",
      "============================================================\n",
      "ğŸ‰ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!\n",
      "============================================================\n",
      "\n",
      "ğŸ“‹ ì˜ìƒ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:\n",
      "                                               title     channel  view_count  \\\n",
      "0  GAME CHANGING Korean skincare products?!!ğŸ˜±ğŸ¤© #k...  Aylen Park    10101292   \n",
      "1  Products k-beauty insiders are obsessed with r...  Sarah Chey      270526   \n",
      "2  pink kbeauty lippies ğŸ€ #shorts #koreanbeauty #...       Dahia     6712066   \n",
      "3                        Ancient vs Modern Kbeauty âœ¨  krystallee      621998   \n",
      "4  Viral K-Beauty Products That are Actually Wort...  Skin Cupid     2876653   \n",
      "\n",
      "   comment_count  \n",
      "0           1207  \n",
      "1             28  \n",
      "2            462  \n",
      "3             96  \n",
      "4            699  \n",
      "\n",
      "ğŸ’¬ ëŒ“ê¸€ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:\n",
      "                                         video_title              author  \\\n",
      "0  GAME CHANGING Korean skincare products?!!ğŸ˜±ğŸ¤© #k...          @AylenPark   \n",
      "1  GAME CHANGING Korean skincare products?!!ğŸ˜±ğŸ¤© #k...           @Kayyaeyy   \n",
      "2  GAME CHANGING Korean skincare products?!!ğŸ˜±ğŸ¤© #k...          @AylenPark   \n",
      "3  GAME CHANGING Korean skincare products?!!ğŸ˜±ğŸ¤© #k...  @kuenholmstrom4525   \n",
      "4  GAME CHANGING Korean skincare products?!!ğŸ˜±ğŸ¤© #k...     @paulaarias5302   \n",
      "\n",
      "                                                text  like_count  \n",
      "0  All the products are linked in my Amazon Store...        1649  \n",
      "1                                                Hiâ¤           6  \n",
      "2  @lakid9749when you click on my channel it take...          43  \n",
      "3  @lakid9749I know AI voice is so annoying and t...          24  \n",
      "4                       @AylenParkI love your videos           2  \n"
     ]
    }
   ],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# .env íŒŒì¼ ë¡œë“œ\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"YouTube API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=api_key)\n",
    "\n",
    "\n",
    "def search_videos(keyword, max_results=10, order=\"relevance\"):\n",
    "    \"\"\"\n",
    "    í‚¤ì›Œë“œë¡œ ìœ íŠœë¸Œ ì˜ìƒ ê²€ìƒ‰\n",
    "    \n",
    "    Args:\n",
    "        keyword (str): ê²€ìƒ‰ í‚¤ì›Œë“œ\n",
    "        max_results (int): ê²€ìƒ‰í•  ì˜ìƒ ê°œìˆ˜ (ìµœëŒ€ 50)\n",
    "        order (str): ì •ë ¬ ë°©ì‹ - 'relevance'(ê´€ë ¨ì„±), 'date'(ìµœì‹ ìˆœ), 'viewCount'(ì¡°íšŒìˆ˜)\n",
    "    \n",
    "    Returns:\n",
    "        list: ì˜ìƒ ì •ë³´ ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ” '{keyword}' í‚¤ì›Œë“œë¡œ ì˜ìƒ ê²€ìƒ‰ ì¤‘...\")\n",
    "    \n",
    "    try:\n",
    "        search_response = youtube.search().list(\n",
    "            q=keyword,\n",
    "            part=\"snippet\",\n",
    "            maxResults=min(max_results, 50),\n",
    "            type=\"video\",\n",
    "            order=order,\n",
    "            regionCode=\"KR\"  # í•œêµ­ ì§€ì—­ ì„¤ì •\n",
    "        ).execute()\n",
    "        \n",
    "        video_ids = [item[\"id\"][\"videoId\"] for item in search_response[\"items\"]]\n",
    "        print(f\"âœ… {len(video_ids)}ê°œì˜ ì˜ìƒì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        return video_ids\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def get_video_details(video_ids):\n",
    "    \"\"\"\n",
    "    ì˜ìƒì˜ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ì¡°íšŒìˆ˜, ì¢‹ì•„ìš”, ëŒ“ê¸€ìˆ˜ ë“±)\n",
    "    \n",
    "    Args:\n",
    "        video_ids (list): ì˜ìƒ ID ë¦¬ìŠ¤íŠ¸\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: ì˜ìƒ ì •ë³´ ë°ì´í„°í”„ë ˆì„\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ“Š {len(video_ids)}ê°œ ì˜ìƒì˜ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘...\")\n",
    "    \n",
    "    videos_data = []\n",
    "    \n",
    "    # YouTube APIëŠ” í•œ ë²ˆì— ìµœëŒ€ 50ê°œê¹Œì§€ë§Œ ì¡°íšŒ ê°€ëŠ¥\n",
    "    for i in range(0, len(video_ids), 50):\n",
    "        batch_ids = video_ids[i:i+50]\n",
    "        \n",
    "        try:\n",
    "            video_response = youtube.videos().list(\n",
    "                part=\"snippet,statistics,contentDetails\",\n",
    "                id=\",\".join(batch_ids)\n",
    "            ).execute()\n",
    "            \n",
    "            for item in video_response[\"items\"]:\n",
    "                video_info = {\n",
    "                    \"video_id\": item[\"id\"],\n",
    "                    \"title\": item[\"snippet\"][\"title\"],\n",
    "                    \"channel\": item[\"snippet\"][\"channelTitle\"],\n",
    "                    \"published_at\": item[\"snippet\"][\"publishedAt\"],\n",
    "                    \"description\": item[\"snippet\"][\"description\"],\n",
    "                    \"view_count\": int(item[\"statistics\"].get(\"viewCount\", 0)),\n",
    "                    \"like_count\": int(item[\"statistics\"].get(\"likeCount\", 0)),\n",
    "                    \"comment_count\": int(item[\"statistics\"].get(\"commentCount\", 0)),\n",
    "                    \"duration\": item[\"contentDetails\"][\"duration\"],\n",
    "                    \"tags\": \", \".join(item[\"snippet\"].get(\"tags\", [])),\n",
    "                    \"url\": f\"https://www.youtube.com/watch?v={item['id']}\"\n",
    "                }\n",
    "                videos_data.append(video_info)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ì˜ìƒ ì •ë³´ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}\")\n",
    "            continue\n",
    "    \n",
    "    df = pd.DataFrame(videos_data)\n",
    "    print(f\"âœ… {len(df)}ê°œ ì˜ìƒ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_video_comments(video_id, max_comments=100):\n",
    "    \"\"\"\n",
    "    íŠ¹ì • ì˜ìƒì˜ ëŒ“ê¸€ ê°€ì ¸ì˜¤ê¸°\n",
    "    \n",
    "    Args:\n",
    "        video_id (str): ì˜ìƒ ID\n",
    "        max_comments (int): ê°€ì ¸ì˜¬ ìµœëŒ€ ëŒ“ê¸€ ìˆ˜\n",
    "    \n",
    "    Returns:\n",
    "        list: ëŒ“ê¸€ ì •ë³´ ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "    \n",
    "    try:\n",
    "        while len(comments) < max_comments:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet,replies\",\n",
    "                videoId=video_id,\n",
    "                maxResults=min(100, max_comments - len(comments)),\n",
    "                pageToken=next_page_token,\n",
    "                textFormat=\"plainText\",\n",
    "                order=\"relevance\"  # 'time'(ìµœì‹ ìˆœ) ë˜ëŠ” 'relevance'(ê´€ë ¨ì„±ìˆœ)\n",
    "            )\n",
    "            response = request.execute()\n",
    "            \n",
    "            for item in response[\"items\"]:\n",
    "                top_comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"]\n",
    "                \n",
    "                comment_info = {\n",
    "                    \"comment_id\": item[\"snippet\"][\"topLevelComment\"][\"id\"],\n",
    "                    \"video_id\": video_id,\n",
    "                    \"author\": top_comment[\"authorDisplayName\"],\n",
    "                    \"text\": top_comment[\"textDisplay\"],\n",
    "                    \"like_count\": top_comment[\"likeCount\"],\n",
    "                    \"published_at\": top_comment[\"publishedAt\"],\n",
    "                    \"reply_count\": item[\"snippet\"][\"totalReplyCount\"]\n",
    "                }\n",
    "                comments.append(comment_info)\n",
    "                \n",
    "                # ëŒ€ëŒ“ê¸€ë„ ìˆ˜ì§‘ (ìˆëŠ” ê²½ìš°)\n",
    "                if \"replies\" in item:\n",
    "                    for reply in item[\"replies\"][\"comments\"]:\n",
    "                        reply_snippet = reply[\"snippet\"]\n",
    "                        reply_info = {\n",
    "                            \"comment_id\": reply[\"id\"],\n",
    "                            \"video_id\": video_id,\n",
    "                            \"author\": reply_snippet[\"authorDisplayName\"],\n",
    "                            \"text\": reply_snippet[\"textDisplay\"],\n",
    "                            \"like_count\": reply_snippet[\"likeCount\"],\n",
    "                            \"published_at\": reply_snippet[\"publishedAt\"],\n",
    "                            \"reply_count\": 0,\n",
    "                            \"is_reply\": True,\n",
    "                            \"parent_id\": comment_info[\"comment_id\"]\n",
    "                        }\n",
    "                        comments.append(reply_info)\n",
    "            \n",
    "            next_page_token = response.get(\"nextPageToken\")\n",
    "            \n",
    "            if not next_page_token:\n",
    "                break\n",
    "            \n",
    "            time.sleep(0.5)  # API í˜¸ì¶œ ì œí•œ ë°©ì§€\n",
    "    \n",
    "    except Exception as e:\n",
    "        if \"commentsDisabled\" in str(e):\n",
    "            print(f\"  âš ï¸  ëŒ“ê¸€ì´ ë¹„í™œì„±í™”ëœ ì˜ìƒì…ë‹ˆë‹¤.\")\n",
    "        else:\n",
    "            print(f\"  âŒ ëŒ“ê¸€ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}\")\n",
    "    \n",
    "    return comments\n",
    "\n",
    "\n",
    "def collect_all_comments(video_ids, videos_df, max_comments_per_video=100):\n",
    "    \"\"\"\n",
    "    ì—¬ëŸ¬ ì˜ìƒì˜ ëŒ“ê¸€ì„ ì¼ê´„ ìˆ˜ì§‘ (ì˜ìƒ ì œëª© í¬í•¨)\n",
    "    \n",
    "    Args:\n",
    "        video_ids (list): ì˜ìƒ ID ë¦¬ìŠ¤íŠ¸\n",
    "        videos_df (pd.DataFrame): ì˜ìƒ ì •ë³´ ë°ì´í„°í”„ë ˆì„\n",
    "        max_comments_per_video (int): ì˜ìƒë‹¹ ìµœëŒ€ ëŒ“ê¸€ ìˆ˜\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: ëŒ“ê¸€ ë°ì´í„°í”„ë ˆì„ (ì˜ìƒ ì œëª© í¬í•¨)\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ’¬ {len(video_ids)}ê°œ ì˜ìƒì˜ ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘...\")\n",
    "    \n",
    "    all_comments = []\n",
    "    \n",
    "    # ì˜ìƒ IDë¥¼ í‚¤ë¡œ, ì œëª©ê³¼ ì±„ë„ëª…ì„ ê°’ìœ¼ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ ìƒì„±\n",
    "    video_info_dict = {}\n",
    "    for _, row in videos_df.iterrows():\n",
    "        video_info_dict[row['video_id']] = {\n",
    "            'title': row['title'],\n",
    "            'channel': row['channel'],\n",
    "            'url': row['url']\n",
    "        }\n",
    "    \n",
    "    for idx, video_id in enumerate(video_ids, 1):\n",
    "        print(f\"  [{idx}/{len(video_ids)}] ì˜ìƒ {video_id} ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘...\")\n",
    "        \n",
    "        comments = get_video_comments(video_id, max_comments_per_video)\n",
    "        \n",
    "        # ê° ëŒ“ê¸€ì— ì˜ìƒ ì œëª©ê³¼ ì±„ë„ëª… ì¶”ê°€\n",
    "        for comment in comments:\n",
    "            if video_id in video_info_dict:\n",
    "                comment['video_title'] = video_info_dict[video_id]['title']\n",
    "                comment['video_channel'] = video_info_dict[video_id]['channel']\n",
    "                comment['video_url'] = video_info_dict[video_id]['url']\n",
    "        \n",
    "        all_comments.extend(comments)\n",
    "        \n",
    "        print(f\"    âœ… {len(comments)}ê°œ ëŒ“ê¸€ ìˆ˜ì§‘\")\n",
    "        time.sleep(1)  # API í˜¸ì¶œ ì œí•œ ë°©ì§€\n",
    "    \n",
    "    df = pd.DataFrame(all_comments)\n",
    "    \n",
    "    # ì»¬ëŸ¼ ìˆœì„œ ì¬ì •ë ¬ (ì˜ìƒ ì •ë³´ë¥¼ ì•ìª½ì— ë°°ì¹˜)\n",
    "    if not df.empty:\n",
    "        cols = ['video_id', 'video_title', 'video_channel', 'video_url', \n",
    "                'comment_id', 'author', 'text', 'like_count', \n",
    "                'published_at', 'reply_count']\n",
    "        \n",
    "        # ëŒ€ëŒ“ê¸€ ê´€ë ¨ ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ì¶”ê°€\n",
    "        if 'is_reply' in df.columns:\n",
    "            cols.extend(['is_reply', 'parent_id'])\n",
    "        \n",
    "        df = df[cols]\n",
    "    \n",
    "    print(f\"\\nâœ… ì´ {len(df)}ê°œì˜ ëŒ“ê¸€ ìˆ˜ì§‘ ì™„ë£Œ\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def save_to_csv(df, filename):\n",
    "    \"\"\"ë°ì´í„°í”„ë ˆì„ì„ CSV íŒŒì¼ë¡œ ì €ì¥\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filepath = f\"{filename}_{timestamp}.csv\"\n",
    "    df.to_csv(filepath, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"ğŸ’¾ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# ë©”ì¸ ì‹¤í–‰ ì½”ë“œ\n",
    "# ========================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ì„¤ì •\n",
    "    KEYWORD = \"K-beauty\"  # ê²€ìƒ‰ í‚¤ì›Œë“œ\n",
    "    MAX_VIDEOS = 10  # ê²€ìƒ‰í•  ì˜ìƒ ê°œìˆ˜\n",
    "    MAX_COMMENTS_PER_VIDEO = 50  # ì˜ìƒë‹¹ ëŒ“ê¸€ ìˆ˜\n",
    "    ORDER = \"relevance\"  # 'relevance', 'date', 'viewCount'\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"ğŸ¥ YouTube ì˜ìƒ ë° ëŒ“ê¸€ ìˆ˜ì§‘ ì‹œì‘\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. ì˜ìƒ ê²€ìƒ‰\n",
    "    video_ids = search_videos(KEYWORD, max_results=MAX_VIDEOS, order=ORDER)\n",
    "    \n",
    "    if not video_ids:\n",
    "        print(\"âŒ ê²€ìƒ‰ëœ ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        exit()\n",
    "    \n",
    "    # 2. ì˜ìƒ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘\n",
    "    videos_df = get_video_details(video_ids)\n",
    "    \n",
    "    # 3. ëŒ“ê¸€ ìˆ˜ì§‘ (ì˜ìƒ ì •ë³´ ì „ë‹¬)\n",
    "    comments_df = collect_all_comments(video_ids, videos_df, max_comments_per_video=MAX_COMMENTS_PER_VIDEO)\n",
    "    \n",
    "    # 4. ë°ì´í„° ì €ì¥\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ğŸ’¾ ë°ì´í„° ì €ì¥ ì¤‘...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    video_file = save_to_csv(videos_df, f\"youtube_videos_{KEYWORD}\")\n",
    "    comment_file = save_to_csv(comments_df, f\"youtube_comments_{KEYWORD}\")\n",
    "    \n",
    "    # 5. ê¸°ë³¸ í†µê³„ ì¶œë ¥\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"âœ… ìˆ˜ì§‘ëœ ì˜ìƒ: {len(videos_df)}ê°œ\")\n",
    "    print(f\"âœ… ìˆ˜ì§‘ëœ ëŒ“ê¸€: {len(comments_df)}ê°œ\")\n",
    "    print(f\"âœ… í‰ê·  ì¡°íšŒìˆ˜: {videos_df['view_count'].mean():,.0f}\")\n",
    "    print(f\"âœ… í‰ê·  ì¢‹ì•„ìš”: {videos_df['like_count'].mean():,.0f}\")\n",
    "    print(f\"âœ… ì´ ì¡°íšŒìˆ˜: {videos_df['view_count'].sum():,.0f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ğŸ‰ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 6. ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°\n",
    "    print(\"\\nğŸ“‹ ì˜ìƒ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:\")\n",
    "    print(videos_df[['title', 'channel', 'view_count', 'comment_count']].head())\n",
    "    \n",
    "    print(\"\\nğŸ’¬ ëŒ“ê¸€ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:\")\n",
    "    if not comments_df.empty:\n",
    "        print(comments_df[['video_title', 'author', 'text', 'like_count']].head())\n",
    "    else:\n",
    "        print(\"ìˆ˜ì§‘ëœ ëŒ“ê¸€ì´ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04fd26a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¡œë“œ (ì²« ì‹¤í–‰ ì‹œ ìë™ ë‹¤ìš´ë¡œë“œ)\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "454c59c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wie ich er bitten?\n"
     ]
    }
   ],
   "source": [
    "input_text = \"translate English to German: How old are you?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# ì¶œë ¥ ìƒì„±\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9700b37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wie ich er bitten?\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# ëª¨ë¸ì„ GPUì— ë¡œë“œ\n",
    "model = T5ForConditionalGeneration.from_pretrained(\n",
    "    \"google/flan-t5-small\",\n",
    "    device_map=\"auto\"  # GPUê°€ ìˆìœ¼ë©´ ìë™ìœ¼ë¡œ ë°°ì¹˜\n",
    ")\n",
    "\n",
    "input_text = \"translate English to German: How old are you?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5088cb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cu118\n",
      "True\n",
      "11.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)  # ì˜ˆ: 2.7.0\n",
    "print(torch.cuda.is_available())  # Trueê°€ ë‚˜ì™€ì•¼ í•¨\n",
    "print(torch.version.cuda)  # ì„¤ì¹˜ëœ CUDA ë²„ì „ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e84c65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
