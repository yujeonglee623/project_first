import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from datetime import datetime
import re
from wordcloud import WordCloud
import io
from googleapiclient.discovery import build
from dotenv import load_dotenv
import os
import time
from transformers import AutoTokenizer, AutoModelWithLMHead
import torch

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="YouTube ëŒ“ê¸€ ë¶„ì„ ëŒ€ì‹œë³´ë“œ",
    page_icon="ğŸ¥",
    layout="wide"
)

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'Malgun Gothic'  # Windows
# plt.rcParams['font.family'] = 'AppleGothic'  # Mac
plt.rcParams['axes.unicode_minus'] = False


class YouTubeCommentAnalyzer:
    """YouTube ëŒ“ê¸€ ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self, comments_df, videos_df=None):
        self.comments_df = comments_df.copy()
        self.videos_df = videos_df.copy() if videos_df is not None else None
        
        # ë‚ ì§œ ì»¬ëŸ¼ ë³€í™˜
        if 'published_at' in self.comments_df.columns:
            self.comments_df['published_at'] = pd.to_datetime(self.comments_df['published_at'])
    
    
    def preprocess_text(self, text):
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        if pd.isna(text):
            return ""
        
        text = str(text).lower()
        text = re.sub(r'http\S+|www\S+', '', text)
        text = re.sub(r'[^ê°€-í£a-z0-9\s]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    
    def extract_keywords(self, min_length=2, top_n=50):
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        all_text = ' '.join(self.comments_df['text'].apply(self.preprocess_text))
        words = all_text.split()
        words = [w for w in words if len(w) >= min_length]
        
        stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
                    'of', 'is', 'are', 'was', 'were', 'been', 'be', 'have', 'has', 'had',
                    'ê·¸', 'ì´', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë“¤', 'ë°', 'ë˜í•œ', 'í•˜ë‹¤', 'ìˆë‹¤', 'ë˜ë‹¤',
                    'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ê·¸ëŸ°', 'ì´ëŸ°', 'ì €ëŸ°'}
        
        words = [w for w in words if w not in stopwords]
        word_freq = Counter(words)
        
        return word_freq.most_common(top_n)
    
    
    def wordcloud(self, width=1200, height=800):
        """ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±"""
        all_text = ' '.join(self.comments_df['text'].apply(self.preprocess_text))
        
        wordcloud = WordCloud(
            font_path='malgun.ttf',
            width=width,
            height=height,
            background_color='white',
            max_words=100,
            relative_scaling=0.3,
            colormap='viridis'
        ).generate(all_text)
        
        fig, ax = plt.subplots(figsize=(15, 10))
        ax.imshow(wordcloud, interpolation='bilinear')
        ax.axis('off')
        ax.set_title('ëŒ“ê¸€ ì›Œë“œí´ë¼ìš°ë“œ', fontsize=20, pad=20)
        plt.tight_layout()
        
        return fig
    
    
    def keyword_frequency(self, top_n=20):
        """í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„"""
        keywords = self.extract_keywords(top_n=top_n)
        words, counts = zip(*keywords)
        
        fig, ax = plt.subplots(figsize=(12, 8))
        ax.barh(range(len(words)), counts, color='skyblue')
        ax.set_yticks(range(len(words)))
        ax.set_yticklabels(words)
        ax.set_xlabel('ë¹ˆë„', fontsize=12)
        ax.set_title(f'ìƒìœ„ {top_n}ê°œ í‚¤ì›Œë“œ ë¹ˆë„', fontsize=16, pad=20)
        ax.invert_yaxis()
        plt.tight_layout()
        
        freq_df = pd.DataFrame(keywords, columns=['í‚¤ì›Œë“œ', 'ë¹ˆë„'])
        
        return fig, freq_df
    
    
    def sentiment_keywords(self):
        """ê°ì„± í‚¤ì›Œë“œ ë¶„ì„"""
        positive_words = {
            'ì¢‹ë‹¤', 'ìµœê³ ', 'ëŒ€ë°•', 'ì˜ˆì˜ë‹¤', 'ì´ì˜ë‹¤', 'ë©‹ì§€ë‹¤', 'í›Œë¥­í•˜ë‹¤', 
            'ì™„ë²½', 'ì¢‹ì•„', 'ê°ì‚¬', 'ì‚¬ë‘', 'í–‰ë³µ', 'ì¶”ì²œ', 'êµ¿', 'good', 
            'best', 'love', 'amazing', 'perfect', 'great', 'excellent',
            'ì¢‹ì•„ìš”', 'ì¢‹ë„¤ìš”', 'ë©‹ìˆë‹¤', 'ì•„ë¦„ë‹µë‹¤', 'ìµœê³ ë‹¤', 'ì§±'
        }
        
        negative_words = {
            'ì‹«ë‹¤', 'ë³„ë¡œ', 'ì•ˆì¢‹ë‹¤', 'ë‚˜ì˜ë‹¤', 'ìµœì•…', 'í˜•í¸ì—†ë‹¤',
            'ì‹«ì–´', 'ì‹¤ë§', 'ë³„ë¡œë„¤', 'ì•„ì‰½ë‹¤', 'bad', 'worst', 'hate',
            'ì‹«ì–´ìš”', 'ë³„ë¡œì˜ˆìš”', 'ê·¸ì €ê·¸ë ‡ë‹¤', 'ì§€ë£¨í•˜ë‹¤'
        }
        
        def calculate_sentiment(text):
            text = self.preprocess_text(text)
            words = text.split()
            
            pos_count = sum(1 for w in words if w in positive_words)
            neg_count = sum(1 for w in words if w in negative_words)
            
            return pos_count, neg_count
        
        self.comments_df[['positive_count', 'negative_count']] = \
            self.comments_df['text'].apply(lambda x: pd.Series(calculate_sentiment(x)))
        
        def classify_sentiment(row):
            if row['positive_count'] > row['negative_count']:
                return 'ê¸ì •'
            elif row['positive_count'] < row['negative_count']:
                return 'ë¶€ì •'
            else:
                return 'ì¤‘ë¦½'
        
        self.comments_df['sentiment'] = self.comments_df.apply(classify_sentiment, axis=1)
        sentiment_counts = self.comments_df['sentiment'].value_counts()
        
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        
        colors = ['#90EE90', '#FFB6C1', '#D3D3D3']
        axes[0].pie(sentiment_counts.values, labels=sentiment_counts.index, 
                   autopct='%1.1f%%', colors=colors, startangle=90)
        axes[0].set_title('ëŒ“ê¸€ ê°ì„± ë¶„í¬', fontsize=14, pad=20)
        
        axes[1].bar(sentiment_counts.index, sentiment_counts.values, color=colors)
        axes[1].set_xlabel('ê°ì„±', fontsize=12)
        axes[1].set_ylabel('ëŒ“ê¸€ ìˆ˜', fontsize=12)
        axes[1].set_title('ê°ì„±ë³„ ëŒ“ê¸€ ìˆ˜', fontsize=14, pad=20)
        
        plt.tight_layout()
        
        sentiment_df = self.comments_df[['text', 'sentiment', 'positive_count', 'negative_count']]
        
        return fig, sentiment_counts, sentiment_df
    
    
    def time_trend(self, interval='D'):
        """ì‹œê°„ëŒ€ë³„ íŠ¸ë Œë“œ ë¶„ì„"""
        if 'published_at' not in self.comments_df.columns:
            return None, None
        
        time_counts = self.comments_df.set_index('published_at').resample(interval).size()
        time_likes = self.comments_df.set_index('published_at')['like_count'].resample(interval).sum()
        
        fig, axes = plt.subplots(2, 1, figsize=(14, 10))
        
        axes[0].plot(time_counts.index, time_counts.values, marker='o', linewidth=2)
        axes[0].set_xlabel('ë‚ ì§œ', fontsize=12)
        axes[0].set_ylabel('ëŒ“ê¸€ ìˆ˜', fontsize=12)
        axes[0].set_title('ì‹œê°„ëŒ€ë³„ ëŒ“ê¸€ ìˆ˜ ì¶”ì´', fontsize=14, pad=20)
        axes[0].grid(True, alpha=0.3)
        
        axes[1].plot(time_likes.index, time_likes.values, marker='o', 
                    color='coral', linewidth=2)
        axes[1].set_xlabel('ë‚ ì§œ', fontsize=12)
        axes[1].set_ylabel('ì¢‹ì•„ìš” ìˆ˜', fontsize=12)
        axes[1].set_title('ì‹œê°„ëŒ€ë³„ ì¢‹ì•„ìš” ìˆ˜ ì¶”ì´', fontsize=14, pad=20)
        axes[1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        trend_df = pd.DataFrame({
            'ë‚ ì§œ': time_counts.index,
            'ëŒ“ê¸€_ìˆ˜': time_counts.values,
            'ì¢‹ì•„ìš”_ìˆ˜': time_likes.values
        })
        
        return fig, trend_df
    
    
    def cooccurrence(self, top_n=15):
        """í‚¤ì›Œë“œ ë™ì‹œì¶œí˜„ ë¶„ì„"""
        top_keywords = [word for word, _ in self.extract_keywords(top_n=top_n)]
        cooc_matrix = pd.DataFrame(0, index=top_keywords, columns=top_keywords)
        
        for text in self.comments_df['text']:
            text = self.preprocess_text(text)
            words = set(text.split())
            
            for word1 in top_keywords:
                if word1 in words:
                    for word2 in top_keywords:
                        if word2 in words:
                            cooc_matrix.loc[word1, word2] += 1
        
        fig, ax = plt.subplots(figsize=(14, 12))
        sns.heatmap(cooc_matrix, annot=True, fmt='d', cmap='YlOrRd', 
                   cbar_kws={'label': 'ë™ì‹œì¶œí˜„ ë¹ˆë„'}, ax=ax)
        ax.set_title(f'ìƒìœ„ {top_n}ê°œ í‚¤ì›Œë“œ ë™ì‹œì¶œí˜„ ë¶„ì„', fontsize=16, pad=20)
        ax.set_xlabel('í‚¤ì›Œë“œ', fontsize=12)
        ax.set_ylabel('í‚¤ì›Œë“œ', fontsize=12)
        plt.tight_layout()
        
        return fig, cooc_matrix
    
    
    def topic_comparison(self):
        """ì˜ìƒë³„ í† í”½ ë¹„êµ ë¶„ì„"""
        if 'video_title' not in self.comments_df.columns:
            return None, None
        
        video_keywords = {}
        
        for video_title in self.comments_df['video_title'].unique()[:10]:
            video_comments = self.comments_df[
                self.comments_df['video_title'] == video_title
            ]['text']
            
            all_text = ' '.join(video_comments.apply(self.preprocess_text))
            words = all_text.split()
            words = [w for w in words if len(w) >= 2]
            
            word_freq = Counter(words)
            top_words = [word for word, _ in word_freq.most_common(5)]
            
            video_keywords[video_title[:30] + '...'] = top_words
        
        comparison_df = pd.DataFrame(video_keywords).T
        comparison_df.columns = [f'í‚¤ì›Œë“œ{i+1}' for i in range(comparison_df.shape[1])]
        
        fig, ax = plt.subplots(figsize=(14, 8))
        ax.axis('tight')
        ax.axis('off')
        
        table = ax.table(cellText=comparison_df.values,
                        rowLabels=comparison_df.index,
                        colLabels=comparison_df.columns,
                        cellLoc='center',
                        loc='center',
                        bbox=[0, 0, 1, 1])
        
        table.auto_set_font_size(False)
        table.set_fontsize(9)
        table.scale(1, 2)
        
        for i in range(len(comparison_df.columns)):
            table[(0, i)].set_facecolor('#4CAF50')
            table[(0, i)].set_text_props(weight='bold', color='white')
        
        for i in range(len(comparison_df)):
            table[(i+1, -1)].set_facecolor('#E8F5E9')
            table[(i+1, -1)].set_text_props(weight='bold')
        
        plt.title('ì˜ìƒë³„ ì£¼ìš” í‚¤ì›Œë“œ ë¹„êµ', fontsize=16, pad=20)
        
        return fig, comparison_df


def search_and_collect_data(keyword, max_videos, max_comments_per_video, order):
    """YouTube APIë¥¼ í†µí•œ ë°ì´í„° ìˆ˜ì§‘"""
    load_dotenv()
    api_key = os.getenv("YOUTUBE_API_KEY")
    
    if not api_key:
        st.error("YouTube API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        return None, None
    
    youtube = build("youtube", "v3", developerKey=api_key)
    
    # ì˜ìƒ ê²€ìƒ‰
    try:
        search_response = youtube.search().list(
            q=keyword,
            part="snippet",
            maxResults=min(max_videos, 50),
            type="video",
            order=order,
            regionCode="KR"
        ).execute()
        
        video_ids = [item["id"]["videoId"] for item in search_response["items"]]
    except Exception as e:
        st.error(f"ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return None, None
    
    # ì˜ìƒ ìƒì„¸ ì •ë³´
    videos_data = []
    try:
        for i in range(0, len(video_ids), 50):
            batch_ids = video_ids[i:i+50]
            video_response = youtube.videos().list(
                part="snippet,statistics,contentDetails",
                id=",".join(batch_ids)
            ).execute()
            
            for item in video_response["items"]:
                video_info = {
                    "video_id": item["id"],
                    "title": item["snippet"]["title"],
                    "channel": item["snippet"]["channelTitle"],
                    "published_at": item["snippet"]["publishedAt"],
                    "description": item["snippet"]["description"],
                    "view_count": int(item["statistics"].get("viewCount", 0)),
                    "like_count": int(item["statistics"].get("likeCount", 0)),
                    "comment_count": int(item["statistics"].get("commentCount", 0)),
                    "duration": item["contentDetails"]["duration"],
                    "tags": ", ".join(item["snippet"].get("tags", [])),
                    "url": f"https://www.youtube.com/watch?v={item['id']}"
                }
                videos_data.append(video_info)
    except Exception as e:
        st.error(f"ì˜ìƒ ì •ë³´ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
    
    videos_df = pd.DataFrame(videos_data)
    
    # ëŒ“ê¸€ ìˆ˜ì§‘
    all_comments = []
    video_info_dict = {}
    for _, row in videos_df.iterrows():
        video_info_dict[row['video_id']] = {
            'title': row['title'],
            'channel': row['channel'],
            'url': row['url']
        }
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for idx, video_id in enumerate(video_ids):
        status_text.text(f"ì˜ìƒ {idx+1}/{len(video_ids)} ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘...")
        progress_bar.progress((idx + 1) / len(video_ids))
        
        try:
            comments = []
            next_page_token = None
            
            while len(comments) < max_comments_per_video:
                request = youtube.commentThreads().list(
                    part="snippet,replies",
                    videoId=video_id,
                    maxResults=min(100, max_comments_per_video - len(comments)),
                    pageToken=next_page_token,
                    textFormat="plainText",
                    order="relevance"
                )
                response = request.execute()
                
                for item in response["items"]:
                    top_comment = item["snippet"]["topLevelComment"]["snippet"]
                    
                    comment_info = {
                        "comment_id": item["snippet"]["topLevelComment"]["id"],
                        "video_id": video_id,
                        "author": top_comment["authorDisplayName"],
                        "text": top_comment["textDisplay"],
                        "like_count": top_comment["likeCount"],
                        "published_at": top_comment["publishedAt"],
                        "reply_count": item["snippet"]["totalReplyCount"]
                    }
                    
                    if video_id in video_info_dict:
                        comment_info['video_title'] = video_info_dict[video_id]['title']
                        comment_info['video_channel'] = video_info_dict[video_id]['channel']
                        comment_info['video_url'] = video_info_dict[video_id]['url']
                    
                    comments.append(comment_info)
                
                next_page_token = response.get("nextPageToken")
                if not next_page_token:
                    break
                
                time.sleep(0.5)
            
            all_comments.extend(comments)
        
        except Exception as e:
            if "commentsDisabled" not in str(e):
                st.warning(f"ì˜ìƒ {video_id} ëŒ“ê¸€ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        
        time.sleep(1)
    
    progress_bar.empty()
    status_text.empty()
    
    comments_df = pd.DataFrame(all_comments)
    
    return videos_df, comments_df


# ========================================
# Streamlit ë©”ì¸ ì•±
# ========================================

def main():
    st.title("ğŸ¥ YouTube ëŒ“ê¸€ ë¶„ì„ ëŒ€ì‹œë³´ë“œ")
    st.markdown("---")
    
    # ì‚¬ì´ë“œë°” - ë°ì´í„° ìˆ˜ì§‘/ì—…ë¡œë“œë§Œ
    st.sidebar.header("ğŸ“‚ ë°ì´í„° ì†ŒìŠ¤")
    data_source = st.sidebar.radio(
        "ë°ì´í„° ì…ë ¥ ë°©ì‹ ì„ íƒ",
        ["APIë¡œ ì‹¤ì‹œê°„ ìˆ˜ì§‘", "CSV íŒŒì¼ ì—…ë¡œë“œ"]
    )
    
    videos_df = None
    comments_df = None
    
    if data_source == "APIë¡œ ì‹¤ì‹œê°„ ìˆ˜ì§‘":
        st.sidebar.subheader("ğŸ” ê²€ìƒ‰ ì„¤ì •")
        keyword = st.sidebar.text_input("ê²€ìƒ‰ í‚¤ì›Œë“œ", value="K-beauty")
        max_videos = st.sidebar.slider("ì˜ìƒ ê°œìˆ˜", 1, 50, 10)
        max_comments = st.sidebar.slider("ì˜ìƒë‹¹ ëŒ“ê¸€ ìˆ˜", 10, 200, 50)
        order = st.sidebar.selectbox(
            "ì •ë ¬ ë°©ì‹",
            ["relevance", "date", "viewCount"],
            format_func=lambda x: {"relevance": "ê´€ë ¨ì„±ìˆœ", "date": "ìµœì‹ ìˆœ", "viewCount": "ì¡°íšŒìˆ˜ìˆœ"}[x]
        )
        
        if st.sidebar.button("ğŸš€ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘"):
            with st.spinner("ë°ì´í„° ìˆ˜ì§‘ ì¤‘..."):
                videos_df, comments_df = search_and_collect_data(
                    keyword, max_videos, max_comments, order
                )
            
            if videos_df is not None and comments_df is not None:
                st.success(f"âœ… ì˜ìƒ {len(videos_df)}ê°œ, ëŒ“ê¸€ {len(comments_df)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ!")
                
                # ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸ì— ì €ì¥
                st.session_state['videos_df'] = videos_df
                st.session_state['comments_df'] = comments_df
    
    else:  # CSV íŒŒì¼ ì—…ë¡œë“œ
        st.sidebar.subheader("ğŸ“¤ íŒŒì¼ ì—…ë¡œë“œ")
        comments_file = st.sidebar.file_uploader("ëŒ“ê¸€ CSV íŒŒì¼", type=['csv'])
        videos_file = st.sidebar.file_uploader("ì˜ìƒ CSV íŒŒì¼ (ì„ íƒ)", type=['csv'])
        
        if comments_file:
            comments_df = pd.read_csv(comments_file)
            st.session_state['comments_df'] = comments_df
            st.sidebar.success(f"âœ… ëŒ“ê¸€ {len(comments_df)}ê°œ ë¡œë“œ")
        
        if videos_file:
            videos_df = pd.read_csv(videos_file)
            st.session_state['videos_df'] = videos_df
            st.sidebar.success(f"âœ… ì˜ìƒ {len(videos_df)}ê°œ ë¡œë“œ")
    
    # ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸ì—ì„œ ë°ì´í„° ë¡œë“œ
    if 'comments_df' in st.session_state:
        comments_df = st.session_state['comments_df']
    if 'videos_df' in st.session_state:
        videos_df = st.session_state['videos_df']
    
    # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì•ˆë‚´ ë©”ì‹œì§€
    if comments_df is None or comments_df.empty:
        st.info("ğŸ‘† ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê±°ë‚˜ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        return
    
    # ê¸°ë³¸ í†µê³„
    st.header("ğŸ“ˆ ê¸°ë³¸ í†µê³„")
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("ì´ ëŒ“ê¸€ ìˆ˜", f"{len(comments_df):,}")
    with col2:
        st.metric("í‰ê·  ì¢‹ì•„ìš”", f"{comments_df['like_count'].mean():.1f}")
    with col3:
        st.metric("ì´ ì¢‹ì•„ìš”", f"{comments_df['like_count'].sum():,}")
    with col4:
        if videos_df is not None:
            st.metric("ë¶„ì„ ì˜ìƒ ìˆ˜", f"{len(videos_df)}")
    
    st.markdown("---")
    
    # íƒ­ìœ¼ë¡œ ë¶„ì„ ëª¨ë“œ êµ¬ë¶„
    tabs = st.tabs([
        "â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œ",
        "ğŸ“Š í‚¤ì›Œë“œ ë¹ˆë„",
        "ğŸ˜ŠğŸ˜¢ ê°ì„± ë¶„ì„",
        "ğŸ“ˆ ì‹œê°„ íŠ¸ë Œë“œ",
        "ğŸ”— ë™ì‹œì¶œí˜„",
        "ğŸ¬ í† í”½ ë¹„êµ",
        "ğŸ“‹ ì›ë³¸ ë°ì´í„°"
    ])
    
    analyzer = YouTubeCommentAnalyzer(comments_df, videos_df)
    
    # íƒ­ 1: ì›Œë“œí´ë¼ìš°ë“œ
    with tabs[0]:
        st.header("â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œ")
        if st.button("ğŸ” ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±", key="btn_wordcloud"):
            with st.spinner("ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì¤‘..."):
                fig = analyzer.wordcloud()
                st.pyplot(fig)
        else:
            st.info("ğŸ‘† ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•˜ì„¸ìš”.")
    
    # íƒ­ 2: í‚¤ì›Œë“œ ë¹ˆë„
    with tabs[1]:
        st.header("ğŸ“Š í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„")
        top_n = st.slider("í‘œì‹œí•  í‚¤ì›Œë“œ ê°œìˆ˜", 10, 50, 20, key="keyword_top_n")
        
        if st.button("ğŸ” í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„", key="btn_keyword"):
            with st.spinner("í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„ ì¤‘..."):
                fig, freq_df = analyzer.keyword_frequency(top_n=top_n)
                st.pyplot(fig)
                
                st.subheader("ğŸ“‹ í‚¤ì›Œë“œ ë°ì´í„°")
                st.dataframe(freq_df, use_container_width=True)
                
                csv = freq_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
                st.download_button(
                    "ğŸ’¾ CSV ë‹¤ìš´ë¡œë“œ",
                    csv,
                    "keyword_frequency.csv",
                    "text/csv",
                    key='download-keyword-csv'
                )
        else:
            st.info("ğŸ‘† ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ í‚¤ì›Œë“œ ë¹ˆë„ë¥¼ ë¶„ì„í•˜ì„¸ìš”.")
    
    # íƒ­ 3: ê°ì„± ë¶„ì„
    with tabs[2]:
        st.header("ğŸ˜ŠğŸ˜¢ ê°ì„± ë¶„ì„")
        
        if st.button("ğŸ” ê°ì„± ë¶„ì„ ì‹¤í–‰", key="btn_sentiment"):
            with st.spinner("ê°ì„± ë¶„ì„ ì¤‘..."):
                fig, sentiment_counts, sentiment_df = analyzer.sentiment_keywords()
                st.pyplot(fig)
                
                col1, col2, col3 = st.columns(3)
                for idx, (sentiment, count) in enumerate(sentiment_counts.items()):
                    with [col1, col2, col3][idx]:
                        st.metric(sentiment, f"{count:,}ê°œ")
                
                st.subheader("ğŸ“‹ ê°ì„± ë¶„ë¥˜ ë°ì´í„°")
                st.dataframe(sentiment_df.head(100), use_container_width=True)
                
                csv = sentiment_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
                st.download_button(
                    "ğŸ’¾ CSV ë‹¤ìš´ë¡œë“œ",
                    csv,
                    "sentiment_analysis.csv",
                    "text/csv",
                    key='download-sentiment-csv'
                )
        else:
            st.info("ğŸ‘† ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ê°ì„± ë¶„ì„ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
    
    # íƒ­ 4: ì‹œê°„ íŠ¸ë Œë“œ
    with tabs[3]:
        st.header("ğŸ“ˆ ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„")
        interval = st.radio("ì‹œê°„ ê°„ê²©", ["D (ì¼)", "W (ì£¼)", "M (ì›”)"], horizontal=True, key="time_interval")
        interval_code = interval.split()[0]
        
        if st.button("ğŸ” ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„", key="btn_time"):
            with st.spinner("ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„ ì¤‘..."):
                fig, trend_df = analyzer.time_trend(interval=interval_code)
                if fig:
                    st.pyplot(fig)
                    
                    st.subheader("ğŸ“‹ íŠ¸ë Œë“œ ë°ì´í„°")
                    st.dataframe(trend_df, use_container_width=True)
                    
                    csv = trend_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
                    st.download_button(
                        "ğŸ’¾ CSV ë‹¤ìš´ë¡œë“œ",
                        csv,
                        "time_trend.csv",
                        "text/csv",
                        key='download-trend-csv'
                    )
                else:
                    st.warning("published_at ì»¬ëŸ¼ì´ ì—†ì–´ ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.info("ğŸ‘† ì‹œê°„ ê°„ê²©ì„ ì„ íƒí•˜ê³  ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ë¶„ì„í•˜ì„¸ìš”.")
    
    # íƒ­ 5: ë™ì‹œì¶œí˜„
    with tabs[4]:
        st.header("ğŸ”— í‚¤ì›Œë“œ ë™ì‹œì¶œí˜„ ë¶„ì„")
        cooc_n = st.slider("ë¶„ì„í•  í‚¤ì›Œë“œ ê°œìˆ˜", 5, 20, 15, key="cooc_n")
        
        if st.button("ğŸ” ë™ì‹œì¶œí˜„ ë¶„ì„", key="btn_cooc"):
            with st.spinner("ë™ì‹œì¶œí˜„ ë¶„ì„ ì¤‘..."):
                fig, cooc_matrix = analyzer.cooccurrence(top_n=cooc_n)
                st.pyplot(fig)
                
                st.subheader("ğŸ“‹ ë™ì‹œì¶œí˜„ ë§¤íŠ¸ë¦­ìŠ¤")
                st.dataframe(cooc_matrix, use_container_width=True)
                
                csv = cooc_matrix.to_csv(encoding='utf-8-sig').encode('utf-8-sig')
                st.download_button(
                    "ğŸ’¾ CSV ë‹¤ìš´ë¡œë“œ",
                    csv,
                    "cooccurrence_matrix.csv",
                    "text/csv",
                    key='download-cooc-csv'
                )
        else:
            st.info("ğŸ‘† í‚¤ì›Œë“œ ê°œìˆ˜ë¥¼ ì„ íƒí•˜ê³  ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ë¶„ì„í•˜ì„¸ìš”.")
    
    # íƒ­ 6: í† í”½ ë¹„êµ
    with tabs[5]:
        st.header("ğŸ¬ ì˜ìƒë³„ í† í”½ ë¹„êµ")
        
        if st.button("ğŸ” í† í”½ ë¹„êµ ë¶„ì„", key="btn_topic"):
            with st.spinner("í† í”½ ë¹„êµ ë¶„ì„ ì¤‘..."):
                fig, comparison_df = analyzer.topic_comparison()
                if fig:
                    st.pyplot(fig)
                    
                    st.subheader("ğŸ“‹ í† í”½ ë¹„êµ ë°ì´í„°")
                    st.dataframe(comparison_df, use_container_width=True)
                    
                    csv = comparison_df.to_csv(encoding='utf-8-sig').encode('utf-8-sig')
                    st.download_button(
                        "ğŸ’¾ CSV ë‹¤ìš´ë¡œë“œ",
                        csv,
                        "topic_comparison.csv",
                        "text/csv",
                        key='download-topic-csv'
                    )
                else:
                    st.warning("video_title ì»¬ëŸ¼ì´ ì—†ì–´ í† í”½ ë¹„êµ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.info("ğŸ‘† ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ í† í”½ ë¹„êµë¥¼ ë¶„ì„í•˜ì„¸ìš”.")
    
    # íƒ­ 7: ì›ë³¸ ë°ì´í„°
    with tabs[6]:
        st.header("ğŸ“‹ ì›ë³¸ ë°ì´í„°")
        
        data_type = st.radio("ë°ì´í„° ìœ í˜• ì„ íƒ", ["ëŒ“ê¸€ ë°ì´í„°", "ì˜ìƒ ë°ì´í„°"], horizontal=True)
        
        if data_type == "ëŒ“ê¸€ ë°ì´í„°":
            st.subheader("ğŸ’¬ ëŒ“ê¸€ ë°ì´í„°")
            st.dataframe(comments_df, use_container_width=True, height=600)
            
            csv = comments_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
            st.download_button(
                "ğŸ’¾ ëŒ“ê¸€ ë°ì´í„° CSV ë‹¤ìš´ë¡œë“œ",
                csv,
                "comments_data.csv",
                "text/csv",
                key='download-comments-raw'
            )
        
        else:
            if videos_df is not None and not videos_df.empty:
                st.subheader("ğŸ¥ ì˜ìƒ ë°ì´í„°")
                st.dataframe(videos_df, use_container_width=True, height=600)
                
                csv = videos_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
                st.download_button(
                    "ğŸ’¾ ì˜ìƒ ë°ì´í„° CSV ë‹¤ìš´ë¡œë“œ",
                    csv,
                    "videos_data.csv",
                    "text/csv",
                    key='download-videos-raw'
                )
            else:
                st.warning("ì˜ìƒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    st.markdown("---")
    st.header("ğŸ“„ Market Insight Report Generator (flan-t5-base ê¸°ë°˜)")
    st.write("ì €ì¥ëœ ë¶„ì„ CSV íŒŒì¼ì„ ê¸°ë°˜ìœ¼ë¡œ ìš”ì•½ ë³´ê³ ì„œë¥¼ ìë™ ìƒì„±í•©ë‹ˆë‹¤.")

    available_files = [f for f in os.listdir(SAVE_DIR) if f.endswith(".csv")]

    if not available_files:
        st.warning("ë¶„ì„ ê²°ê³¼ CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ìœ„ ë¶„ì„ íƒ­ì—ì„œ ë¶„ì„ì„ ì‹¤í–‰í•˜ê³  CSVë¥¼ ë‹¤ìš´ë¡œë“œ/ì €ì¥í•˜ì„¸ìš”.")
    else:
        selected_files = st.multiselect("ğŸ“‚ ë³´ê³ ì„œì— í¬í•¨í•  íŒŒì¼ ì„ íƒ", available_files, default=available_files)

        if st.button("ğŸ§  ë³´ê³ ì„œ ìƒì„±"):
            full_text = ""
            for f in selected_files:
                file_path = os.path.join(SAVE_DIR, f)
                try:
                    # CSVë¥¼ pandasë¡œ ë¡œë“œí•˜ê³  ì„œìˆ ì  í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ì´ì „ ì œì•ˆì²˜ëŸ¼)
                    df = pd.read_csv(file_path, encoding="utf-8-sig")
                    content = f"Market insight from {f}:\n"
                    content += f"This dataset has {len(df)} rows.\n"
                    content += "Key statistics:\n" + df.describe().to_string() + "\n\n"
                    content += "Sample data:\n" + df.head(5).to_string(index=False) + "\n\n"
                    full_text += content + "\n\n"
                except Exception as e:
                    st.error(f"íŒŒì¼ {f} ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
                    continue

            if full_text:
                with st.spinner("flan-t5-base ëª¨ë¸ì´ ìš”ì•½ ë³´ê³ ì„œë¥¼ ìƒì„± ì¤‘..."):
                    summary = summarize_text(full_text, tokenizer, model, max_input=1024, max_output=500)

                st.subheader("ğŸ“ˆ AI ìë™ ìƒì„± ë³´ê³ ì„œ ì´ˆì•ˆ")
                st.text_area("ìš”ì•½ ê²°ê³¼", summary, height=400)

                st.download_button(
                    "ğŸ’¾ ìš”ì•½ ë³´ê³ ì„œ ì €ì¥",
                    summary.encode("utf-8-sig"),
                    "Market_Insight_Report.txt",
                    "text/plain"
                )



SAVE_DIR = "analysis_results"
os.makedirs(SAVE_DIR, exist_ok=True)

from transformers import AutoTokenizer, AutoModelWithLMHead  # WithLMHead ì‚¬ìš© (ëª¨ë¸ ì¹´ë“œ ì¶”ì²œ)

@st.cache_resource
def load_common_gen_model():
    """Hugging Face mrm8488/t5-base-finetuned-common_gen ëª¨ë¸ ë¡œë“œ"""
    model_name = "mrm8488/t5-base-finetuned-common_gen"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelWithLMHead.from_pretrained(model_name)
    if torch.cuda.is_available():
        model.half()  # FP16ìœ¼ë¡œ VRAM ì ˆì•½
        model.to("cuda")
    return tokenizer, model

tokenizer, model = load_common_gen_model()

def generate_report(text, tokenizer, model, max_length=64):
    """mrm8488/t5-base-finetuned-common_genì„ ì´ìš©í•œ ë³´ê³ ì„œ ìƒì„± í•¨ìˆ˜"""
    # ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ í‚¤ì›Œë“œ ë¬¸ìì—´ë¡œ ë³€í™˜ (ë‹¹ì‹  CSV ì˜ˆì‹œì²˜ëŸ¼)
    # full_textê°€ "Insight from file: keywords beauty 50 makeup 30 ..." ì‹ì´ë©´ splitìœ¼ë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ
    keywords = ' '.join(text.split()[:20])  # ë„ˆë¬´ ê¸¸ë©´ ìë¦„ (ëª¨ë¸ ì…ë ¥ ì œí•œ ~512 í† í°)
    
    inputs = tokenizer([keywords], return_tensors="pt", truncation=True)
    if torch.cuda.is_available():
        inputs = {k: v.to("cuda") for k, v in inputs.items()}
    
    # ìƒì„±: max_lengthë¡œ ê¸¸ì´ ì œì–´, num_beamsë¡œ í’ˆì§ˆ í–¥ìƒ
    output_ids = model.generate(**inputs, max_length=max_length, num_beams=4, early_stopping=True)
    generated = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    
    return generate

if __name__ == "__main__":
    main()