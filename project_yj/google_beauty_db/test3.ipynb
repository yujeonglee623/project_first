{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed54e9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 16:22:20.616 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:20.619 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:20.710 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Users\\Admin\\miniconda3\\envs\\gpt_env\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-11-03 16:22:20.711 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:20.711 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:20.711 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:21.224 Thread 'Thread-3': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:21.226 Thread 'Thread-3': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:21.226 Thread 'Thread-3': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.023 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.024 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.024 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.026 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.027 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.027 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.028 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.028 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.029 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.029 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.030 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.030 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.030 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.031 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.031 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.031 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.032 Session state does not function when running a script without `streamlit run`\n",
      "2025-11-03 16:22:23.033 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.033 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.034 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.034 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.035 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.035 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.036 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.036 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.036 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.037 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.037 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.037 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.037 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.038 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.038 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.038 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.038 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.038 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.039 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.040 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.040 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.040 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.042 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.043 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.043 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.044 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.044 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.044 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.044 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.045 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.046 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.046 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.046 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.046 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.046 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.046 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.046 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.048 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.049 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.049 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.052 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.052 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:23.053 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "import io\n",
    "from googleapiclient.discovery import build\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch  # GPU ê´€ë ¨ ì½”ë“œ ì‚¬ìš© ì‹œ í•„ìš”\n",
    "\n",
    "# í˜ì´ì§€ ì„¤ì • (top-level)\n",
    "st.set_page_config(\n",
    "    page_title=\"YouTube ëŒ“ê¸€ ë¶„ì„ ëŒ€ì‹œë³´ë“œ\",\n",
    "    page_icon=\"ğŸ¥\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì •\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'  # Windows\n",
    "# plt.rcParams['font.family'] = 'AppleGothic'  # Mac\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# SAVE_DIRì™€ ëª¨ë¸ ë¡œë“œ (top-level, imports í›„ ë°”ë¡œ ì •ì˜)\n",
    "SAVE_DIR = \"analysis_results\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "@st.cache_resource\n",
    "def load_t5_model():\n",
    "    \"\"\"Hugging Face google/flan-t5-base ëª¨ë¸ ë¡œë“œ\"\"\"\n",
    "    model_name = \"google/flan-t5-base\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    if torch.cuda.is_available():\n",
    "        model.half()\n",
    "        model.to(\"cuda\")\n",
    "    return tokenizer, model\n",
    "\n",
    "tokenizer, model = load_t5_model()\n",
    "\n",
    "def summarize_text(text, tokenizer, model, max_input=512, max_output=300):\n",
    "    \"\"\"flan-t5-baseë¥¼ ì´ìš©í•œ ìš”ì•½ í•¨ìˆ˜\"\"\"\n",
    "    text = text[:max_input]\n",
    "    input_text = f\"Generate a concise market insight report from this CSV data: {text}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=max_input)\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "    summary_ids = model.generate(**inputs, max_length=max_output, min_length=100, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# YouTubeCommentAnalyzer í´ë˜ìŠ¤ (ì „ì²´ ë‚´ìš©)\n",
    "class YouTubeCommentAnalyzer:\n",
    "    \"\"\"YouTube ëŒ“ê¸€ ë¶„ì„ í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self, comments_df, videos_df=None):\n",
    "        self.comments_df = comments_df.copy()\n",
    "        self.videos_df = videos_df.copy() if videos_df is not None else None\n",
    "        \n",
    "        # ë‚ ì§œ ì»¬ëŸ¼ ë³€í™˜\n",
    "        if 'published_at' in self.comments_df.columns:\n",
    "            self.comments_df['published_at'] = pd.to_datetime(self.comments_df['published_at'])\n",
    "    \n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        text = str(text).lower()\n",
    "        text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "        text = re.sub(r'[^ê°€-í£a-z0-9\\s]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    \n",
    "    def extract_keywords(self, min_length=2, top_n=50):\n",
    "        \"\"\"í‚¤ì›Œë“œ ì¶”ì¶œ\"\"\"\n",
    "        all_text = ' '.join(self.comments_df['text'].apply(self.preprocess_text))\n",
    "        words = all_text.split()\n",
    "        words = [w for w in words if len(w) >= min_length]\n",
    "        \n",
    "        stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',\n",
    "                    'of', 'is', 'are', 'was', 'were', 'been', 'be', 'have', 'has', 'had',\n",
    "                    'ê·¸', 'ì´', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë“¤', 'ë°', 'ë˜í•œ', 'í•˜ë‹¤', 'ìˆë‹¤', 'ë˜ë‹¤',\n",
    "                    'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ê·¸ëŸ°', 'ì´ëŸ°', 'ì €ëŸ°'}\n",
    "        \n",
    "        words = [w for w in words if w not in stopwords]\n",
    "        word_freq = Counter(words)\n",
    "        \n",
    "        return word_freq.most_common(top_n)\n",
    "    \n",
    "    \n",
    "    def wordcloud(self, width=1200, height=800):\n",
    "        \"\"\"ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±\"\"\"\n",
    "        all_text = ' '.join(self.comments_df['text'].apply(self.preprocess_text))\n",
    "        \n",
    "        wordcloud = WordCloud(\n",
    "            font_path='malgun.ttf',\n",
    "            width=width,\n",
    "            height=height,\n",
    "            background_color='white',\n",
    "            max_words=100,\n",
    "            relative_scaling=0.3,\n",
    "            colormap='viridis'\n",
    "        ).generate(all_text)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(15, 10))\n",
    "        ax.imshow(wordcloud, interpolation='bilinear')\n",
    "        ax.axis('off')\n",
    "        ax.set_title('ëŒ“ê¸€ ì›Œë“œí´ë¼ìš°ë“œ', fontsize=20, pad=20)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    \n",
    "    def keyword_frequency(self, top_n=20):\n",
    "        \"\"\"í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„\"\"\"\n",
    "        keywords = self.extract_keywords(top_n=top_n)\n",
    "        words, counts = zip(*keywords)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        ax.barh(range(len(words)), counts, color='skyblue')\n",
    "        ax.set_yticks(range(len(words)))\n",
    "        ax.set_yticklabels(words)\n",
    "        ax.set_xlabel('ë¹ˆë„', fontsize=12)\n",
    "        ax.set_title(f'ìƒìœ„ {top_n}ê°œ í‚¤ì›Œë“œ ë¹ˆë„', fontsize=16, pad=20)\n",
    "        ax.invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        freq_df = pd.DataFrame(keywords, columns=['í‚¤ì›Œë“œ', 'ë¹ˆë„'])\n",
    "        \n",
    "        return fig, freq_df\n",
    "    \n",
    "    \n",
    "    def sentiment_keywords(self):\n",
    "        \"\"\"ê°ì„± í‚¤ì›Œë“œ ë¶„ì„\"\"\"\n",
    "        positive_words = {\n",
    "            'ì¢‹ë‹¤', 'ìµœê³ ', 'ëŒ€ë°•', 'ì˜ˆì˜ë‹¤', 'ì´ì˜ë‹¤', 'ë©‹ì§€ë‹¤', 'í›Œë¥­í•˜ë‹¤', \n",
    "            'ì™„ë²½', 'ì¢‹ì•„', 'ê°ì‚¬', 'ì‚¬ë‘', 'í–‰ë³µ', 'ì¶”ì²œ', 'êµ¿', 'good', \n",
    "            'best', 'love', 'amazing', 'perfect', 'great', 'excellent',\n",
    "            'ì¢‹ì•„ìš”', 'ì¢‹ë„¤ìš”', 'ë©‹ìˆë‹¤', 'ì•„ë¦„ë‹µë‹¤', 'ìµœê³ ë‹¤', 'ì§±'\n",
    "        }\n",
    "        \n",
    "        negative_words = {\n",
    "            'ì‹«ë‹¤', 'ë³„ë¡œ', 'ì•ˆì¢‹ë‹¤', 'ë‚˜ì˜ë‹¤', 'ìµœì•…', 'í˜•í¸ì—†ë‹¤',\n",
    "            'ì‹«ì–´', 'ì‹¤ë§', 'ë³„ë¡œë„¤', 'ì•„ì‰½ë‹¤', 'bad', 'worst', 'hate',\n",
    "            'ì‹«ì–´ìš”', 'ë³„ë¡œì˜ˆìš”', 'ê·¸ì €ê·¸ë ‡ë‹¤', 'ì§€ë£¨í•˜ë‹¤'\n",
    "        }\n",
    "        \n",
    "        def calculate_sentiment(text):\n",
    "            text = self.preprocess_text(text)\n",
    "            words = text.split()\n",
    "            \n",
    "            pos_count = sum(1 for w in words if w in positive_words)\n",
    "            neg_count = sum(1 for w in words if w in negative_words)\n",
    "            \n",
    "            return pos_count, neg_count\n",
    "        \n",
    "        self.comments_df[['positive_count', 'negative_count']] = \\\n",
    "            self.comments_df['text'].apply(lambda x: pd.Series(calculate_sentiment(x)))\n",
    "        \n",
    "        def classify_sentiment(row):\n",
    "            if row['positive_count'] > row['negative_count']:\n",
    "                return 'ê¸ì •'\n",
    "            elif row['positive_count'] < row['negative_count']:\n",
    "                return 'ë¶€ì •'\n",
    "            else:\n",
    "                return 'ì¤‘ë¦½'\n",
    "        \n",
    "        self.comments_df['sentiment'] = self.comments_df.apply(classify_sentiment, axis=1)\n",
    "        sentiment_counts = self.comments_df['sentiment'].value_counts()\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        colors = ['#90EE90', '#FFB6C1', '#D3D3D3']\n",
    "        axes[0].pie(sentiment_counts.values, labels=sentiment_counts.index, \n",
    "                   autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "        axes[0].set_title('ëŒ“ê¸€ ê°ì„± ë¶„í¬', fontsize=14, pad=20)\n",
    "        \n",
    "        axes[1].bar(sentiment_counts.index, sentiment_counts.values, color=colors)\n",
    "        axes[1].set_xlabel('ê°ì„±', fontsize=12)\n",
    "        axes[1].set_ylabel('ëŒ“ê¸€ ìˆ˜', fontsize=12)\n",
    "        axes[1].set_title('ê°ì„±ë³„ ëŒ“ê¸€ ìˆ˜', fontsize=14, pad=20)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        sentiment_df = self.comments_df[['text', 'sentiment', 'positive_count', 'negative_count']]\n",
    "        \n",
    "        return fig, sentiment_counts, sentiment_df\n",
    "    \n",
    "    \n",
    "    def time_trend(self, interval='D'):\n",
    "        \"\"\"ì‹œê°„ëŒ€ë³„ íŠ¸ë Œë“œ ë¶„ì„\"\"\"\n",
    "        if 'published_at' not in self.comments_df.columns:\n",
    "            return None, None\n",
    "        \n",
    "        time_counts = self.comments_df.set_index('published_at').resample(interval).size()\n",
    "        time_likes = self.comments_df.set_index('published_at')['like_count'].resample(interval).sum()\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "        \n",
    "        axes[0].plot(time_counts.index, time_counts.values, marker='o', linewidth=2)\n",
    "        axes[0].set_xlabel('ë‚ ì§œ', fontsize=12)\n",
    "        axes[0].set_ylabel('ëŒ“ê¸€ ìˆ˜', fontsize=12)\n",
    "        axes[0].set_title('ì‹œê°„ëŒ€ë³„ ëŒ“ê¸€ ìˆ˜ ì¶”ì´', fontsize=14, pad=20)\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        axes[1].plot(time_likes.index, time_likes.values, marker='o', \n",
    "                    color='coral', linewidth=2)\n",
    "        axes[1].set_xlabel('ë‚ ì§œ', fontsize=12)\n",
    "        axes[1].set_ylabel('ì¢‹ì•„ìš” ìˆ˜', fontsize=12)\n",
    "        axes[1].set_title('ì‹œê°„ëŒ€ë³„ ì¢‹ì•„ìš” ìˆ˜ ì¶”ì´', fontsize=14, pad=20)\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        trend_df = pd.DataFrame({\n",
    "            'ë‚ ì§œ': time_counts.index,\n",
    "            'ëŒ“ê¸€_ìˆ˜': time_counts.values,\n",
    "            'ì¢‹ì•„ìš”_ìˆ˜': time_likes.values\n",
    "        })\n",
    "        \n",
    "        return fig, trend_df\n",
    "    \n",
    "    \n",
    "    def cooccurrence(self, top_n=15):\n",
    "        \"\"\"í‚¤ì›Œë“œ ë™ì‹œì¶œí˜„ ë¶„ì„\"\"\"\n",
    "        top_keywords = [word for word, _ in self.extract_keywords(top_n=top_n)]\n",
    "        cooc_matrix = pd.DataFrame(0, index=top_keywords, columns=top_keywords)\n",
    "        \n",
    "        for text in self.comments_df['text']:\n",
    "            text = self.preprocess_text(text)\n",
    "            words = set(text.split())\n",
    "            \n",
    "            for word1 in top_keywords:\n",
    "                if word1 in words:\n",
    "                    for word2 in top_keywords:\n",
    "                        if word2 in words:\n",
    "                            cooc_matrix.loc[word1, word2] += 1\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(14, 12))\n",
    "        sns.heatmap(cooc_matrix, annot=True, fmt='d', cmap='YlOrRd', \n",
    "                   cbar_kws={'label': 'ë™ì‹œì¶œí˜„ ë¹ˆë„'}, ax=ax)\n",
    "        ax.set_title(f'ìƒìœ„ {top_n}ê°œ í‚¤ì›Œë“œ ë™ì‹œì¶œí˜„ ë¶„ì„', fontsize=16, pad=20)\n",
    "        ax.set_xlabel('í‚¤ì›Œë“œ', fontsize=12)\n",
    "        ax.set_ylabel('í‚¤ì›Œë“œ', fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        return fig, cooc_matrix\n",
    "    \n",
    "    \n",
    "    def topic_comparison(self):\n",
    "        \"\"\"ì˜ìƒë³„ í† í”½ ë¹„êµ ë¶„ì„\"\"\"\n",
    "        if 'video_title' not in self.comments_df.columns:\n",
    "            return None, None\n",
    "        \n",
    "        video_keywords = {}\n",
    "        \n",
    "        for video_title in self.comments_df['video_title'].unique()[:10]:\n",
    "            video_comments = self.comments_df[\n",
    "                self.comments_df['video_title'] == video_title\n",
    "            ]['text']\n",
    "            \n",
    "            all_text = ' '.join(video_comments.apply(self.preprocess_text))\n",
    "            words = all_text.split()\n",
    "            words = [w for w in words if len(w) >= 2]\n",
    "            \n",
    "            word_freq = Counter(words)\n",
    "            top_words = [word for word, _ in word_freq.most_common(5)]\n",
    "            \n",
    "            video_keywords[video_title[:30] + '...'] = top_words\n",
    "        \n",
    "        comparison_df = pd.DataFrame(video_keywords).T\n",
    "        comparison_df.columns = [f'í‚¤ì›Œë“œ{i+1}' for i in range(comparison_df.shape[1])]\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(14, 8))\n",
    "        ax.axis('tight')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        table = ax.table(cellText=comparison_df.values,\n",
    "                        rowLabels=comparison_df.index,\n",
    "                        colLabels=comparison_df.columns,\n",
    "                        cellLoc='center',\n",
    "                        loc='center',\n",
    "                        bbox=[0, 0, 1, 1])\n",
    "        \n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(9)\n",
    "        table.scale(1, 2)\n",
    "        \n",
    "        for i in range(len(comparison_df.columns)):\n",
    "            table[(0, i)].set_facecolor('#4CAF50')\n",
    "            table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "        \n",
    "        for i in range(len(comparison_df)):\n",
    "            table[(i+1, -1)].set_facecolor('#E8F5E9')\n",
    "            table[(i+1, -1)].set_text_props(weight='bold')\n",
    "        \n",
    "        plt.title('ì˜ìƒë³„ ì£¼ìš” í‚¤ì›Œë“œ ë¹„êµ', fontsize=16, pad=20)\n",
    "        \n",
    "        return fig, comparison_df\n",
    "\n",
    "# search_and_collect_data í•¨ìˆ˜ (ì „ì²´ ë‚´ìš©)\n",
    "def search_and_collect_data(keyword, max_videos, max_comments_per_video, order):\n",
    "    \"\"\"YouTube APIë¥¼ í†µí•œ ë°ì´í„° ìˆ˜ì§‘\"\"\"\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "    \n",
    "    if not api_key:\n",
    "        st.error(\"YouTube API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "        return None, None\n",
    "    \n",
    "    youtube = build(\"youtube\", \"v3\", developerKey=api_key)\n",
    "    \n",
    "    # ì˜ìƒ ê²€ìƒ‰\n",
    "    try:\n",
    "        search_response = youtube.search().list(\n",
    "            q=keyword,\n",
    "            part=\"snippet\",\n",
    "            maxResults=min(max_videos, 50),\n",
    "            type=\"video\",\n",
    "            order=order,\n",
    "            regionCode=\"KR\"\n",
    "        ).execute()\n",
    "        \n",
    "        video_ids = [item[\"id\"][\"videoId\"] for item in search_response[\"items\"]]\n",
    "    except Exception as e:\n",
    "        st.error(f\"ê²€ìƒ‰ ì˜¤ë¥˜: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    # ì˜ìƒ ìƒì„¸ ì •ë³´\n",
    "    videos_data = []\n",
    "    try:\n",
    "        for i in range(0, len(video_ids), 50):\n",
    "            batch_ids = video_ids[i:i+50]\n",
    "            video_response = youtube.videos().list(\n",
    "                part=\"snippet,statistics,contentDetails\",\n",
    "                id=\",\".join(batch_ids)\n",
    "            ).execute()\n",
    "            \n",
    "            for item in video_response[\"items\"]:\n",
    "                video_info = {\n",
    "                    \"video_id\": item[\"id\"],\n",
    "                    \"title\": item[\"snippet\"][\"title\"],\n",
    "                    \"channel\": item[\"snippet\"][\"channelTitle\"],\n",
    "                    \"published_at\": item[\"snippet\"][\"publishedAt\"],\n",
    "                    \"description\": item[\"snippet\"][\"description\"],\n",
    "                    \"view_count\": int(item[\"statistics\"].get(\"viewCount\", 0)),\n",
    "                    \"like_count\": int(item[\"statistics\"].get(\"likeCount\", 0)),\n",
    "                    \"comment_count\": int(item[\"statistics\"].get(\"commentCount\", 0)),\n",
    "                    \"duration\": item[\"contentDetails\"][\"duration\"],\n",
    "                    \"tags\": \", \".join(item[\"snippet\"].get(\"tags\", [])),\n",
    "                    \"url\": f\"https://www.youtube.com/watch?v={item['id']}\"\n",
    "                }\n",
    "                videos_data.append(video_info)\n",
    "    except Exception as e:\n",
    "        st.error(f\"ì˜ìƒ ì •ë³´ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}\")\n",
    "    \n",
    "    videos_df = pd.DataFrame(videos_data)\n",
    "    \n",
    "    # ëŒ“ê¸€ ìˆ˜ì§‘\n",
    "    all_comments = []\n",
    "    video_info_dict = {}\n",
    "    for _, row in videos_df.iterrows():\n",
    "        video_info_dict[row['video_id']] = {\n",
    "            'title': row['title'],\n",
    "            'channel': row['channel'],\n",
    "            'url': row['url']\n",
    "        }\n",
    "    \n",
    "    progress_bar = st.progress(0)\n",
    "    status_text = st.empty()\n",
    "    \n",
    "    for idx, video_id in enumerate(video_ids):\n",
    "        status_text.text(f\"ì˜ìƒ {idx+1}/{len(video_ids)} ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘...\")\n",
    "        progress_bar.progress((idx + 1) / len(video_ids))\n",
    "        \n",
    "        try:\n",
    "            comments = []\n",
    "            next_page_token = None\n",
    "            \n",
    "            while len(comments) < max_comments_per_video:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet,replies\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=min(100, max_comments_per_video - len(comments)),\n",
    "                    pageToken=next_page_token,\n",
    "                    textFormat=\"plainText\",\n",
    "                    order=\"relevance\"\n",
    "                )\n",
    "                response = request.execute()\n",
    "                \n",
    "                for item in response[\"items\"]:\n",
    "                    top_comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"]\n",
    "                    \n",
    "                    comment_info = {\n",
    "                        \"comment_id\": item[\"snippet\"][\"topLevelComment\"][\"id\"],\n",
    "                        \"video_id\": video_id,\n",
    "                        \"author\": top_comment[\"authorDisplayName\"],\n",
    "                        \"text\": top_comment[\"textDisplay\"],\n",
    "                        \"like_count\": top_comment[\"likeCount\"],\n",
    "                        \"published_at\": top_comment[\"publishedAt\"],\n",
    "                        \"reply_count\": item[\"snippet\"][\"totalReplyCount\"]\n",
    "                    }\n",
    "                    \n",
    "                    if video_id in video_info_dict:\n",
    "                        comment_info['video_title'] = video_info_dict[video_id]['title']\n",
    "                        comment_info['video_channel'] = video_info_dict[video_id]['channel']\n",
    "                        comment_info['video_url'] = video_info_dict[video_id]['url']\n",
    "                    \n",
    "                    comments.append(comment_info)\n",
    "                \n",
    "                next_page_token = response.get(\"nextPageToken\")\n",
    "                if not next_page_token:\n",
    "                    break\n",
    "                \n",
    "                time.sleep(0.5)\n",
    "            \n",
    "            all_comments.extend(comments)\n",
    "        \n",
    "        except Exception as e:\n",
    "            if \"commentsDisabled\" not in str(e):\n",
    "                st.warning(f\"ì˜ìƒ {video_id} ëŒ“ê¸€ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}\")\n",
    "        \n",
    "        time.sleep(1)\n",
    "    \n",
    "    progress_bar.empty()\n",
    "    status_text.empty()\n",
    "    \n",
    "    comments_df = pd.DataFrame(all_comments)\n",
    "    \n",
    "    return videos_df, comments_df\n",
    "\n",
    "# ì•± ë³¸ë¬¸ (main() ë‚´ìš© top-levelë¡œ ì´ë™, if __name__ ì œê±°)\n",
    "st.title(\"ğŸ¥ YouTube ëŒ“ê¸€ ë¶„ì„ ëŒ€ì‹œë³´ë“œ\")\n",
    "st.markdown(\"---\")\n",
    "\n",
    "# ì‚¬ì´ë“œë°” - ë°ì´í„° ìˆ˜ì§‘/ì—…ë¡œë“œë§Œ\n",
    "st.sidebar.header(\"ğŸ“‚ ë°ì´í„° ì†ŒìŠ¤\")\n",
    "data_source = st.sidebar.radio(\n",
    "    \"ë°ì´í„° ì…ë ¥ ë°©ì‹ ì„ íƒ\",\n",
    "    [\"APIë¡œ ì‹¤ì‹œê°„ ìˆ˜ì§‘\", \"CSV íŒŒì¼ ì—…ë¡œë“œ\"]\n",
    ")\n",
    "\n",
    "videos_df = None\n",
    "comments_df = None\n",
    "\n",
    "if data_source == \"APIë¡œ ì‹¤ì‹œê°„ ìˆ˜ì§‘\":\n",
    "    st.sidebar.subheader(\"ğŸ” ê²€ìƒ‰ ì„¤ì •\")\n",
    "    keyword = st.sidebar.text_input(\"ê²€ìƒ‰ í‚¤ì›Œë“œ\", value=\"K-beauty\")\n",
    "    max_videos = st.sidebar.slider(\"ì˜ìƒ ê°œìˆ˜\", 1, 50, 10)\n",
    "    max_comments = st.sidebar.slider(\"ì˜ìƒë‹¹ ëŒ“ê¸€ ìˆ˜\", 10, 200, 50)\n",
    "    order = st.sidebar.selectbox(\n",
    "        \"ì •ë ¬ ë°©ì‹\",\n",
    "        [\"relevance\", \"date\", \"viewCount\"],\n",
    "        format_func=lambda x: {\"relevance\": \"ê´€ë ¨ì„±ìˆœ\", \"date\": \"ìµœì‹ ìˆœ\", \"viewCount\": \"ì¡°íšŒìˆ˜ìˆœ\"}[x]\n",
    "    )\n",
    "    \n",
    "    if st.sidebar.button(\"ğŸš€ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘\"):\n",
    "        with st.spinner(\"ë°ì´í„° ìˆ˜ì§‘ ì¤‘...\"):\n",
    "            videos_df, comments_df = search_and_collect_data(\n",
    "                keyword, max_videos, max_comments, order\n",
    "            )\n",
    "        \n",
    "        if videos_df is not None and comments_df is not None:\n",
    "            st.success(f\"âœ… ì˜ìƒ {len(videos_df)}ê°œ, ëŒ“ê¸€ {len(comments_df)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ!\")\n",
    "            \n",
    "            # ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸ì— ì €ì¥\n",
    "            st.session_state['videos_df'] = videos_df\n",
    "            st.session_state['comments_df'] = comments_df\n",
    "\n",
    "else:  # CSV íŒŒì¼ ì—…ë¡œë“œ\n",
    "    st.sidebar.subheader(\"ğŸ“¤ íŒŒì¼ ì—…ë¡œë“œ\")\n",
    "    comments_file = st.sidebar.file_uploader(\"ëŒ“ê¸€ CSV íŒŒì¼\", type=['csv'])\n",
    "    videos_file = st.sidebar.file_uploader(\"ì˜ìƒ CSV íŒŒì¼ (ì„ íƒ)\", type=['csv'])\n",
    "    \n",
    "    if comments_file:\n",
    "        comments_df = pd.read_csv(comments_file)\n",
    "        st.session_state['comments_df'] = comments_df\n",
    "        st.sidebar.success(f\"âœ… ëŒ“ê¸€ {len(comments_df)}ê°œ ë¡œë“œ\")\n",
    "    \n",
    "    if videos_file:\n",
    "        videos_df = pd.read_csv(videos_file)\n",
    "        st.session_state['videos_df'] = videos_df\n",
    "        st.sidebar.success(f\"âœ… ì˜ìƒ {len(videos_df)}ê°œ ë¡œë“œ\")\n",
    "\n",
    "# ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸ì—ì„œ ë°ì´í„° ë¡œë“œ\n",
    "if 'comments_df' in st.session_state:\n",
    "    comments_df = st.session_state['comments_df']\n",
    "if 'videos_df' in st.session_state:\n",
    "    videos_df = st.session_state['videos_df']\n",
    "\n",
    "# ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì•ˆë‚´ ë©”ì‹œì§€\n",
    "if comments_df is None or comments_df.empty:\n",
    "    st.info(\"ğŸ‘† ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê±°ë‚˜ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.\")\n",
    "else:\n",
    "    # ê¸°ë³¸ í†µê³„\n",
    "    st.header(\"ğŸ“ˆ ê¸°ë³¸ í†µê³„\")\n",
    "    col1, col2, col3, col4 = st.columns(4)\n",
    "    \n",
    "    with col1:\n",
    "        st.metric(\"ì´ ëŒ“ê¸€ ìˆ˜\", f\"{len(comments_df):,}\")\n",
    "    with col2:\n",
    "        st.metric(\"í‰ê·  ì¢‹ì•„ìš”\", f\"{comments_df['like_count'].mean():.1f}\")\n",
    "    with col3:\n",
    "        st.metric(\"ì´ ì¢‹ì•„ìš”\", f\"{comments_df['like_count'].sum():,}\")\n",
    "    with col4:\n",
    "        if videos_df is not None:\n",
    "            st.metric(\"ë¶„ì„ ì˜ìƒ ìˆ˜\", f\"{len(videos_df)}\")\n",
    "    \n",
    "    st.markdown(\"---\")\n",
    "    \n",
    "    # íƒ­ìœ¼ë¡œ ë¶„ì„ ëª¨ë“œ êµ¬ë¶„\n",
    "    tabs = st.tabs([\n",
    "        \"â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œ\",\n",
    "        \"ğŸ“Š í‚¤ì›Œë“œ ë¹ˆë„\",\n",
    "        \"ğŸ˜ŠğŸ˜¢ ê°ì„± ë¶„ì„\",\n",
    "        \"ğŸ“ˆ ì‹œê°„ íŠ¸ë Œë“œ\",\n",
    "        \"ğŸ”— ë™ì‹œì¶œí˜„\",\n",
    "        \"ğŸ¬ í† í”½ ë¹„êµ\",\n",
    "        \"ğŸ“‹ ì›ë³¸ ë°ì´í„°\"\n",
    "    ])\n",
    "    \n",
    "    analyzer = YouTubeCommentAnalyzer(comments_df, videos_df)\n",
    "    \n",
    "    # íƒ­ 1: ì›Œë“œí´ë¼ìš°ë“œ\n",
    "    with tabs[0]:\n",
    "        st.header(\"â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œ\")\n",
    "        if st.button(\"ğŸ” ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±\", key=\"btn_wordcloud\"):\n",
    "            with st.spinner(\"ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì¤‘...\"):\n",
    "                fig = analyzer.wordcloud()\n",
    "                st.pyplot(fig)\n",
    "        else:\n",
    "            st.info(\"ğŸ‘† ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•˜ì„¸ìš”.\")\n",
    "    \n",
    "    # íƒ­ 2: í‚¤ì›Œë“œ ë¹ˆë„\n",
    "    with tabs[1]:\n",
    "        st.header(\"ğŸ“Š í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„\")\n",
    "        top_n = st.slider(\"í‘œì‹œí•  í‚¤ì›Œë“œ ê°œìˆ˜\", 10, 50, 20, key=\"keyword_top_n\")\n",
    "        \n",
    "        if st.button(\"ğŸ” í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„\", key=\"btn_keyword\"):\n",
    "            with st.spinner(\"í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„ ì¤‘...\"):\n",
    "                fig, freq_df = analyzer.keyword_frequency(top_n=top_n)\n",
    "                st.pyplot(fig)\n",
    "                \n",
    "                st.subheader(\"ğŸ“‹ í‚¤ì›Œë“œ ë°ì´í„°\")\n",
    "                st.dataframe(freq_df, use_container_width=True)\n",
    "                \n",
    "                csv = freq_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')\n",
    "                st.download_button(\n",
    "                    \"ğŸ’¾ CSV ë‹¤ìš´ë¡œë“œ\",\n",
    "                    csv,\n",
    "                    \"keyword_frequency.csv\",\n",
    "                    \"text/csv\",\n",
    "                    key='download-keyword-csv'\n",
    "                )\n",
    "        else:\n",
    "            st.info(\"ğŸ‘† ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ í‚¤ì›Œë“œ ë¹ˆë„ë¥¼ ë¶„ì„í•˜ì„¸ìš”.\")\n",
    "    \n",
    "    # íƒ­ 3: ê°ì„± ë¶„ì„\n",
    "    with tabs[2]:\n",
    "        st.header(\"ğŸ˜ŠğŸ˜¢ ê°ì„± ë¶„ì„\")\n",
    "        \n",
    "        if st.button(\"ğŸ” ê°ì„± ë¶„ì„ ì‹¤í–‰\", key=\"btn_sentiment\"):\n",
    "            with st.spinner(\"ê°ì„± ë¶„ì„ ì¤‘...\"):\n",
    "                fig, sentiment_counts, sentiment_df = analyzer.sentiment_keywords()\n",
    "                st.pyplot(fig)\n",
    "                \n",
    "                col1, col2, col3 = st.columns(3)\n",
    "                for idx, (sentiment, count) in enumerate(sentiment_counts.items()):\n",
    "                    with [col1, col2, col3][idx]:\n",
    "                        st.metric(sentiment, f\"{count:,}ê°œ\")\n",
    "                \n",
    "                st.subheader(\"ğŸ“‹ ê°ì„± ë¶„ë¥˜ ë°ì´í„°\")\n",
    "                st.dataframe(sentiment_df.head(100), use_container_width=True)\n",
    "                \n",
    "                csv = sentiment_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')\n",
    "                st.download_button(\n",
    "                    \"ğŸ’¾ CSV ë‹¤ìš´ë¡œë“œ\",\n",
    "                    csv,\n",
    "                    \"sentiment_analysis.csv\",\n",
    "                    \"text/csv\",\n",
    "                    key='download-sentiment-csv'\n",
    "                )\n",
    "        else:\n",
    "            st.info(\"ğŸ‘† ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ê°ì„± ë¶„ì„ì„ ì‹¤í–‰í•˜ì„¸ìš”.\")\n",
    "    \n",
    "    # íƒ­ 4: ì‹œê°„ íŠ¸ë Œë“œ\n",
    "    with tabs[3]:\n",
    "        st.header(\"ğŸ“ˆ ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„\")\n",
    "        interval = st.radio(\"ì‹œê°„ ê°„ê²©\", [\"D (ì¼)\", \"W (ì£¼)\", \"M (ì›”)\"], horizontal=True, key=\"time_interval\")\n",
    "        interval_code = interval.split()[0]\n",
    "        \n",
    "        if st.button(\"ğŸ” ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„\", key=\"btn_time\"):\n",
    "            with st.spinner(\"ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„ ì¤‘...\"):\n",
    "                fig, trend_df = analyzer.time_trend(interval=interval_code)\n",
    "                if fig:\n",
    "                    st.pyplot(fig)\n",
    "                    \n",
    "                    st.subheader(\"ğŸ“‹ íŠ¸ë Œë“œ ë°ì´í„°\")\n",
    "                    st.dataframe(trend_df, use_container_width=True)\n",
    "                    \n",
    "                    csv = trend_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')\n",
    "                    st.download_button(\n",
    "                        \"ğŸ’¾ CSV ë‹¤ìš´ë¡œë“œ\",\n",
    "                        csv,\n",
    "                        \"time_trend.csv\",\n",
    "                        \"text/csv\",\n",
    "                        key='download-trend-csv'\n",
    "                    )\n",
    "                else:\n",
    "                    st.warning(\"published_at ì»¬ëŸ¼ì´ ì—†ì–´ ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        else:\n",
    "            st.info(\"ğŸ‘† ì‹œê°„ ê°„ê²©ì„ ì„ íƒí•˜ê³  ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ë¶„ì„í•˜ì„¸ìš”.\")\n",
    "    \n",
    "    # íƒ­ 5: ë™ì‹œì¶œí˜„\n",
    "    with tabs[4]:\n",
    "        st.header(\"ğŸ”— í‚¤ì›Œë“œ ë™ì‹œì¶œí˜„ ë¶„ì„\")\n",
    "        cooc_n = st.slider(\"ë¶„ì„í•  í‚¤ì›Œë“œ ê°œìˆ˜\", 5, 20, 15, key=\"cooc_n\")\n",
    "        \n",
    "        if st.button(\"ğŸ” ë™ì‹œì¶œí˜„ ë¶„ì„\", key=\"btn_cooc\"):\n",
    "            with st.spinner(\"ë™ì‹œì¶œí˜„ ë¶„ì„ ì¤‘...\"):\n",
    "                fig, cooc_matrix = analyzer.cooccurrence(top_n=cooc_n)\n",
    "                st.pyplot(fig)\n",
    "                \n",
    "                st.subheader(\"ğŸ“‹ ë™ì‹œì¶œí˜„ ë§¤íŠ¸ë¦­ìŠ¤\")\n",
    "                st.dataframe(cooc_matrix, use_container_width=True)\n",
    "                \n",
    "                csv = cooc_matrix.to_csv(encoding='utf-8-sig').encode('utf-8-sig')\n",
    "                st.download_button(\n",
    "                    \"ğŸ’¾ CSV ë‹¤ìš´ë¡œë“œ\",\n",
    "                    csv,\n",
    "                    \"cooccurrence_matrix.csv\",\n",
    "                    \"text/csv\",\n",
    "                    key='download-cooc-csv'\n",
    "                )\n",
    "        else:\n",
    "            st.info(\"ğŸ‘† í‚¤ì›Œë“œ ê°œìˆ˜ë¥¼ ì„ íƒí•˜ê³  ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ë¶„ì„í•˜ì„¸ìš”.\")\n",
    "    \n",
    "    # íƒ­ 6: í† í”½ ë¹„êµ\n",
    "    with tabs[5]:\n",
    "        st.header(\"ğŸ¬ ì˜ìƒë³„ í† í”½ ë¹„êµ\")\n",
    "        \n",
    "        if st.button(\"ğŸ” í† í”½ ë¹„êµ ë¶„ì„\", key=\"btn_topic\"):\n",
    "            with st.spinner(\"í† í”½ ë¹„êµ ë¶„ì„ ì¤‘...\"):\n",
    "                fig, comparison_df = analyzer.topic_comparison()\n",
    "                if fig:\n",
    "                    st.pyplot(fig)\n",
    "                    \n",
    "                    st.subheader(\"ğŸ“‹ í† í”½ ë¹„êµ ë°ì´í„°\")\n",
    "                    st.dataframe(comparison_df, use_container_width=True)\n",
    "                    \n",
    "                    csv = comparison_df.to_csv(encoding='utf-8-sig').encode('utf-8-sig')\n",
    "                    st.download_button(\n",
    "                        \"ğŸ’¾ CSV ë‹¤ìš´ë¡œë“œ\",\n",
    "                        csv,\n",
    "                        \"topic_comparison.csv\",\n",
    "                        \"text/csv\",\n",
    "                        key='download-topic-csv'\n",
    "                    )\n",
    "                else:\n",
    "                    st.warning(\"video_title ì»¬ëŸ¼ì´ ì—†ì–´ í† í”½ ë¹„êµ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        else:\n",
    "            st.info(\"ğŸ‘† ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ í† í”½ ë¹„êµë¥¼ ë¶„ì„í•˜ì„¸ìš”.\")\n",
    "    \n",
    "    # íƒ­ 7: ì›ë³¸ ë°ì´í„°\n",
    "    with tabs[6]:\n",
    "        st.header(\"ğŸ“‹ ì›ë³¸ ë°ì´í„°\")\n",
    "        \n",
    "        data_type = st.radio(\"ë°ì´í„° ìœ í˜• ì„ íƒ\", [\"ëŒ“ê¸€ ë°ì´í„°\", \"ì˜ìƒ ë°ì´í„°\"], horizontal=True)\n",
    "        \n",
    "        if data_type == \"ëŒ“ê¸€ ë°ì´í„°\":\n",
    "            st.subheader(\"ğŸ’¬ ëŒ“ê¸€ ë°ì´í„°\")\n",
    "            st.dataframe(comments_df, use_container_width=True, height=600)\n",
    "            \n",
    "            csv = comments_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')\n",
    "            st.download_button(\n",
    "                \"ğŸ’¾ ëŒ“ê¸€ ë°ì´í„° CSV ë‹¤ìš´ë¡œë“œ\",\n",
    "                csv,\n",
    "                \"comments_data.csv\",\n",
    "                \"text/csv\",\n",
    "                key='download-comments-raw'\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            if videos_df is not None and not videos_df.empty:\n",
    "                st.subheader(\"ğŸ¥ ì˜ìƒ ë°ì´í„°\")\n",
    "                st.dataframe(videos_df, use_container_width=True, height=600)\n",
    "                \n",
    "                csv = videos_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')\n",
    "                st.download_button(\n",
    "                    \"ğŸ’¾ ì˜ìƒ ë°ì´í„° CSV ë‹¤ìš´ë¡œë“œ\",\n",
    "                    csv,\n",
    "                    \"videos_data.csv\",\n",
    "                    \"text/csv\",\n",
    "                    key='download-videos-raw'\n",
    "                )\n",
    "            else:\n",
    "                st.warning(\"ì˜ìƒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    # ìƒˆ ì„¹ì…˜: ë³´ê³ ì„œ ìƒì„± (íƒ­ ì•„ë˜ì— ì¶”ê°€)\n",
    "    st.markdown(\"---\")\n",
    "    st.header(\"ğŸ“„ Market Insight Report Generator (flan-t5-base ê¸°ë°˜)\")\n",
    "    st.write(\"ì €ì¥ëœ ë¶„ì„ CSV íŒŒì¼ì„ ê¸°ë°˜ìœ¼ë¡œ ìš”ì•½ ë³´ê³ ì„œë¥¼ ìë™ ìƒì„±í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    available_files = [f for f in os.listdir(SAVE_DIR) if f.endswith(\".csv\")]\n",
    "\n",
    "    if not available_files:\n",
    "        st.warning(\"ë¶„ì„ ê²°ê³¼ CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ìœ„ ë¶„ì„ íƒ­ì—ì„œ ë¶„ì„ì„ ì‹¤í–‰í•˜ê³  CSVë¥¼ SAVE_DIRì— ì €ì¥í•˜ì„¸ìš”.\")\n",
    "    else:\n",
    "        selected_files = st.multiselect(\"ğŸ“‚ ë³´ê³ ì„œì— í¬í•¨í•  íŒŒì¼ ì„ íƒ\", available_files, default=available_files)\n",
    "\n",
    "        if st.button(\"ğŸ§  ë³´ê³ ì„œ ìƒì„±\"):\n",
    "            full_text = \"\"\n",
    "            for f in selected_files:\n",
    "                file_path = os.path.join(SAVE_DIR, f)\n",
    "                try:\n",
    "                    df = pd.read_csv(file_path, encoding=\"utf-8-sig\")\n",
    "                    content = f\"Insight from {f}:\\nRows: {len(df)}\\nStats:\\n{df.describe().to_string()}\\n\\nSample:\\n{df.head(5).to_string(index=False)}\\n\\n\"\n",
    "                    full_text += content\n",
    "                except Exception as e:\n",
    "                    st.error(f\"íŒŒì¼ {f} ì˜¤ë¥˜: {e}\")\n",
    "                    continue\n",
    "\n",
    "            if full_text:\n",
    "                with st.spinner(\"ëª¨ë¸ì´ ë³´ê³ ì„œë¥¼ ìƒì„± ì¤‘...\"):\n",
    "                    summary = summarize_text(full_text, tokenizer, model, max_input=1024, max_output=500)\n",
    "\n",
    "                st.subheader(\"ğŸ“ˆ AI ìƒì„± ë³´ê³ ì„œ\")\n",
    "                st.text_area(\"ìš”ì•½ ê²°ê³¼\", summary, height=400)\n",
    "\n",
    "                st.download_button(\n",
    "                    \"ğŸ’¾ ë³´ê³ ì„œ ì €ì¥\",\n",
    "                    summary.encode(\"utf-8-sig\"),\n",
    "                    \"Market_Insight_Report.txt\",\n",
    "                    \"text/plain\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6dfcf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 16:22:26.936 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:26.937 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:26.938 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:26.938 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:27.451 Thread 'Thread-4': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:27.478 Thread 'Thread-4': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:27.479 Thread 'Thread-4': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "917b076b30a944a8b282a7b30bcdc10d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\gpt_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--mrm8488--t5-base-finetuned-common_gen. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a2235d979e3470fb3b456016fa57f4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e983043f08354380975065cd6ba5e9f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd823814bfba401ba210cad7add7ffb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "c:\\Users\\Admin\\miniconda3\\envs\\gpt_env\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:2263: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44e12206d9e84bfdbbfa89791ecc208d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.19G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 16:22:54.808 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:54.809 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-03 16:22:54.809 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead  # WithLMHead ì‚¬ìš© (ëª¨ë¸ ì¹´ë“œ ì¶”ì²œ)\n",
    "\n",
    "@st.cache_resource\n",
    "def load_common_gen_model():\n",
    "    \"\"\"Hugging Face mrm8488/t5-base-finetuned-common_gen ëª¨ë¸ ë¡œë“œ\"\"\"\n",
    "    model_name = \"mrm8488/t5-base-finetuned-common_gen\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelWithLMHead.from_pretrained(model_name)\n",
    "    if torch.cuda.is_available():\n",
    "        model.half()  # FP16ìœ¼ë¡œ VRAM ì ˆì•½\n",
    "        model.to(\"cuda\")\n",
    "    return tokenizer, model\n",
    "\n",
    "tokenizer, model = load_common_gen_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d21bdf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
