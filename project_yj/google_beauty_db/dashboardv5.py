import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from datetime import datetime
import re
from wordcloud import WordCloud
import io
from googleapiclient.discovery import build
from dotenv import load_dotenv 
import os
import time
import requests 

# ========================================
# Streamlit ê¸°ë³¸ ì„¤ì • ë° AI ì„¤ì •
# ========================================

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="YouTube ëŒ“ê¸€ ë¶„ì„ ëŒ€ì‹œë³´ë“œ",
    page_icon="ğŸ¥",
    layout="wide"
)

# í•œê¸€ í°íŠ¸ ì„¤ì • (ì‹œìŠ¤í…œì— 'Malgun Gothic'ì´ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•¨)
plt.rcParams['font.family'] = 'Malgun Gothic' 
plt.rcParams['axes.unicode_minus'] = False

SAVE_DIR = "analysis_results"
os.makedirs(SAVE_DIR, exist_ok=True)


def generate_openai_report(keywords, user_focus_prompt, api_key, model_name="gpt-4o"):
    """ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ì™€ ë¶„ì„ ë°ì´í„°ë¥¼ ì¡°í•©í•˜ì—¬ OpenAI APIë¥¼ ì´ìš©í•œ ì¼ë°˜ ë³´ê³ ì„œ ë¬¸ì¥ ìƒì„± í•¨ìˆ˜ (í•­ëª©ë‹¹ ìµœëŒ€ 5ë¬¸ì¥ ì œí•œ)"""
    
    if not api_key:
        return "Error: OpenAI API Key is missing. Please set the OPENAI_API_KEY in the .env file."

    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸: ê¸¸ì´ë¥¼ 'ìµœëŒ€ 5ë¬¸ì¥'ìœ¼ë¡œ ì œí•œí•˜ê³  ë¶„ì„ê°€ ì—­í•  ê°•ì¡°
    system_prompt = (
        "You are a professional YouTube Market Analyst. "
        "Your task is to analyze the provided raw data summary or statistical analysis "
        "and generate a comprehensive, insightful, and professional English summary **limited to a maximum of 5 detailed sentences**. "
        "The summary must strictly address the User Focus prompt, covering key trends, sentiment drivers, quantitative findings, and strategic implications. "
        "If positive or negative comments are provided in the sample, use them as evidence to explain the sentiment drivers. "
        "Do not use markdown headers or lists. Just provide the summary text."
    )
    
    # User Prompt: ì‚¬ìš©ì ì…ë ¥ê³¼ ë¶„ì„ ë°ì´í„°ë¥¼ í•©ì³ì„œ ì „ë‹¬
    full_user_prompt = (
        f"**User Focus:** {user_focus_prompt}\n\n"
        f"**Analysis Data for Context:** {keywords}\n\n"
        "Generate the insightful report summary in English, focusing on the User Focus above."
    )


    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}",
    }

    # API Payload (5ë¬¸ì¥ ë¶„ëŸ‰ì— ë§ê²Œ í† í° ì œí•œ)
    payload = {
        "model": model_name,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": full_user_prompt}
        ],
        "max_tokens": 250, 
        "temperature": 0.3, 
    }

    try:
        response = requests.post("https://api.openai.com/v1/chat/completions", headers=headers, json=payload, timeout=40)
        response.raise_for_status() 
        
        result = response.json()
        
        summary = result['choices'][0]['message']['content'].strip()
        return summary

    except requests.exceptions.HTTPError as errh:
        return f"HTTP Error: {errh}. Check if your API key is valid and the model name is correct. (Status: {response.status_code})"
    except requests.exceptions.ConnectionError as errc:
        return f"Error Connecting: {errc}"
    except requests.exceptions.Timeout as errt:
        return f"Timeout Error: {errt}"
    except requests.exceptions.RequestException as err:
        return f"An Unexpected Error: {err}"
    except Exception as e:
        return f"API Error occurred: {e}. Check response structure."


def generate_executive_report(keywords, user_exec_prompt, api_key, model_name="gpt-4o"):
    """ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ì™€ ë¶„ì„ ë°ì´í„°ë¥¼ ì¡°í•©í•˜ì—¬ OpenAI APIë¥¼ ì´ìš©í•œ ì„ì›ì§„ ë³´ê³ ì„œ ìƒì„± (êµ­ë¬¸+ì˜ë¬¸ ë¶„ë¦¬)"""
    
    if not api_key:
        return "Error: OpenAI API Key is missing."

    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸: ì„ì›ì§„ ë³´ê³ ì„œ í˜•ì‹ ìš”ì²­
    system_prompt = (
        "You are a Senior Executive Market Analyst specializing in YouTube Trends. "
        "Your task is to analyze the provided data and generate a highly summarized, actionable **Executive Report** based on the user's focus. "
        "The response MUST contain two clearly separated sections: **English Summary** (5-7 sentences) and its **Korean Summary** (5-7 sentences). "
        "The output format MUST strictly follow the structure: 'English Summary: [English Text] Korean Summary: [Korean Text]'. "
        "Focus on key insights, strategic implications, and high-level trends. "
    )
    
    # User Prompt: ì‚¬ìš©ì ì…ë ¥ê³¼ ë¶„ì„ ë°ì´í„°ë¥¼ í•©ì³ì„œ ì „ë‹¬
    full_user_exec_prompt = (
        f"**User Focus for Executive Report:** {user_exec_prompt}\n\n"
        f"**Analysis Data for Context:** {keywords}\n\n"
        "Generate the Executive Report in the required dual-language format."
    )

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}",
    }

    # API Payload
    payload = {
        "model": model_name,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": full_user_exec_prompt}
        ],
        "max_tokens": 800, 
        "temperature": 0.3, 
    }

    try:
        response = requests.post("https://api.openai.com/v1/chat/completions", headers=headers, json=payload, timeout=50)
        response.raise_for_status()
        result = response.json()
        summary = result['choices'][0]['message']['content'].strip()
        
        # ê²°ê³¼ íŒŒì‹±
        eng_match = re.search(r'English Summary:\s*(.*?)\s*Korean Summary:', summary, re.DOTALL)
        kor_match = re.search(r'Korean Summary:\s*(.*)', summary, re.DOTALL)

        english_summary = eng_match.group(1).strip() if eng_match else "Parsing Error: English Summary not found."
        korean_summary = kor_match.group(1).strip() if kor_match else "Parsing Error: Korean Summary not found."

        return english_summary, korean_summary

    except Exception as e:
        error_msg = f"API Error occurred: {e}. Check API key or rate limits."
        return error_msg, error_msg

# ========================================
# ë¶„ì„ í´ë˜ìŠ¤ (YouTubeCommentAnalyzer)
# ========================================

class YouTubeCommentAnalyzer:
    """YouTube ëŒ“ê¸€ ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self, comments_df, videos_df=None):
        self.comments_df = comments_df.copy()
        self.videos_df = videos_df.copy() if videos_df is not None else None
        
        # ë‚ ì§œ ì»¬ëŸ¼ ë³€í™˜
        if 'published_at' in self.comments_df.columns:
            self.comments_df['published_at'] = pd.to_datetime(self.comments_df['published_at'])
    
    
    def preprocess_text(self, text):
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        if pd.isna(text):
            return ""
        
        text = str(text).lower()
        text = re.sub(r'http\S+|www\S+', '', text)
        text = re.sub(r'[^ê°€-í£a-z0-9\s]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    
    def extract_keywords(self, min_length=2, top_n=50):
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        all_text = ' '.join(self.comments_df['text'].apply(self.preprocess_text))
        words = all_text.split()
        words = [w for w in words if len(w) >= min_length]
        
        stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
                     'of', 'is', 'are', 'was', 'were', 'been', 'be', 'have', 'has', 'had',
                     'ê·¸', 'ì´', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë“¤', 'ë°', 'ë˜í•œ', 'í•˜ë‹¤', 'ìˆë‹¤', 'ë˜ë‹¤',
                     'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ê·¸ëŸ°', 'ì´ëŸ°', 'ì €ëŸ°'}
        
        words = [w for w in words if w not in stopwords]
        word_freq = Counter(words)
        
        return word_freq.most_common(top_n)
    
    
    def wordcloud(self, width=1200, height=800):
        """ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±"""
        all_text = ' '.join(self.comments_df['text'].apply(self.preprocess_text))
        
        wordcloud = WordCloud(
            # font_path='malgun.ttf', 
            width=width,
            height=height,
            background_color='white',
            max_words=100,
            relative_scaling=0.3,
            colormap='viridis'
        ).generate(all_text)
        
        fig, ax = plt.subplots(figsize=(15, 10))
        ax.imshow(wordcloud, interpolation='bilinear')
        ax.axis('off')
        ax.set_title('ëŒ“ê¸€ ì›Œë“œí´ë¼ìš°ë“œ', fontsize=20, pad=20)
        plt.tight_layout()
        
        return fig
    
    
    def keyword_frequency(self, top_n=20):
        """í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„"""
        keywords = self.extract_keywords(top_n=top_n)
        words, counts = zip(*keywords)
        
        fig, ax = plt.subplots(figsize=(12, 8))
        ax.barh(range(len(words)), counts, color='skyblue')
        ax.set_yticks(range(len(words)))
        ax.set_yticklabels(words)
        ax.set_xlabel('ë¹ˆë„', fontsize=12)
        ax.set_title(f'ìƒìœ„ {top_n}ê°œ í‚¤ì›Œë“œ ë¹ˆë„', fontsize=16, pad=20)
        ax.invert_yaxis()
        plt.tight_layout()
        
        # CSV ì¶œë ¥ì„ ìœ„í•´ ì»¬ëŸ¼ëª…ì„ ì˜ì–´ë¡œ ë³€ê²½
        freq_df = pd.DataFrame(keywords, columns=['Keyword', 'Frequency'])
        
        return fig, freq_df
    
    
    def sentiment_keywords(self):
        """ê°ì„± í‚¤ì›Œë“œ ë¶„ì„"""
        positive_words = {
            'ì¢‹ë‹¤', 'ìµœê³ ', 'ëŒ€ë°•', 'ì˜ˆì˜ë‹¤', 'ì´ì˜ë‹¤', 'ë©‹ì§€ë‹¤', 'í›Œë¥­í•˜ë‹¤', 
            'ì™„ë²½', 'ì¢‹ì•„', 'ê°ì‚¬', 'ì‚¬ë‘', 'í–‰ë³µ', 'ì¶”ì²œ', 'êµ¿', 'good', 
            'best', 'love', 'amazing', 'perfect', 'great', 'excellent',
            'ì¢‹ì•„ìš”', 'ì¢‹ë„¤ìš”', 'ë©‹ìˆë‹¤', 'ì•„ë¦„ë‹µë‹¤', 'ìµœê³ ë‹¤', 'ì§±'
        }
        
        negative_words = {
            'ì‹«ë‹¤', 'ë³„ë¡œ', 'ì•ˆì¢‹ë‹¤', 'ë‚˜ì˜ë‹¤', 'ìµœì•…', 'í˜•í¸ì—†ë‹¤',
            'ì‹«ì–´', 'ì‹¤ë§', 'ë³„ë¡œë„¤', 'ì•„ì‰½ë‹¤', 'bad', 'worst', 'hate',
            'ì‹«ì–´ìš”', 'ë³„ë¡œì˜ˆìš”', 'ê·¸ì €ê·¸ë ‡ë‹¤', 'ì§€ë£¨í•˜ë‹¤'
        }
        
        def calculate_sentiment(text):
            text = self.preprocess_text(text)
            words = text.split()
            
            pos_count = sum(1 for w in words if w in positive_words)
            neg_count = sum(1 for w in words if w in negative_words)
            
            return pos_count, neg_count
        
        # ë‚´ë¶€ ì»¬ëŸ¼ëª… ë³€ê²½
        self.comments_df[['PositiveCount', 'NegativeCount']] = \
            self.comments_df['text'].apply(lambda x: pd.Series(calculate_sentiment(x)))
        
        def classify_sentiment(row):
            if row['PositiveCount'] > row['NegativeCount']:
                return 'ê¸ì •'
            elif row['PositiveCount'] < row['NegativeCount']:
                return 'ë¶€ì •'
            else:
                return 'ì¤‘ë¦½'
        
        self.comments_df['sentiment'] = self.comments_df.apply(classify_sentiment, axis=1)
        sentiment_counts = self.comments_df['sentiment'].value_counts()
        
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        
        colors = ['#90EE90', '#FFB6C1', '#D3D3D3']
        
        order = ['ê¸ì •', 'ë¶€ì •', 'ì¤‘ë¦½']
        ordered_counts = sentiment_counts.reindex(order, fill_value=0)
        ordered_counts = ordered_counts[ordered_counts > 0]
        ordered_colors = [c for s, c in zip(order, colors) if s in ordered_counts.index]
        
        axes[0].pie(ordered_counts.values, labels=ordered_counts.index, 
                      autopct='%1.1f%%', colors=ordered_colors, startangle=90)
        axes[0].set_title('ëŒ“ê¸€ ê°ì„± ë¶„í¬', fontsize=14, pad=20)
        
        axes[1].bar(ordered_counts.index, ordered_counts.values, color=ordered_colors)
        axes[1].set_xlabel('ê°ì„±', fontsize=12)
        axes[1].set_ylabel('ëŒ“ê¸€ ìˆ˜', fontsize=12)
        axes[1].set_title('ê°ì„±ë³„ ëŒ“ê¸€ ìˆ˜', fontsize=14, pad=20)
        
        plt.tight_layout()
        
        # CSV ì¶œë ¥ì„ ìœ„í•´ ì»¬ëŸ¼ëª…ì„ ì˜ì–´ë¡œ ë³€ê²½
        sentiment_df = self.comments_df[['text', 'sentiment', 'PositiveCount', 'NegativeCount', 'like_count']].rename(
            columns={'sentiment': 'Sentiment', 'text': 'Text', 'like_count': 'LikeCount'}
        )
        
        return fig, sentiment_counts, sentiment_df
    
    
    def time_trend(self, interval='D'):
        """ì‹œê°„ëŒ€ë³„ íŠ¸ë Œë“œ ë¶„ì„"""
        if 'published_at' not in self.comments_df.columns:
            return None, None
        
        time_counts = self.comments_df.set_index('published_at').resample(interval).size()
        time_likes = self.comments_df.set_index('published_at')['like_count'].resample(interval).sum()
        
        fig, axes = plt.subplots(2, 1, figsize=(14, 10))
        
        axes[0].plot(time_counts.index, time_counts.values, marker='o', linewidth=2)
        axes[0].set_xlabel('ë‚ ì§œ', fontsize=12)
        axes[0].set_ylabel('ëŒ“ê¸€ ìˆ˜', fontsize=12)
        axes[0].set_title('ì‹œê°„ëŒ€ë³„ ëŒ“ê¸€ ìˆ˜ ì¶”ì´', fontsize=14, pad=20)
        axes[0].grid(True, alpha=0.3)
        
        axes[1].plot(time_likes.index, time_likes.values, marker='o', 
                      color='coral', linewidth=2)
        axes[1].set_xlabel('ë‚ ì§œ', fontsize=12)
        axes[1].set_ylabel('ì¢‹ì•„ìš” ìˆ˜', fontsize=12)
        axes[1].set_title('ì‹œê°„ëŒ€ë³„ ì¢‹ì•„ìš” ìˆ˜ ì¶”ì´', fontsize=14, pad=20)
        axes[1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # CSV ì¶œë ¥ì„ ìœ„í•´ ì»¬ëŸ¼ëª…ì„ ì˜ì–´ë¡œ ë³€ê²½
        trend_df = pd.DataFrame({
            'Date': time_counts.index,
            'CommentCount': time_counts.values,
            'LikeCount': time_likes.values
        })
        
        return fig, trend_df
    
    
    def cooccurrence(self, top_n=15):
        """í‚¤ì›Œë“œ ë™ì‹œì¶œí˜„ ë¶„ì„"""
        top_keywords = [word for word, _ in self.extract_keywords(top_n=top_n)]
        cooc_matrix = pd.DataFrame(0, index=top_keywords, columns=top_keywords)
        
        for text in self.comments_df['text']:
            text = self.preprocess_text(text)
            words = set(text.split())
            
            for word1 in top_keywords:
                if word1 in words:
                    for word2 in top_keywords:
                        if word2 in words:
                            cooc_matrix.loc[word1, word2] += 1
        
        fig, ax = plt.subplots(figsize=(14, 12))
        sns.heatmap(cooc_matrix, annot=True, fmt='d', cmap='YlOrRd', 
                      cbar_kws={'label': 'ë™ì‹œì¶œí˜„ ë¹ˆë„'}, ax=ax)
        ax.set_title(f'ìƒìœ„ {top_n}ê°œ í‚¤ì›Œë“œ ë™ì‹œì¶œí˜„ ë¶„ì„', fontsize=16, pad=20)
        ax.set_xlabel('Keyword2', fontsize=12) 
        ax.set_ylabel('Keyword1', fontsize=12) 
        plt.tight_layout()
        
        # CSV ì¶œë ¥ì„ ìœ„í•´ ì»¬ëŸ¼ëª…ì„ ì˜ì–´ë¡œ ë³€ê²½
        cooc_matrix.index.name = 'Keyword1'
        cooc_matrix.columns.name = 'Keyword2'
        
        return fig, cooc_matrix
    
    
    def topic_comparison(self):
        """ì˜ìƒë³„ í† í”½ ë¹„êµ ë¶„ì„"""
        if 'video_title' not in self.comments_df.columns:
            return None, None
        
        video_keywords = {}
        
        for video_title in self.comments_df['video_title'].unique()[:10]:
            video_comments = self.comments_df[
                self.comments_df['video_title'] == video_title
            ]['text']
            
            all_text = ' '.join(video_comments.apply(self.preprocess_text))
            words = all_text.split()
            words = [w for w in words if len(w) >= 2]
            
            word_freq = Counter(words)
            top_words = [word for word, _ in word_freq.most_common(5)]
            
            video_keywords[video_title[:30] + '...'] = top_words
        
        comparison_df = pd.DataFrame(video_keywords).T
        # CSV ì¶œë ¥ì„ ìœ„í•´ ì»¬ëŸ¼ëª…ì„ ì˜ì–´ë¡œ ë³€ê²½
        comparison_df.columns = [f'Keyword{i+1}' for i in range(comparison_df.shape[1])]
        comparison_df.index.name = 'VideoTitle' 
        
        fig, ax = plt.subplots(figsize=(14, 8))
        ax.axis('tight')
        ax.axis('off')
        
        cell_text = comparison_df.reset_index().values.tolist() 
        col_labels = ['VideoTitle'] + comparison_df.columns.tolist()
        
        table = ax.table(cellText=cell_text,
                          rowLabels=None, 
                          colLabels=col_labels,
                          cellLoc='center',
                          loc='center',
                          bbox=[0, 0, 1, 1])
        
        table.auto_set_font_size(False)
        table.set_fontsize(9)
        table.scale(1, 2)
        
        # í—¤ë” ì…€ ìŠ¤íƒ€ì¼ë§
        for i in range(len(col_labels)):
            table[(0, i)].set_facecolor('#4CAF50')
            table[(0, i)].set_text_props(weight='bold', color='white')

        # ë¡œìš° ë ˆì´ë¸” ìŠ¤íƒ€ì¼ë§ (ì²« ë²ˆì§¸ ë°ì´í„° ì»¬ëŸ¼)
        for i in range(len(comparison_df)):
            table[(i+1, 0)].set_facecolor('#E8F5E9') 
            table[(i+1, 0)].set_text_props(weight='bold')
            
        plt.title('ì˜ìƒë³„ ì£¼ìš” í‚¤ì›Œë“œ ë¹„êµ', fontsize=16, pad=20)
        
        return fig, comparison_df


def search_and_collect_data(keyword, max_videos, max_comments_per_video, order):
    """YouTube APIë¥¼ í†µí•œ ë°ì´í„° ìˆ˜ì§‘"""
    load_dotenv()
    api_key = os.getenv("YOUTUBE_API_KEY")
    
    if not api_key:
        st.error("YouTube API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        return None, None
    
    youtube = build("youtube", "v3", developerKey=api_key)
    
    # ì˜ìƒ ê²€ìƒ‰
    try:
        search_response = youtube.search().list(
            q=keyword,
            part="snippet",
            maxResults=min(max_videos, 50),
            type="video",
            order=order,
            regionCode="KR"
        ).execute()
        
        video_ids = [item["id"]["videoId"] for item in search_response["items"]]
    except Exception as e:
        st.error(f"ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return None, None
    
    # ì˜ìƒ ìƒì„¸ ì •ë³´
    videos_data = []
    try:
        for i in range(0, len(video_ids), 50):
            batch_ids = video_ids[i:i+50]
            video_response = youtube.videos().list(
                part="snippet,statistics,contentDetails",
                id=",".join(batch_ids)
            ).execute()
            
            for item in video_response["items"]:
                video_info = {
                    "video_id": item["id"],
                    "title": item["snippet"]["title"],
                    "channel": item["snippet"]["channelTitle"],
                    "published_at": item["snippet"]["publishedAt"],
                    "description": item["snippet"]["description"],
                    "view_count": int(item["statistics"].get("viewCount", 0)),
                    "like_count": int(item["statistics"].get("likeCount", 0)),
                    "comment_count": int(item["statistics"].get("commentCount", 0)),
                    "duration": item["contentDetails"]["duration"],
                    "tags": ", ".join(item["snippet"].get("tags", [])),
                    "url": f"https://www.youtube.com/watch?v={item['id']}"
                }
                videos_data.append(video_info)
    except Exception as e:
        st.error(f"ì˜ìƒ ì •ë³´ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
    
    videos_df = pd.DataFrame(videos_data)
    
    # ëŒ“ê¸€ ìˆ˜ì§‘
    all_comments = []
    video_info_dict = {}
    for _, row in videos_df.iterrows():
        video_info_dict[row['video_id']] = {
            'title': row['title'],
            'channel': row['channel'],
            'url': row['url']
        }
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for idx, video_id in enumerate(video_ids):
        status_text.text(f"ì˜ìƒ {idx+1}/{len(video_ids)} ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘...")
        progress_bar.progress((idx + 1) / len(video_ids))
        
        try:
            comments = []
            next_page_token = None
            
            while len(comments) < max_comments_per_video:
                request = youtube.commentThreads().list(
                    part="snippet,replies",
                    videoId=video_id,
                    maxResults=min(100, max_comments_per_video - len(comments)),
                    pageToken=next_page_token,
                    textFormat="plainText",
                    order="relevance"
                )
                response = request.execute()
                
                for item in response["items"]:
                    top_comment = item["snippet"]["topLevelComment"]["snippet"]
                    
                    comment_info = {
                        "comment_id": item["snippet"]["topLevelComment"]["id"],
                        "video_id": video_id,
                        "author": top_comment["authorDisplayName"],
                        "text": top_comment["textDisplay"],
                        "like_count": top_comment["likeCount"],
                        "published_at": top_comment["publishedAt"],
                        "reply_count": item["snippet"]["totalReplyCount"]
                    }
                    
                    if video_id in video_info_dict:
                        comment_info['video_title'] = video_info_dict[video_id]['title']
                        comment_info['video_channel'] = video_info_dict[video_id]['channel']
                        comment_info['video_url'] = video_info_dict[video_id]['url']
                    
                    comments.append(comment_info)
                
                next_page_token = response.get("nextPageToken")
                if not next_page_token:
                    break
                
                time.sleep(0.5)
            
            all_comments.extend(comments)
        
        except Exception as e:
            if "commentsDisabled" not in str(e):
                st.warning(f"ì˜ìƒ {video_id} ëŒ“ê¸€ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        
        time.sleep(1)
    
    progress_bar.empty()
    status_text.empty()
    
    comments_df = pd.DataFrame(all_comments)
    
    return videos_df, comments_df


# ========================================
# Streamlit ë©”ì¸ ì•±
# ========================================

def main():
    st.title("ğŸ¥ YouTube ëŒ“ê¸€ ë¶„ì„ ëŒ€ì‹œë³´ë“œ")
    st.markdown("---")
    
    # OpenAI API Key ì„¤ì • (.env íŒŒì¼ì—ì„œ ë¡œë“œ)
    load_dotenv()
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    
    # ì‚¬ì´ë“œë°” - ë°ì´í„° ìˆ˜ì§‘/ì—…ë¡œë“œë§Œ
    st.sidebar.header("ğŸ“‚ ë°ì´í„° ì†ŒìŠ¤")
    data_source = st.sidebar.radio(
        "ë°ì´í„° ì…ë ¥ ë°©ì‹ ì„ íƒ",
        ["APIë¡œ ì‹¤ì‹œê°„ ìˆ˜ì§‘", "CSV íŒŒì¼ ì—…ë¡œë“œ"]
    )
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ë° ë¡œë“œ
    if 'videos_df' not in st.session_state:
        st.session_state['videos_df'] = None
    if 'comments_df' not in st.session_state:
        st.session_state['comments_df'] = None
    
    videos_df = st.session_state['videos_df']
    comments_df = st.session_state['comments_df']
    
    if data_source == "APIë¡œ ì‹¤ì‹œê°„ ìˆ˜ì§‘":
        st.sidebar.subheader("ğŸ” ê²€ìƒ‰ ì„¤ì •")
        keyword = st.sidebar.text_input("ê²€ìƒ‰ í‚¤ì›Œë“œ", value="K-beauty")
        max_videos = st.sidebar.slider("ì˜ìƒ ê°œìˆ˜", 1, 50, 10)
        max_comments = st.sidebar.slider("ì˜ìƒë‹¹ ëŒ“ê¸€ ìˆ˜", 10, 200, 50)
        order = st.sidebar.selectbox(
            "ì •ë ¬ ë°©ì‹",
            ["relevance", "date", "viewCount"],
            format_func=lambda x: {"relevance": "ê´€ë ¨ì„±ìˆœ", "date": "ìµœì‹ ìˆœ", "viewCount": "ì¡°íšŒìˆ˜ìˆœ"}[x]
        )
        
        if st.sidebar.button("ğŸš€ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘"):
            with st.spinner("ë°ì´í„° ìˆ˜ì§‘ ì¤‘..."):
                videos_df_new, comments_df_new = search_and_collect_data(
                    keyword, max_videos, max_comments, order
                )
            
            if videos_df_new is not None and comments_df_new is not None and not comments_df_new.empty:
                st.success(f"âœ… ì˜ìƒ {len(videos_df_new)}ê°œ, ëŒ“ê¸€ {len(comments_df_new)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ!")
                
                # ğŸ”´ API ìˆ˜ì§‘ ì§í›„ ì›ë³¸ íŒŒì¼ íƒ€ì„ìŠ¤íƒ¬í”„ ì €ì¥
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                
                # 1. ì˜ìƒ ë°ì´í„° ì €ì¥
                videos_file_name = f"videos_data_{timestamp}.csv"
                videos_file_path = os.path.join(SAVE_DIR, videos_file_name)
                videos_df_new.to_csv(videos_file_path, index=False, encoding='utf-8-sig')
                st.success(f"ğŸ’¾ ì˜ìƒ ë°ì´í„°ê°€ **{videos_file_name}**ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

                # 2. ëŒ“ê¸€ ë°ì´í„° ì €ì¥
                comments_file_name = f"comments_data_{timestamp}.csv"
                comments_file_path = os.path.join(SAVE_DIR, comments_file_name)
                comments_df_new.to_csv(comments_file_path, index=False, encoding='utf-8-sig')
                st.success(f"ğŸ’¾ ëŒ“ê¸€ ë°ì´í„°ê°€ **{comments_file_name}**ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

                # ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸ì— ì €ì¥ (UI ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´ ì¬í• ë‹¹)
                st.session_state['videos_df'] = videos_df_new
                st.session_state['comments_df'] = comments_df_new
                st.rerun() 
            elif comments_df_new is not None and comments_df_new.empty:
                st.warning("ìˆ˜ì§‘ëœ ëŒ“ê¸€ì´ ì—†ìŠµë‹ˆë‹¤. ê²€ìƒ‰ ì¡°ê±´ì´ë‚˜ API ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    
    else: # CSV íŒŒì¼ ì—…ë¡œë“œ
        st.sidebar.subheader("ğŸ“¤ íŒŒì¼ ì—…ë¡œë“œ")
        comments_file = st.sidebar.file_uploader("ëŒ“ê¸€ CSV íŒŒì¼", type=['csv'])
        videos_file = st.sidebar.file_uploader("ì˜ìƒ CSV íŒŒì¼ (ì„ íƒ)", type=['csv'])
        
        if comments_file:
            comments_df = pd.read_csv(comments_file)
            st.session_state['comments_df'] = comments_df
            st.sidebar.success(f"âœ… ëŒ“ê¸€ {len(comments_df)}ê°œ ë¡œë“œ")
        
        if videos_file:
            videos_df = pd.read_csv(videos_file)
            st.session_state['videos_df'] = videos_df
            st.sidebar.success(f"âœ… ì˜ìƒ {len(videos_df)}ê°œ ë¡œë“œ")
        
        if comments_file or videos_file:
            st.rerun()

    # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì•ˆë‚´ ë©”ì‹œì§€
    if comments_df is None or comments_df.empty:
        st.info("ğŸ‘† ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê±°ë‚˜ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        return
    
    # ê¸°ë³¸ í†µê³„
    st.header("ğŸ“ˆ ê¸°ë³¸ í†µê³„")
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("ì´ ëŒ“ê¸€ ìˆ˜", f"{len(comments_df):,}")
    with col2:
        st.metric("í‰ê·  ì¢‹ì•„ìš”", f"{comments_df['like_count'].mean():.1f}")
    with col3:
        st.metric("ì´ ì¢‹ì•„ìš”", f"{comments_df['like_count'].sum():,}")
    with col4:
        if videos_df is not None:
            st.metric("ë¶„ì„ ì˜ìƒ ìˆ˜", f"{len(videos_df)}")
    
    st.markdown("---")
    
    # íƒ­ìœ¼ë¡œ ë¶„ì„ ëª¨ë“œ êµ¬ë¶„ (ì´ 9ê°œ íƒ­ìœ¼ë¡œ ì¬êµ¬ì„±)
    tabs = st.tabs([
        "â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œ",
        "ğŸ“Š í‚¤ì›Œë“œ ë¹ˆë„",
        "ğŸ˜ŠğŸ˜¢ ê°ì„± ë¶„ì„",
        "ğŸ“ˆ ì‹œê°„ íŠ¸ë Œë“œ",
        "ğŸ”— ë™ì‹œì¶œí˜„",
        "ğŸ¬ í† í”½ ë¹„êµ",
        "ğŸ“‹ ì›ë³¸ ë°ì´í„°",
        "ğŸ“„ ì¼ë°˜ ë³´ê³ ì„œ", 
        "ğŸ’¼ ì„ì›ì§„ ë³´ê³ ì„œ" # ğŸ”´ [ì¶”ê°€] ì„ì›ì§„ ë³´ê³ ì„œ íƒ­
    ])
    
    analyzer = YouTubeCommentAnalyzer(comments_df, videos_df)
    
    # íƒ­ 1: ì›Œë“œí´ë¼ìš°ë“œ
    with tabs[0]:
        st.header("â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œ")
        if st.button("ğŸ” ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±", key="btn_wordcloud"):
            with st.spinner("ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì¤‘..."):
                fig = analyzer.wordcloud()
                st.pyplot(fig)
        else:
            st.info("ğŸ‘† ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•˜ì„¸ìš”.")

    # íƒ­ 2: í‚¤ì›Œë“œ ë¹ˆë„ (íƒ€ì„ìŠ¤íƒ¬í”„ ì €ì¥ ì ìš©)
    with tabs[1]:
        st.header("ğŸ“Š í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„")
        top_n = st.slider("í‘œì‹œí•  í‚¤ì›Œë“œ ê°œìˆ˜", 10, 50, 20, key="keyword_top_n")
        
        if st.button("ğŸ” í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„", key="btn_keyword"):
            with st.spinner("í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„ ì¤‘..."):
                fig, freq_df = analyzer.keyword_frequency(top_n=top_n)
                st.pyplot(fig)
                st.session_state['freq_df'] = freq_df 
                
                st.subheader("ğŸ“‹ í‚¤ì›Œë“œ ë°ì´í„° (English Column)")
                st.dataframe(freq_df, use_container_width=True)
                
                # ğŸ”´ [ìˆ˜ì •] íƒ€ì„ìŠ¤íƒ¬í”„ ì €ì¥
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                csv_file_name = f"keyword_frequency_{timestamp}.csv"
                freq_df.to_csv(os.path.join(SAVE_DIR, csv_file_name), index=False, encoding='utf-8-sig')
                st.success(f"ë¶„ì„ ê²°ê³¼ê°€ ì„œë²„ í´ë” `{SAVE_DIR}`ì— **{csv_file_name}**ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

                csv = freq_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
                st.download_button("ğŸ’¾ CSV ë‹¤ìš´ë¡œë“œ", csv, csv_file_name, "text/csv", key='download-keyword-csv')
        else:
            if 'freq_df' in st.session_state:
                freq_df = st.session_state['freq_df']
                st.subheader("ğŸ“‹ ë§ˆì§€ë§‰ ë¶„ì„ ê²°ê³¼ (í‚¤ì›Œë“œ ë°ì´í„° - English Column)")
                st.dataframe(freq_df, use_container_width=True)
            else:
                st.info("ğŸ‘† ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ í‚¤ì›Œë“œ ë¹ˆë„ë¥¼ ë¶„ì„í•˜ì„¸ìš”.")
    
    # íƒ­ 3: ê°ì„± ë¶„ì„ (íƒ€ì„ìŠ¤íƒ¬í”„ ì €ì¥ ì ìš©)
    with tabs[2]:
        st.header("ğŸ˜ŠğŸ˜¢ ê°ì„± ë¶„ì„")
        
        if st.button("ğŸ” ê°ì„± ë¶„ì„ ì‹¤í–‰", key="btn_sentiment"):
            with st.spinner("ê°ì„± ë¶„ì„ ì¤‘..."):
                fig, sentiment_counts, sentiment_df = analyzer.sentiment_keywords()
                st.pyplot(fig)
                st.session_state['sentiment_df'] = sentiment_df 
                
                col1, col2, col3 = st.columns(3)
                for idx, (sentiment, count) in enumerate(sentiment_counts.items()):
                    with [col1, col2, col3][idx]:
                        st.metric(sentiment, f"{count:,}ê°œ")
                
                st.subheader("ğŸ“‹ ê°ì„± ë¶„ë¥˜ ë°ì´í„° (English Column)")
                st.dataframe(sentiment_df.head(100), use_container_width=True)
                
                # ğŸ”´ [ìˆ˜ì •] íƒ€ì„ìŠ¤íƒ¬í”„ ì €ì¥
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                csv_file_name = f"sentiment_analysis_{timestamp}.csv"
                sentiment_df.to_csv(os.path.join(SAVE_DIR, csv_file_name), index=False, encoding='utf-8-sig')
                st.success(f"ë¶„ì„ ê²°ê³¼ê°€ ì„œë²„ í´ë” `{SAVE_DIR}`ì— **{csv_file_name}**ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

                csv = sentiment_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
                st.download_button("ğŸ’¾ CSV ë‹¤ìš´ë¡œë“œ", csv, csv_file_name, "text/csv", key='download-sentiment-csv')
        else:
            if 'sentiment_df' in st.session_state:
                sentiment_df = st.session_state['sentiment_df']
                st.subheader("ğŸ“‹ ë§ˆì§€ë§‰ ë¶„ì„ ê²°ê³¼ (ê°ì„± ë¶„ë¥˜ ë°ì´í„° - English Column)")
                st.dataframe(sentiment_df.head(100), use_container_width=True)
            else:
                st.info("ğŸ‘† ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ê°ì„± ë¶„ì„ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
    
    # íƒ­ 4: ì‹œê°„ íŠ¸ë Œë“œ (íƒ€ì„ìŠ¤íƒ¬í”„ ì €ì¥ ì ìš©)
    with tabs[3]:
        st.header("ğŸ“ˆ ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„")
        interval = st.radio("ì‹œê°„ ê°„ê²©", ["D (ì¼)", "W (ì£¼)", "M (ì›”)"], horizontal=True, key="time_interval")
        interval_code = interval.split()[0]
        
        if st.button("ğŸ” ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„", key="btn_time"):
            with st.spinner("ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„ ì¤‘..."):
                fig, trend_df = analyzer.time_trend(interval=interval_code)
                if fig:
                    st.pyplot(fig)
                    st.session_state['trend_df'] = trend_df 
                    
                    st.subheader("ğŸ“‹ íŠ¸ë Œë“œ ë°ì´í„° (English Column)")
                    st.dataframe(trend_df, use_container_width=True)
                    
                    # ğŸ”´ [ìˆ˜ì •] íƒ€ì„ìŠ¤íƒ¬í”„ ì €ì¥
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    csv_file_name = f"time_trend_{timestamp}.csv"
                    trend_df.to_csv(os.path.join(SAVE_DIR, csv_file_name), index=False, encoding='utf-8-sig')
                    st.success(f"ë¶„ì„ ê²°ê³¼ê°€ ì„œë²„ í´ë” `{SAVE_DIR}`ì— **{csv_file_name}**ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

                    csv = trend_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
                    st.download_button("ğŸ’¾ CSV ë‹¤ìš´ë¡œë“œ", csv, csv_file_name, "text/csv", key='download-trend-csv')
                else:
                    st.warning("published_at ì»¬ëŸ¼ì´ ì—†ì–´ ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            if 'trend_df' in st.session_state:
                trend_df = st.session_state['trend_df']
                st.subheader("ğŸ“‹ ë§ˆì§€ë§‰ ë¶„ì„ ê²°ê³¼ (íŠ¸ë Œë“œ ë°ì´í„° - English Column)")
                st.dataframe(trend_df, use_container_width=True)
            else:
                st.info("ğŸ‘† ì‹œê°„ ê°„ê²©ì„ ì„ íƒí•˜ê³  ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ë¶„ì„í•˜ì„¸ìš”.")
    
    # íƒ­ 5: ë™ì‹œì¶œí˜„ (íƒ€ì„ìŠ¤íƒ¬í”„ ì €ì¥ ì ìš©)
    with tabs[4]:
        st.header("ğŸ”— í‚¤ì›Œë“œ ë™ì‹œì¶œí˜„ ë¶„ì„")
        cooc_n = st.slider("ë¶„ì„í•  í‚¤ì›Œë“œ ê°œìˆ˜", 5, 20, 15, key="cooc_n")
        
        if st.button("ğŸ” ë™ì‹œì¶œí˜„ ë¶„ì„", key="btn_cooc"):
            with st.spinner("ë™ì‹œì¶œí˜„ ë¶„ì„ ì¤‘..."):
                fig, cooc_matrix = analyzer.cooccurrence(top_n=cooc_n)
                st.pyplot(fig)
                st.session_state['cooc_df'] = cooc_matrix 
                
                st.subheader("ğŸ“‹ ë™ì‹œì¶œí˜„ ë§¤íŠ¸ë¦­ìŠ¤ (English Index/Column)")
                st.dataframe(cooc_matrix, use_container_width=True)
                
                # ğŸ”´ [ìˆ˜ì •] íƒ€ì„ìŠ¤íƒ¬í”„ ì €ì¥
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                csv_file_name = f"cooccurrence_matrix_{timestamp}.csv"
                cooc_matrix.to_csv(os.path.join(SAVE_DIR, csv_file_name), encoding='utf-8-sig') 
                st.success(f"ë¶„ì„ ê²°ê³¼ê°€ ì„œë²„ í´ë” `{SAVE_DIR}`ì— **{csv_file_name}**ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                
                csv = cooc_matrix.to_csv(encoding='utf-8-sig').encode('utf-8-sig')
                st.download_button("ğŸ’¾ CSV ë‹¤ìš´ë¡œë“œ", csv, csv_file_name, "text/csv", key='download-cooc-csv')
        else:
            if 'cooc_df' in st.session_state:
                cooc_matrix = st.session_state['cooc_df']
                st.subheader("ğŸ“‹ ë§ˆì§€ë§‰ ë¶„ì„ ê²°ê³¼ (ë™ì‹œì¶œí˜„ ë§¤íŠ¸ë¦­ìŠ¤ - English Index/Column)")
                st.dataframe(cooc_matrix, use_container_width=True)
            else:
                st.info("ğŸ‘† í‚¤ì›Œë“œ ê°œìˆ˜ë¥¼ ì„ íƒí•˜ê³  ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ë¶„ì„í•˜ì„¸ìš”.")
    
    # íƒ­ 6: í† í”½ ë¹„êµ (íƒ€ì„ìŠ¤íƒ¬í”„ ì €ì¥ ì ìš©)
    with tabs[5]:
        st.header("ğŸ¬ ì˜ìƒë³„ í† í”½ ë¹„êµ")
        
        if st.button("ğŸ” í† í”½ ë¹„êµ ë¶„ì„", key="btn_topic"):
            with st.spinner("í† í”½ ë¹„êµ ë¶„ì„ ì¤‘..."):
                fig, comparison_df = analyzer.topic_comparison()
                if fig:
                    st.pyplot(fig)
                    st.session_state['topic_df'] = comparison_df 
                    
                    st.subheader("ğŸ“‹ í† í”½ ë¹„êµ ë°ì´í„° (English Column)")
                    st.dataframe(comparison_df, use_container_width=True)
                    
                    # ğŸ”´ [ìˆ˜ì •] íƒ€ì„ìŠ¤íƒ¬í”„ ì €ì¥
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    csv_file_name = f"topic_comparison_{timestamp}.csv"
                    comparison_df.to_csv(os.path.join(SAVE_DIR, csv_file_name), encoding='utf-8-sig') 
                    st.success(f"ë¶„ì„ ê²°ê³¼ê°€ ì„œë²„ í´ë” `{SAVE_DIR}`ì— **{csv_file_name}**ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

                    csv = comparison_df.to_csv(encoding='utf-8-sig').encode('utf-8-sig')
                    st.download_button("ğŸ’¾ CSV ë‹¤ìš´ë¡œë“œ", csv, csv_file_name, "text/csv", key='download-topic-csv')
                else:
                    st.warning("video_title ì»¬ëŸ¼ì´ ì—†ì–´ í† í”½ ë¹„êµ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            if 'topic_df' in st.session_state:
                comparison_df = st.session_state['topic_df']
                st.subheader("ğŸ“‹ ë§ˆì§€ë§‰ ë¶„ì„ ê²°ê³¼ (í† í”½ ë¹„êµ ë°ì´í„° - English Column)")
                st.dataframe(comparison_df, use_container_width=True)
            else:
                st.info("ğŸ‘† ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ í† í”½ ë¹„êµë¥¼ ë¶„ì„í•˜ì„¸ìš”.")

    # íƒ­ 7: ì›ë³¸ ë°ì´í„° (ë‹¤ìš´ë¡œë“œ íŒŒì¼ì— íƒ€ì„ìŠ¤íƒ¬í”„ ì ìš©)
    with tabs[6]:
        st.header("ğŸ“‹ ì›ë³¸ ë°ì´í„°")
        
        data_type = st.radio("ë°ì´í„° ìœ í˜• ì„ íƒ", ["ëŒ“ê¸€ ë°ì´í„°", "ì˜ìƒ ë°ì´í„°"], horizontal=True)
        
        # ëŒ“ê¸€ ë°ì´í„°
        if data_type == "ëŒ“ê¸€ ë°ì´í„°":
            st.subheader("ğŸ’¬ ëŒ“ê¸€ ë°ì´í„°")
            st.dataframe(comments_df, use_container_width=True, height=600)
            
            # ğŸ”´ [ìˆ˜ì •] ë‹¤ìš´ë¡œë“œ íŒŒì¼ì— íƒ€ì„ìŠ¤íƒ¬í”„ ì ìš©
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            csv_file_name = f"comments_data_{timestamp}.csv"
            
            csv = comments_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
            st.download_button(
                "ğŸ’¾ ëŒ“ê¸€ ë°ì´í„° CSV ë‹¤ìš´ë¡œë“œ",
                csv,
                csv_file_name,
                "text/csv",
                key='download-comments-raw'
            )
        
        # ì˜ìƒ ë°ì´í„°
        else:
            if videos_df is not None and not videos_df.empty:
                st.subheader("ğŸ¥ ì˜ìƒ ë°ì´í„°")
                st.dataframe(videos_df, use_container_width=True, height=600)
                
                # ğŸ”´ [ìˆ˜ì •] ë‹¤ìš´ë¡œë“œ íŒŒì¼ì— íƒ€ì„ìŠ¤íƒ¬í”„ ì ìš©
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                csv_file_name = f"videos_data_{timestamp}.csv"

                csv = videos_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
                st.download_button(
                    "ğŸ’¾ ì˜ìƒ ë°ì´í„° CSV ë‹¤ìš´ë¡œë“œ",
                    csv,
                    csv_file_name,
                    "text/csv",
                    key='download-videos-raw'
                )
            else:
                st.warning("ì˜ìƒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # ========================================
    # íƒ­ 8: ğŸ“„ ì¼ë°˜ ë³´ê³ ì„œ ìƒì„± ì„¹ì…˜ (í”„ë¡¬í”„íŠ¸ ì…ë ¥ ê¸°ëŠ¥ í¬í•¨)
    # ========================================
    with tabs[7]:
        st.header("ğŸ“„ Market Insight Report Generator (OpenAI API ê¸°ë°˜)")
        st.write("ë¶„ì„ CSV íŒŒì¼ì— í¬í•¨ëœ í•µì‹¬ í‚¤ì›Œë“œì™€ í†µê³„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ **ì‚¬ìš©ì ì§€ì • í”„ë¡¬í”„íŠ¸**ì— ë§ì¶˜ ìš”ì•½ ë³´ê³ ì„œë¥¼ ìë™ ìƒì„±í•©ë‹ˆë‹¤.")
        st.warning("âš ï¸ **OpenAI API Key**ê°€ `.env` íŒŒì¼ì— ì„¤ì •ë˜ì–´ ìˆì–´ì•¼ í•˜ë©°, ë³´ê³ ì„œì— í¬í•¨í•  CSV íŒŒì¼ì€ ë¨¼ì € ë¶„ì„ íƒ­ì—ì„œ ì‹¤í–‰í•˜ê³  **`analysis_results` í´ë”ì— ì €ì¥**í•´ì•¼ í•©ë‹ˆë‹¤.")

        user_focus_prompt = st.text_area(
            "âœï¸ ë³´ê³ ì„œì˜ í•µì‹¬ ë¶„ì„ ì£¼ì œ ë° ì§ˆë¬¸ (Focus Prompt)",
            value="Analyze the key positive and negative sentiment drivers in the selected datasets. What strategic recommendations can be derived from the time trend data for content planning?",
            height=150, key="report_general_prompt"
        )
        
        try:
            available_files = [f for f in os.listdir(SAVE_DIR) if f.endswith(".csv")]
        except FileNotFoundError:
            available_files = []

        if not available_files:
            st.warning("ë¶„ì„ ê²°ê³¼ CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ìœ„ ë¶„ì„ íƒ­ì—ì„œ ë¶„ì„ì„ ì‹¤í–‰í•˜ê³  CSVë¥¼ `analysis_results` í´ë”ì— ì €ì¥í•˜ì„¸ìš”.")
        else:
            selected_files = st.multiselect("ğŸ“‚ ë³´ê³ ì„œì— í¬í•¨í•  íŒŒì¼ ì„ íƒ", available_files, default=available_files)

            if st.button("ğŸ§  ë³´ê³ ì„œ ìƒì„± ì‹¤í–‰"):
                report_sentences = [] 
                
                if not selected_files or not user_focus_prompt.strip() or not OPENAI_API_KEY:
                    st.error("ì…ë ¥ê°’ì„ í™•ì¸í•˜ì„¸ìš”. (íŒŒì¼ ì„ íƒ, í”„ë¡¬í”„íŠ¸ ì…ë ¥, API Key í™•ì¸)")
                    return
                
                raw_comments_df = st.session_state.get('comments_df')
                if raw_comments_df is None or raw_comments_df.empty:
                    st.error("ëŒ“ê¸€ ì›ë³¸ ë°ì´í„°ê°€ ì„¸ì…˜ì— ì—†ìŠµë‹ˆë‹¤. ë°ì´í„° ìˆ˜ì§‘ ë˜ëŠ” ë¡œë“œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                    return
                
                temp_analyzer = YouTubeCommentAnalyzer(raw_comments_df)
                if 'like_count' in raw_comments_df.columns:
                     raw_comments_df['LikeCount'] = raw_comments_df['like_count']
                
                _, _, sentiment_classified_df_full = temp_analyzer.sentiment_keywords() 


                with st.spinner("OpenAI GPT ëª¨ë¸ì´ ë³´ê³ ì„œë¥¼ ìƒì„± ì¤‘..."):
                    for f in selected_files:
                        file_path = os.path.join(SAVE_DIR, f)
                        try:
                            df = pd.read_csv(file_path, encoding="utf-8-sig")

                            keywords = f"File: {f}. "
                            
                            # ë°ì´í„° ì¶”ì¶œ ë° í‚¤ì›Œë“œ ë¬¸ìì—´ ìƒì„± 
                            if 'Frequency' in df.columns: 
                                top_keyword = df.iloc[0]['Keyword']
                                top_count = df.iloc[0]['Frequency']
                                keywords += f"Top keyword is '{top_keyword}' with count {top_count}. Total unique keywords: {len(df)}. "
                            
                            elif 'Sentiment' in df.columns: 
                                sentiment_counts = df['Sentiment'].value_counts()
                                pos = sentiment_counts.get('ê¸ì •', 0)
                                neg = sentiment_counts.get('ë¶€ì •', 0)
                                total = len(df)
                                pos_ratio = pos / total * 100 if total > 0 else 0
                                
                                # ê¸ì • ëŒ“ê¸€ ìƒ˜í”Œë§ (LikeCount ê¸°ì¤€)
                                positive_samples = sentiment_classified_df_full[
                                    (sentiment_classified_df_full['Sentiment'] == 'ê¸ì •') & 
                                    (sentiment_classified_df_full['LikeCount'] > 0) 
                                ].sort_values(by='LikeCount', ascending=False)['Text'].head(3).tolist()
                                
                                if positive_samples:
                                    clean_samples = [temp_analyzer.preprocess_text(s) for s in positive_samples]
                                    sample_text = "Sample positive comments (high like count): " + " | ".join(clean_samples)
                                    keywords += sample_text + " "

                                keywords += f"Total comments {total}. Positive comments: {pos} ({pos_ratio:.1f}%). Negative comments: {neg}. The overall sentiment is mostly Positive. "
                            
                            elif 'CommentCount' in df.columns: 
                                df['Date'] = pd.to_datetime(df['Date'])
                                max_comments_date = df.loc[df['CommentCount'].idxmax(), 'Date'].strftime('%Y-%m-%d')
                                max_comments_count = df['CommentCount'].max()
                                keywords += f"Peak comment count {max_comments_count} occurred on {max_comments_date}. Average comments per period is {df['CommentCount'].mean():.1f}. "
                            
                            elif 'Keyword1' in df.columns and 'Keyword2' in df.columns: 
                                keywords += f"Co-occurrence matrix data. Analyzing relationships between {len(df)} keywords. "
                            
                            elif 'Keyword1' in df.columns: # Topic Comparison
                                top_video_topic = df.index[0]
                                key_terms = [str(x) for x in df.iloc[0].dropna().tolist()]
                                keywords += f"The top video topic is '{top_video_topic}' with key terms: {', '.join(key_terms)}. "
                            
                            elif 'text' in df.columns and 'like_count' in df.columns:
                                total_comments = len(df)
                                avg_likes = df['like_count'].mean()
                                top_comments = df.sort_values(by='like_count', ascending=False)['text'].head(3).tolist()
                                clean_samples = [temp_analyzer.preprocess_text(s) for s in top_comments]
                                top_comment_text = " | ".join(clean_samples)
                                keywords += f"Raw comment data summary. Total records: {total_comments}. Average likes per comment: {avg_likes:.1f}. Top comments by like count: {top_comment_text}. "
                            
                            else: 
                                keywords += f"Dataset rows: {len(df)}. Columns: {', '.join(df.columns)}. Data statistics available. "
                            
                            sentence = generate_openai_report(keywords, user_focus_prompt, OPENAI_API_KEY) 
                            report_sentences.append(f"**{f} Insight:** {sentence}") 
                            
                        except Exception as e:
                            st.error(f"íŒŒì¼ {f} ì²˜ë¦¬ ì˜¤ë¥˜: CSV íŒŒì¼ êµ¬ì¡° í™•ì¸ í•„ìš”. {str(e)}")
                            continue

                if report_sentences:
                    summary = "\n\n".join(report_sentences)
                    
                    final_report = f"""
# YouTube Analysis Auto-Generated Report
## Generated At: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
## User Focus Prompt: {user_focus_prompt}
---
{summary}
"""
                    st.subheader("ğŸ“ˆ AI ìë™ ìƒì„± ë³´ê³ ì„œ ì´ˆì•ˆ")
                    st.text_area("ìš”ì•½ ê²°ê³¼", final_report, height=400)
                    
                    st.download_button(
                        "ğŸ’¾ ìš”ì•½ ë³´ê³ ì„œ ì €ì¥",
                        final_report.encode("utf-8-sig"),
                        "Market_Insight_Report.txt",
                        "text/plain"
                    )
                else:
                    st.error("ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨. íŒŒì¼ ì„ íƒ ë° êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    # ========================================
    # íƒ­ 9: ğŸ’¼ ì„ì›ì§„ ë³´ê³ ì„œ (Executive Summary)
    # ========================================
    with tabs[8]:
        st.header("ğŸ’¼ ì„ì›ì§„ ë³´ê³ ì„œ (Executive Summary)")
        st.write("í•µì‹¬ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ **êµ­ë¬¸ ë° ì˜ë¬¸**ìœ¼ë¡œ ë¶„ë¦¬ëœ, ì„ì›ì§„ ì œì¶œìš©ìœ¼ë¡œ ì í•©í•œ ìš”ì•½ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        st.warning("âš ï¸ ì´ ë³´ê³ ì„œ ìƒì„±ì„ ìœ„í•´ì„œëŠ” **OpenAI API Key**ê°€ í•„ìˆ˜ì´ë©°, ë¶„ì„ íƒ­ì—ì„œ CSV íŒŒì¼ì„ **`analysis_results`** í´ë”ì— ì €ì¥í•´ì•¼ í•©ë‹ˆë‹¤.")
        
        # ğŸ”´ [ì¶”ê°€] ì„ì›ì§„ ë³´ê³ ì„œ í”„ë¡¬í”„íŠ¸ ì…ë ¥ ì˜ì—­
        user_exec_prompt = st.text_area(
            "âœï¸ ì„ì›ì§„ ë³´ê³ ì„œì˜ í•µì‹¬ ë¶„ì„ ì£¼ì œ ë° ì§ˆë¬¸ (Focus Prompt)",
            value="Identify the 3 most critical market insights regarding K-Beauty trends and propose concise strategic actions for brand positioning based on the competitive analysis.",
            height=100, key="report_exec_prompt"
        )
        
        # SAVE_DIRì—ì„œ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        try:
            available_files = [f for f in os.listdir(SAVE_DIR) if f.endswith(".csv")]
        except FileNotFoundError:
            available_files = []

        if not available_files:
            st.warning("ë¶„ì„ ê²°ê³¼ CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë¶„ì„ íƒ­ì—ì„œ íŒŒì¼ì„ ì €ì¥í•˜ì„¸ìš”.")
        else:
            selected_exec_files = st.multiselect("ğŸ“‚ ë³´ê³ ì„œì— í¬í•¨í•  íŒŒì¼ ì„ íƒ", available_files, default=available_files, key="report_exec_files")

            if st.button("ğŸ§  êµ­/ì˜ë¬¸ ì„ì›ì§„ ë³´ê³ ì„œ ìƒì„±", key="btn_generate_exec_report"):
                
                if not selected_exec_files or not user_exec_prompt.strip() or not OPENAI_API_KEY:
                    st.error("ì…ë ¥ê°’ì„ í™•ì¸í•˜ì„¸ìš”. (íŒŒì¼ ì„ íƒ, í”„ë¡¬í”„íŠ¸ ì…ë ¥, API Key í™•ì¸)")
                    return

                # --- 1. í†µí•©ëœ í‚¤ì›Œë“œ/ë°ì´í„° ìš”ì•½ ë¬¸ìì—´ ìƒì„± ---
                full_keywords_for_exec = ""
                with st.spinner("OpenAI GPT ëª¨ë¸ì´ êµ­/ì˜ë¬¸ ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ ë°ì´í„° ì¤€ë¹„ ì¤‘..."):
                    
                    # Sentiment ë¶„ì„ì„ ìœ„í•œ ê¸°ë³¸ Analyzer ê°ì²´ ìƒì„± ë° ë¶„ë¥˜ ë°ì´í„° ì¤€ë¹„
                    raw_comments_df = st.session_state.get('comments_df')
                    temp_analyzer = YouTubeCommentAnalyzer(raw_comments_df)
                    if 'like_count' in raw_comments_df.columns:
                         raw_comments_df['LikeCount'] = raw_comments_df['like_count']
                    _, _, sentiment_classified_df_full = temp_analyzer.sentiment_keywords() 
                    
                    # ëª¨ë“  ì„ íƒëœ íŒŒì¼ì˜ ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ í‚¤ì›Œë“œ ë¬¸ìì—´ë¡œ ì¡°í•© 
                    for f in selected_exec_files:
                        file_path = os.path.join(SAVE_DIR, f)
                        keywords_chunk = f"File: {f}. "
                        try:
                            df = pd.read_csv(file_path, encoding="utf-8-sig")

                            # --- í‚¤ì›Œë“œ ì¶”ì¶œ ë¡œì§ (ì¼ë°˜ ë³´ê³ ì„œ íƒ­ê³¼ ë™ì¼) ---
                            if 'Frequency' in df.columns: 
                                keywords_chunk += f"Top keyword is '{df.iloc[0]['Keyword']}' with count {df.iloc[0]['Frequency']}. Total unique keywords: {len(df)}. "
                            
                            elif 'Sentiment' in df.columns: 
                                sentiment_counts = df['Sentiment'].value_counts()
                                pos_ratio = sentiment_counts.get('ê¸ì •', 0) / len(df) * 100 if len(df) > 0 else 0
                                keywords_chunk += f"Total comments {len(df)}. Positive ratio: {pos_ratio:.1f}%. "
                            
                            elif 'CommentCount' in df.columns: 
                                max_count = df['CommentCount'].max()
                                keywords_chunk += f"Peak comment count {max_count}. Average count per period is {df['CommentCount'].mean():.1f}. "
                            
                            elif 'Keyword1' in df.columns and 'Keyword2' in df.columns: 
                                keywords_chunk += f"Co-occurrence matrix data. Analyzing relationships between {len(df)} keywords. "

                            elif 'Keyword1' in df.columns: # Topic Comparison
                                keywords_chunk += f"The top video topic is '{df.index[0]}' with key terms: {', '.join([str(x) for x in df.iloc[0].dropna().tolist()])}. "
                            
                            elif 'text' in df.columns and 'like_count' in df.columns:
                                avg_likes = df['like_count'].mean()
                                keywords_chunk += f"Raw comment data summary. Total records: {len(df)}. Average likes per comment: {avg_likes:.1f}. "
                            
                            else: keywords_chunk += f"Dataset rows: {len(df)}. Columns: {', '.join(df.columns)}. "
                            
                            full_keywords_for_exec += keywords_chunk
                        except Exception as e:
                            st.error(f"íŒŒì¼ {f} ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
                            continue
                            
                # --- 2. í†µí•©ëœ í‚¤ì›Œë“œì™€ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì„ì›ì§„ ë³´ê³ ì„œ ìƒì„± ---
                with st.spinner("OpenAI GPT ëª¨ë¸ì´ êµ­/ì˜ë¬¸ ë³´ê³ ì„œë¥¼ ìƒì„± ì¤‘..."):
                    english_summary, korean_summary = generate_executive_report(full_keywords_for_exec, user_exec_prompt, OPENAI_API_KEY)
                    
                    # 3. ê²°ê³¼ ì¶œë ¥ ë° ë‹¤ìš´ë¡œë“œ
                    
                    korean_final_report = f"""
# ğŸ”´ YouTube ì„ì›ì§„ ë³´ê³ ì„œ (êµ­ë¬¸)
## ì‘ì„±ì¼: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
## ë¶„ì„ ì£¼ì œ: {user_exec_prompt}
---
### í•µì‹¬ ìš”ì•½ (Korean Summary)
{korean_summary}

---
### ë¶„ì„ì— ì‚¬ìš©ëœ ë°ì´í„° íŒŒì¼
{', '.join(selected_exec_files)}
"""
                    english_final_report = f"""
# ğŸ”´ YouTube Executive Summary (English)
## Generated At: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
## Focus Prompt: {user_exec_prompt}
---
### Executive Summary
{english_summary}

---
### Data Files Used
{', '.join(selected_exec_files)}
"""
                    
                    st.markdown("---")
                    st.subheader("ğŸ‡°ğŸ‡· êµ­ë¬¸ ë³´ê³ ì„œ ì´ˆì•ˆ")
                    st.text_area("êµ­ë¬¸ ìš”ì•½ ê²°ê³¼", korean_final_report, height=300)
                    
                    st.subheader("ğŸ‡ºğŸ‡¸ ì˜ë¬¸ ë³´ê³ ì„œ ì´ˆì•ˆ")
                    st.text_area("ì˜ë¬¸ ìš”ì•½ ê²°ê³¼", english_final_report, height=300)
                    
                    col_kor, col_eng = st.columns(2)
                    
                    with col_kor:
                        st.download_button(
                            "ğŸ’¾ êµ­ë¬¸ ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ (KR.txt)",
                            korean_final_report.encode("utf-8-sig"),
                            "Executive_Report_KR.txt",
                            "text/plain"
                        )
                    with col_eng:
                        st.download_button(
                            "ğŸ’¾ ì˜ë¬¸ ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ (EN.txt)",
                            english_final_report.encode("utf-8-sig"),
                            "Executive_Report_EN.txt",
                            "text/plain"
                        )
                    
                    if "Parsing Error" in english_summary:
                        st.error("ë³´ê³ ì„œ íŒŒì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. OpenAI ì¶œë ¥ í˜•ì‹ì„ í™•ì¸í•˜ì„¸ìš”.")


if __name__ == "__main__":
    main()