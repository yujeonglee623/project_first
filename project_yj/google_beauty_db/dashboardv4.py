import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from datetime import datetime
import re
from wordcloud import WordCloud
import io
from googleapiclient.discovery import build
from dotenv import load_dotenv # .env íŒŒì¼ ë¡œë“œë¥¼ ìœ„í•´ ì¶”ê°€
import os
import time
# NOTE: OpenAI API í˜¸ì¶œì„ ìœ„í•´ 'requests' ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (pip install requests)
import requests 
# PyTorch/HuggingFace ëª¨ë¸ ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì œê±°ë¨

# ========================================
# Streamlit ê¸°ë³¸ ì„¤ì •
# ========================================

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="YouTube ëŒ“ê¸€ ë¶„ì„ ëŒ€ì‹œë³´ë“œ",
    page_icon="ğŸ¥",
    layout="wide"
)

# í•œê¸€ í°íŠ¸ ì„¤ì • (ì‹œìŠ¤í…œì— 'Malgun Gothic'ì´ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•¨)
plt.rcParams['font.family'] = 'Malgun Gothic'  # Windows í™˜ê²½ ê°€ì •
# plt.rcParams['font.family'] = 'AppleGothic'  # Mac í™˜ê²½ì¼ ê²½ìš°
plt.rcParams['axes.unicode_minus'] = False


# ========================================
# AI ëª¨ë¸ ë¡œë”© ë° ìƒì„± í•¨ìˆ˜ (OpenAI API ê¸°ë°˜ìœ¼ë¡œ ëŒ€ì²´)
# ========================================

SAVE_DIR = "analysis_results"
os.makedirs(SAVE_DIR, exist_ok=True)

# ì´ì „ T5 ëª¨ë¸ ë¡œë”© í•¨ìˆ˜ëŠ” ì œê±°ë©ë‹ˆë‹¤.


def generate_openai_report(keywords, api_key, model_name="gpt-4o"):
    """OpenAI APIë¥¼ ì´ìš©í•œ ë³´ê³ ì„œ ë¬¸ì¥ ìƒì„± í•¨ìˆ˜ (GPT-4o ì‚¬ìš©)"""
    
    if not api_key:
        return "Error: OpenAI API Key is missing. Please set the OPENAI_API_KEY in the .env file."

    # System Prompt: AIì˜ ì—­í• ê³¼ ì›í•˜ëŠ” ì¶œë ¥ í˜•ì‹ì„ ëª…í™•íˆ ì •ì˜ (ê³ í’ˆì§ˆ ë¶„ì„ ìœ ë„)
    system_prompt = (
        "You are a professional YouTube Market Analyst. "
        "Your task is to analyze the provided raw data summary or statistical analysis "
        "and generate a comprehensive, insightful, and professional English summary (approximately 5 detailed sentences). " # 5ë¬¸ì¥ ì •ë„ë¡œ ê¸¸ì´ ìˆ˜ì •
        "The summary must cover multiple facets, including key trends, sentiment drivers, quantitative findings, and strategic implications. "
        "If positive or negative comments are provided in the sample, use them as evidence "
        "to explain the sentiment drivers. "
        "Do not use markdown headers or lists. Just provide the summary text."
    )
    
    # User Prompt: ì‹¤ì œ CSVì—ì„œ ì¶”ì¶œí•œ ë°ì´í„°ë¥¼ ì „ë‹¬
    user_prompt = f"Analyze the following data: {keywords}"

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}",
    }

    # API Payload
    payload = {
        "model": model_name,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        "max_tokens": 400, # ì¶©ë¶„í•œ ê¸¸ì´ë¥¼ í™•ë³´í•˜ê¸° ìœ„í•´ í† í° ì¦ê°€
        "temperature": 0.3, # ë¶„ì„ì˜ ì •í™•ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ ì°½ì˜ì„±(Temperature) ë‚®ì¶¤
    }

    try:
        response = requests.post("https://api.openai.com/v1/chat/completions", headers=headers, json=payload, timeout=40)
        response.raise_for_status() # HTTP ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ ì²˜ë¦¬
        
        result = response.json()
        
        # ê²°ê³¼ ì¶”ì¶œ
        summary = result['choices'][0]['message']['content'].strip()
        return summary

    except requests.exceptions.HTTPError as errh:
        return f"HTTP Error: {errh}. Check if your API key is valid and the model name is correct. (Status: {response.status_code})"
    except requests.exceptions.ConnectionError as errc:
        return f"Error Connecting: {errc}"
    except requests.exceptions.Timeout as errt:
        return f"Timeout Error: {errt}"
    except requests.exceptions.RequestException as err:
        return f"An Unexpected Error: {err}"
    except Exception as e:
        return f"API Error occurred: {e}. Check response structure."


# ========================================
# ë¶„ì„ í´ë˜ìŠ¤ (CSV ì»¬ëŸ¼ëª… ì˜ì–´ë¡œ ë³€ê²½)
# ========================================

class YouTubeCommentAnalyzer:
    """YouTube ëŒ“ê¸€ ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self, comments_df, videos_df=None):
        self.comments_df = comments_df.copy()
        self.videos_df = videos_df.copy() if videos_df is not None else None
        
        # ë‚ ì§œ ì»¬ëŸ¼ ë³€í™˜
        if 'published_at' in self.comments_df.columns:
            self.comments_df['published_at'] = pd.to_datetime(self.comments_df['published_at'])
    
    
    def preprocess_text(self, text):
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        if pd.isna(text):
            return ""
        
        text = str(text).lower()
        text = re.sub(r'http\S+|www\S+', '', text)
        text = re.sub(r'[^ê°€-í£a-z0-9\s]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    
    def extract_keywords(self, min_length=2, top_n=50):
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        all_text = ' '.join(self.comments_df['text'].apply(self.preprocess_text))
        words = all_text.split()
        words = [w for w in words if len(w) >= min_length]
        
        stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
                     'of', 'is', 'are', 'was', 'were', 'been', 'be', 'have', 'has', 'had',
                     'ê·¸', 'ì´', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë“¤', 'ë°', 'ë˜í•œ', 'í•˜ë‹¤', 'ìˆë‹¤', 'ë˜ë‹¤',
                     'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ê·¸ëŸ°', 'ì´ëŸ°', 'ì €ëŸ°'}
        
        words = [w for w in words if w not in stopwords]
        word_freq = Counter(words)
        
        return word_freq.most_common(top_n)
    
    
    def wordcloud(self, width=1200, height=800):
        """ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±"""
        all_text = ' '.join(self.comments_df['text'].apply(self.preprocess_text))
        
        wordcloud = WordCloud(
            # font_path='malgun.ttf', 
            width=width,
            height=height,
            background_color='white',
            max_words=100,
            relative_scaling=0.3,
            colormap='viridis'
        ).generate(all_text)
        
        fig, ax = plt.subplots(figsize=(15, 10))
        ax.imshow(wordcloud, interpolation='bilinear')
        ax.axis('off')
        ax.set_title('ëŒ“ê¸€ ì›Œë“œí´ë¼ìš°ë“œ', fontsize=20, pad=20)
        plt.tight_layout()
        
        return fig
    
    
    def keyword_frequency(self, top_n=20):
        """í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„"""
        keywords = self.extract_keywords(top_n=top_n)
        words, counts = zip(*keywords)
        
        fig, ax = plt.subplots(figsize=(12, 8))
        ax.barh(range(len(words)), counts, color='skyblue')
        ax.set_yticks(range(len(words)))
        ax.set_yticklabels(words)
        ax.set_xlabel('ë¹ˆë„', fontsize=12)
        ax.set_title(f'ìƒìœ„ {top_n}ê°œ í‚¤ì›Œë“œ ë¹ˆë„', fontsize=16, pad=20)
        ax.invert_yaxis()
        plt.tight_layout()
        
        # CSV ì¶œë ¥ì„ ìœ„í•´ ì»¬ëŸ¼ëª…ì„ ì˜ì–´ë¡œ ë³€ê²½
        freq_df = pd.DataFrame(keywords, columns=['Keyword', 'Frequency'])
        
        return fig, freq_df
    
    
    def sentiment_keywords(self):
        """ê°ì„± í‚¤ì›Œë“œ ë¶„ì„"""
        positive_words = {
            'ì¢‹ë‹¤', 'ìµœê³ ', 'ëŒ€ë°•', 'ì˜ˆì˜ë‹¤', 'ì´ì˜ë‹¤', 'ë©‹ì§€ë‹¤', 'í›Œë¥­í•˜ë‹¤', 
            'ì™„ë²½', 'ì¢‹ì•„', 'ê°ì‚¬', 'ì‚¬ë‘', 'í–‰ë³µ', 'ì¶”ì²œ', 'êµ¿', 'good', 
            'best', 'love', 'amazing', 'perfect', 'great', 'excellent',
            'ì¢‹ì•„ìš”', 'ì¢‹ë„¤ìš”', 'ë©‹ìˆë‹¤', 'ì•„ë¦„ë‹µë‹¤', 'ìµœê³ ë‹¤', 'ì§±'
        }
        
        negative_words = {
            'ì‹«ë‹¤', 'ë³„ë¡œ', 'ì•ˆì¢‹ë‹¤', 'ë‚˜ì˜ë‹¤', 'ìµœì•…', 'í˜•í¸ì—†ë‹¤',
            'ì‹«ì–´', 'ì‹¤ë§', 'ë³„ë¡œë„¤', 'ì•„ì‰½ë‹¤', 'bad', 'worst', 'hate',
            'ì‹«ì–´ìš”', 'ë³„ë¡œì˜ˆìš”', 'ê·¸ì €ê·¸ë ‡ë‹¤', 'ì§€ë£¨í•˜ë‹¤'
        }
        
        def calculate_sentiment(text):
            text = self.preprocess_text(text)
            words = text.split()
            
            pos_count = sum(1 for w in words if w in positive_words)
            neg_count = sum(1 for w in words if w in negative_words)
            
            return pos_count, neg_count
        
        # ë‚´ë¶€ ì»¬ëŸ¼ëª… ë³€ê²½
        self.comments_df[['PositiveCount', 'NegativeCount']] = \
            self.comments_df['text'].apply(lambda x: pd.Series(calculate_sentiment(x)))
        
        def classify_sentiment(row):
            if row['PositiveCount'] > row['NegativeCount']:
                return 'ê¸ì •'
            elif row['PositiveCount'] < row['NegativeCount']:
                return 'ë¶€ì •'
            else:
                return 'ì¤‘ë¦½'
        
        self.comments_df['sentiment'] = self.comments_df.apply(classify_sentiment, axis=1)
        sentiment_counts = self.comments_df['sentiment'].value_counts()
        
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        
        colors = ['#90EE90', '#FFB6C1', '#D3D3D3']
        axes[0].pie(sentiment_counts.values, labels=sentiment_counts.index, 
                    autopct='%1.1f%%', colors=colors, startangle=90)
        axes[0].set_title('ëŒ“ê¸€ ê°ì„± ë¶„í¬', fontsize=14, pad=20)
        
        axes[1].bar(sentiment_counts.index, sentiment_counts.values, color=colors)
        axes[1].set_xlabel('ê°ì„±', fontsize=12)
        axes[1].set_ylabel('ëŒ“ê¸€ ìˆ˜', fontsize=12)
        axes[1].set_title('ê°ì„±ë³„ ëŒ“ê¸€ ìˆ˜', fontsize=14, pad=20)
        
        plt.tight_layout()
        
        # CSV ì¶œë ¥ì„ ìœ„í•´ ì»¬ëŸ¼ëª…ì„ ì˜ì–´ë¡œ ë³€ê²½
        sentiment_df = self.comments_df[['text', 'sentiment', 'PositiveCount', 'NegativeCount', 'like_count']].rename(
            columns={'sentiment': 'Sentiment', 'text': 'Text', 'like_count': 'LikeCount'}
        )
        
        return fig, sentiment_counts, sentiment_df
    
    
    def time_trend(self, interval='D'):
        """ì‹œê°„ëŒ€ë³„ íŠ¸ë Œë“œ ë¶„ì„"""
        if 'published_at' not in self.comments_df.columns:
            return None, None
        
        time_counts = self.comments_df.set_index('published_at').resample(interval).size()
        time_likes = self.comments_df.set_index('published_at')['like_count'].resample(interval).sum()
        
        fig, axes = plt.subplots(2, 1, figsize=(14, 10))
        
        axes[0].plot(time_counts.index, time_counts.values, marker='o', linewidth=2)
        axes[0].set_xlabel('ë‚ ì§œ', fontsize=12)
        axes[0].set_ylabel('ëŒ“ê¸€ ìˆ˜', fontsize=12)
        axes[0].set_title('ì‹œê°„ëŒ€ë³„ ëŒ“ê¸€ ìˆ˜ ì¶”ì´', fontsize=14, pad=20)
        axes[0].grid(True, alpha=0.3)
        
        axes[1].plot(time_likes.index, time_likes.values, marker='o', 
                      color='coral', linewidth=2)
        axes[1].set_xlabel('ë‚ ì§œ', fontsize=12)
        axes[1].set_ylabel('ì¢‹ì•„ìš” ìˆ˜', fontsize=12)
        axes[1].set_title('ì‹œê°„ëŒ€ë³„ ì¢‹ì•„ìš” ìˆ˜ ì¶”ì´', fontsize=14, pad=20)
        axes[1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # CSV ì¶œë ¥ì„ ìœ„í•´ ì»¬ëŸ¼ëª…ì„ ì˜ì–´ë¡œ ë³€ê²½
        trend_df = pd.DataFrame({
            'Date': time_counts.index,
            'CommentCount': time_counts.values,
            'LikeCount': time_likes.values
        })
        
        return fig, trend_df
    
    
    def cooccurrence(self, top_n=15):
        """í‚¤ì›Œë“œ ë™ì‹œì¶œí˜„ ë¶„ì„"""
        top_keywords = [word for word, _ in self.extract_keywords(top_n=top_n)]
        cooc_matrix = pd.DataFrame(0, index=top_keywords, columns=top_keywords)
        
        for text in self.comments_df['text']:
            text = self.preprocess_text(text)
            words = set(text.split())
            
            for word1 in top_keywords:
                if word1 in words:
                    for word2 in top_keywords:
                        if word2 in words:
                            cooc_matrix.loc[word1, word2] += 1
        
        fig, ax = plt.subplots(figsize=(14, 12))
        sns.heatmap(cooc_matrix, annot=True, fmt='d', cmap='YlOrRd', 
                    cbar_kws={'label': 'ë™ì‹œì¶œí˜„ ë¹ˆë„'}, ax=ax)
        ax.set_title(f'ìƒìœ„ {top_n}ê°œ í‚¤ì›Œë“œ ë™ì‹œì¶œí˜„ ë¶„ì„', fontsize=16, pad=20)
        ax.set_xlabel('í‚¤ì›Œë“œ', fontsize=12)
        ax.set_ylabel('í‚¤ì›Œë“œ', fontsize=12)
        plt.tight_layout()
        
        # CSV ì¶œë ¥ì„ ìœ„í•´ ì»¬ëŸ¼ëª…ì„ ì˜ì–´ë¡œ ë³€ê²½ (í‚¤ì›Œë“œ ìì²´ê°€ ë‚´ìš©ì´ì§€ë§Œ, ë°ì´í„°í”„ë ˆì„ êµ¬ì¡°ë¥¼ ìœ„í•´)
        cooc_matrix.index.name = 'Keyword1'
        cooc_matrix.columns.name = 'Keyword2'
        
        return fig, cooc_matrix
    
    
    def topic_comparison(self):
        """ì˜ìƒë³„ í† í”½ ë¹„êµ ë¶„ì„"""
        if 'video_title' not in self.comments_df.columns:
            return None, None
        
        video_keywords = {}
        
        for video_title in self.comments_df['video_title'].unique()[:10]:
            video_comments = self.comments_df[
                self.comments_df['video_title'] == video_title
            ]['text']
            
            all_text = ' '.join(video_comments.apply(self.preprocess_text))
            words = all_text.split()
            words = [w for w in words if len(w) >= 2]
            
            word_freq = Counter(words)
            top_words = [word for word, _ in word_freq.most_common(5)]
            
            video_keywords[video_title[:30] + '...'] = top_words
        
        comparison_df = pd.DataFrame(video_keywords).T
        # CSV ì¶œë ¥ì„ ìœ„í•´ ì»¬ëŸ¼ëª…ì„ ì˜ì–´ë¡œ ë³€ê²½
        comparison_df.columns = [f'Keyword{i+1}' for i in range(comparison_df.shape[1])]
        
        fig, ax = plt.subplots(figsize=(14, 8))
        ax.axis('tight')
        ax.axis('off')
        
        table = ax.table(cellText=comparison_df.values,
                         rowLabels=comparison_df.index,
                         colLabels=comparison_df.columns,
                         cellLoc='center',
                         loc='center',
                         bbox=[0, 0, 1, 1])
        
        table.auto_set_font_size(False)
        table.set_fontsize(9)
        table.scale(1, 2)
        
        for i in range(len(comparison_df.columns)):
            table[(0, i)].set_facecolor('#4CAF50')
            table[(0, i)].set_text_props(weight='bold', color='white')
        
        for i in range(len(comparison_df)):
            # ë§ˆì§€ë§‰ ì—´ì´ ì•„ë‹Œ, ì²« ë²ˆì§¸ í–‰(í—¤ë”) ì´í›„ì˜ ì²« ë²ˆì§¸ ì—´(ë¡œìš° ë ˆì´ë¸”)ì„ ê°•ì¡°
            table[(i+1, -1)].set_facecolor('#E8F5E9') 
            table[(i+1, 0)].set_facecolor('#E8F5E9') # ë¡œìš° ë ˆì´ë¸” ë°°ê²½ìƒ‰ ì„¤ì •
            table[(i+1, 0)].set_text_props(weight='bold')
        
        plt.title('ì˜ìƒë³„ ì£¼ìš” í‚¤ì›Œë“œ ë¹„êµ', fontsize=16, pad=20)
        
        return fig, comparison_df


def search_and_collect_data(keyword, max_videos, max_comments_per_video, order):
    """YouTube APIë¥¼ í†µí•œ ë°ì´í„° ìˆ˜ì§‘"""
    load_dotenv()
    api_key = os.getenv("YOUTUBE_API_KEY")
    
    if not api_key:
        st.error("YouTube API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        return None, None
    
    youtube = build("youtube", "v3", developerKey=api_key)
    
    # ì˜ìƒ ê²€ìƒ‰
    try:
        search_response = youtube.search().list(
            q=keyword,
            part="snippet",
            maxResults=min(max_videos, 50),
            type="video",
            order=order,
            regionCode="KR"
        ).execute()
        
        video_ids = [item["id"]["videoId"] for item in search_response["items"]]
    except Exception as e:
        st.error(f"ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return None, None
    
    # ì˜ìƒ ìƒì„¸ ì •ë³´
    videos_data = []
    try:
        for i in range(0, len(video_ids), 50):
            batch_ids = video_ids[i:i+50]
            video_response = youtube.videos().list(
                part="snippet,statistics,contentDetails",
                id=",".join(batch_ids)
            ).execute()
            
            for item in video_response["items"]:
                video_info = {
                    "video_id": item["id"],
                    "title": item["snippet"]["title"],
                    "channel": item["snippet"]["channelTitle"],
                    "published_at": item["snippet"]["publishedAt"],
                    "description": item["snippet"]["description"],
                    "view_count": int(item["statistics"].get("viewCount", 0)),
                    "like_count": int(item["statistics"].get("likeCount", 0)),
                    "comment_count": int(item["statistics"].get("commentCount", 0)),
                    "duration": item["contentDetails"]["duration"],
                    "tags": ", ".join(item["snippet"].get("tags", [])),
                    "url": f"https://www.youtube.com/watch?v={item['id']}"
                }
                videos_data.append(video_info)
    except Exception as e:
        st.error(f"ì˜ìƒ ì •ë³´ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
    
    videos_df = pd.DataFrame(videos_data)
    
    # ëŒ“ê¸€ ìˆ˜ì§‘
    all_comments = []
    video_info_dict = {}
    for _, row in videos_df.iterrows():
        video_info_dict[row['video_id']] = {
            'title': row['title'],
            'channel': row['channel'],
            'url': row['url']
        }
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for idx, video_id in enumerate(video_ids):
        status_text.text(f"ì˜ìƒ {idx+1}/{len(video_ids)} ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘...")
        progress_bar.progress((idx + 1) / len(video_ids))
        
        try:
            comments = []
            next_page_token = None
            
            while len(comments) < max_comments_per_video:
                request = youtube.commentThreads().list(
                    part="snippet,replies",
                    videoId=video_id,
                    maxResults=min(100, max_comments_per_video - len(comments)),
                    pageToken=next_page_token,
                    textFormat="plainText",
                    order="relevance"
                )
                response = request.execute()
                
                for item in response["items"]:
                    top_comment = item["snippet"]["topLevelComment"]["snippet"]
                    
                    comment_info = {
                        "comment_id": item["snippet"]["topLevelComment"]["id"],
                        "video_id": video_id,
                        "author": top_comment["authorDisplayName"],
                        "text": top_comment["textDisplay"],
                        "like_count": top_comment["likeCount"],
                        "published_at": top_comment["publishedAt"],
                        "reply_count": item["snippet"]["totalReplyCount"]
                    }
                    
                    if video_id in video_info_dict:
                        comment_info['video_title'] = video_info_dict[video_id]['title']
                        comment_info['video_channel'] = video_info_dict[video_id]['channel']
                        comment_info['video_url'] = video_info_dict[video_id]['url']
                    
                    comments.append(comment_info)
                
                next_page_token = response.get("nextPageToken")
                if not next_page_token:
                    break
                
                time.sleep(0.5)
            
            all_comments.extend(comments)
        
        except Exception as e:
            if "commentsDisabled" not in str(e):
                st.warning(f"ì˜ìƒ {video_id} ëŒ“ê¸€ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        
        time.sleep(1)
    
    progress_bar.empty()
    status_text.empty()
    
    comments_df = pd.DataFrame(all_comments)
    
    return videos_df, comments_df


# ========================================
# Streamlit ë©”ì¸ ì•±
# ========================================

def main():
    st.title("ğŸ¥ YouTube ëŒ“ê¸€ ë¶„ì„ ëŒ€ì‹œë³´ë“œ")
    st.markdown("---")
    
    # OpenAI API Key ì„¤ì • (.env íŒŒì¼ì—ì„œ ë¡œë“œ)
    load_dotenv()
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    
    # ì‚¬ì´ë“œë°” - ë°ì´í„° ìˆ˜ì§‘/ì—…ë¡œë“œë§Œ
    st.sidebar.header("ğŸ“‚ ë°ì´í„° ì†ŒìŠ¤")
    data_source = st.sidebar.radio(
        "ë°ì´í„° ì…ë ¥ ë°©ì‹ ì„ íƒ",
        ["APIë¡œ ì‹¤ì‹œê°„ ìˆ˜ì§‘", "CSV íŒŒì¼ ì—…ë¡œë“œ"]
    )
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ë° ë¡œë“œ
    if 'videos_df' not in st.session_state:
        st.session_state['videos_df'] = None
    if 'comments_df' not in st.session_state:
        st.session_state['comments_df'] = None
    
    videos_df = st.session_state['videos_df']
    comments_df = st.session_state['comments_df']
    
    if data_source == "APIë¡œ ì‹¤ì‹œê°„ ìˆ˜ì§‘":
        st.sidebar.subheader("ğŸ” ê²€ìƒ‰ ì„¤ì •")
        keyword = st.sidebar.text_input("ê²€ìƒ‰ í‚¤ì›Œë“œ", value="K-beauty")
        max_videos = st.sidebar.slider("ì˜ìƒ ê°œìˆ˜", 1, 50, 10)
        max_comments = st.sidebar.slider("ì˜ìƒë‹¹ ëŒ“ê¸€ ìˆ˜", 10, 200, 50)
        order = st.sidebar.selectbox(
            "ì •ë ¬ ë°©ì‹",
            ["relevance", "date", "viewCount"],
            format_func=lambda x: {"relevance": "ê´€ë ¨ì„±ìˆœ", "date": "ìµœì‹ ìˆœ", "viewCount": "ì¡°íšŒìˆ˜ìˆœ"}[x]
        )
        
        if st.sidebar.button("ğŸš€ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘"):
            with st.spinner("ë°ì´í„° ìˆ˜ì§‘ ì¤‘..."):
                videos_df_new, comments_df_new = search_and_collect_data(
                    keyword, max_videos, max_comments, order
                )
            
            if videos_df_new is not None and comments_df_new is not None and not comments_df_new.empty:
                st.success(f"âœ… ì˜ìƒ {len(videos_df_new)}ê°œ, ëŒ“ê¸€ {len(comments_df_new)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ!")
                
                # ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸ì— ì €ì¥ (UI ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´ ì¬í• ë‹¹)
                st.session_state['videos_df'] = videos_df_new
                st.session_state['comments_df'] = comments_df_new
                st.rerun() # ë°ì´í„° ìˆ˜ì§‘ í›„ ì•±ì„ ì¬ì‹¤í–‰í•˜ì—¬ UI ì—…ë°ì´íŠ¸
            elif comments_df_new is not None and comments_df_new.empty:
                 st.warning("ìˆ˜ì§‘ëœ ëŒ“ê¸€ì´ ì—†ìŠµë‹ˆë‹¤. ê²€ìƒ‰ ì¡°ê±´ì´ë‚˜ API ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    
    else:  # CSV íŒŒì¼ ì—…ë¡œë“œ
        st.sidebar.subheader("ğŸ“¤ íŒŒì¼ ì—…ë¡œë“œ")
        comments_file = st.sidebar.file_uploader("ëŒ“ê¸€ CSV íŒŒì¼", type=['csv'])
        videos_file = st.sidebar.file_uploader("ì˜ìƒ CSV íŒŒì¼ (ì„ íƒ)", type=['csv'])
        
        if comments_file:
            comments_df = pd.read_csv(comments_file)
            st.session_state['comments_df'] = comments_df
            st.sidebar.success(f"âœ… ëŒ“ê¸€ {len(comments_df)}ê°œ ë¡œë“œ")
        
        if videos_file:
            videos_df = pd.read_csv(videos_file)
            st.session_state['videos_df'] = videos_df
            st.sidebar.success(f"âœ… ì˜ìƒ {len(videos_df)}ê°œ ë¡œë“œ")
        
        # íŒŒì¼ ì—…ë¡œë“œ í›„ ë°ì´í„° ë¡œë“œë¥¼ ìœ„í•´ rerurn
        if comments_file or videos_file:
            st.rerun()

    # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì•ˆë‚´ ë©”ì‹œì§€
    if comments_df is None or comments_df.empty:
        st.info("ğŸ‘† ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê±°ë‚˜ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        return
    
    # ë°ì´í„°ê°€ ë¡œë“œëœ í›„ë¶€í„° ë¶„ì„ ì‹œì‘
    
    # ê¸°ë³¸ í†µê³„
    st.header("ğŸ“ˆ ê¸°ë³¸ í†µê³„")
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("ì´ ëŒ“ê¸€ ìˆ˜", f"{len(comments_df):,}")
    with col2:
        st.metric("í‰ê·  ì¢‹ì•„ìš”", f"{comments_df['like_count'].mean():.1f}")
    with col3:
        st.metric("ì´ ì¢‹ì•„ìš”", f"{comments_df['like_count'].sum():,}")
    with col4:
        if videos_df is not None:
            st.metric("ë¶„ì„ ì˜ìƒ ìˆ˜", f"{len(videos_df)}")
    
    st.markdown("---")
    
    # íƒ­ìœ¼ë¡œ ë¶„ì„ ëª¨ë“œ êµ¬ë¶„
    tabs = st.tabs([
        "â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œ",
        "ğŸ“Š í‚¤ì›Œë“œ ë¹ˆë„",
        "ğŸ˜ŠğŸ˜¢ ê°ì„± ë¶„ì„",
        "ğŸ“ˆ ì‹œê°„ íŠ¸ë Œë“œ",
        "ğŸ”— ë™ì‹œì¶œí˜„",
        "ğŸ¬ í† í”½ ë¹„êµ",
        "ğŸ“‹ ì›ë³¸ ë°ì´í„°"
    ])
    
    analyzer = YouTubeCommentAnalyzer(comments_df, videos_df)
    
    # ê° ë¶„ì„ ê²°ê³¼ DataFrameì„ session_stateì— ì €ì¥í•˜ê¸° ìœ„í•œ ë”•ì…”ë„ˆë¦¬
    analysis_results = {}

    # íƒ­ 1: ì›Œë“œí´ë¼ìš°ë“œ
    with tabs[0]:
        st.header("â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œ")
        if st.button("ğŸ” ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±", key="btn_wordcloud"):
            with st.spinner("ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì¤‘..."):
                fig = analyzer.wordcloud()
                st.pyplot(fig)
        else:
            st.info("ğŸ‘† ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•˜ì„¸ìš”.")
    
    # íƒ­ 2: í‚¤ì›Œë“œ ë¹ˆë„
    with tabs[1]:
        st.header("ğŸ“Š í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„")
        top_n = st.slider("í‘œì‹œí•  í‚¤ì›Œë“œ ê°œìˆ˜", 10, 50, 20, key="keyword_top_n")
        
        if st.button("ğŸ” í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„", key="btn_keyword"):
            with st.spinner("í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„ ì¤‘..."):
                fig, freq_df = analyzer.keyword_frequency(top_n=top_n)
                st.pyplot(fig)
                analysis_results['keyword_frequency.csv'] = freq_df  # ê²°ê³¼ ì €ì¥
                st.session_state['freq_df'] = freq_df # ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸ì—ë„ ì €ì¥
                
                st.subheader("ğŸ“‹ í‚¤ì›Œë“œ ë°ì´í„° (English Column)")
                st.dataframe(freq_df, use_container_width=True)
                
                csv = freq_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
                st.download_button(
                    "ğŸ’¾ CSV ë‹¤ìš´ë¡œë“œ",
                    csv,
                    "keyword_frequency.csv",
                    "text/csv",
                    key='download-keyword-csv'
                )
        else:
             if 'freq_df' in st.session_state:
                freq_df = st.session_state['freq_df']
                st.subheader("ğŸ“‹ ë§ˆì§€ë§‰ ë¶„ì„ ê²°ê³¼ (í‚¤ì›Œë“œ ë°ì´í„° - English Column)")
                st.dataframe(freq_df, use_container_width=True)
             else:
                st.info("ğŸ‘† ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ í‚¤ì›Œë“œ ë¹ˆë„ë¥¼ ë¶„ì„í•˜ì„¸ìš”.")
    
    # íƒ­ 3: ê°ì„± ë¶„ì„
    with tabs[2]:
        st.header("ğŸ˜ŠğŸ˜¢ ê°ì„± ë¶„ì„")
        
        if st.button("ğŸ” ê°ì„± ë¶„ì„ ì‹¤í–‰", key="btn_sentiment"):
            with st.spinner("ê°ì„± ë¶„ì„ ì¤‘..."):
                fig, sentiment_counts, sentiment_df = analyzer.sentiment_keywords()
                st.pyplot(fig)
                analysis_results['sentiment_analysis.csv'] = sentiment_df # ê²°ê³¼ ì €ì¥
                st.session_state['sentiment_df'] = sentiment_df # ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸ì—ë„ ì €ì¥
                
                col1, col2, col3 = st.columns(3)
                for idx, (sentiment, count) in enumerate(sentiment_counts.items()):
                    with [col1, col2, col3][idx]:
                        st.metric(sentiment, f"{count:,}ê°œ")
                
                st.subheader("ğŸ“‹ ê°ì„± ë¶„ë¥˜ ë°ì´í„° (English Column)")
                st.dataframe(sentiment_df.head(100), use_container_width=True)
                
                csv = sentiment_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
                st.download_button(
                    "ğŸ’¾ CSV ë‹¤ìš´ë¡œë“œ",
                    csv,
                    "sentiment_analysis.csv",
                    "text/csv",
                    key='download-sentiment-csv'
                )
        else:
             if 'sentiment_df' in st.session_state:
                sentiment_df = st.session_state['sentiment_df']
                st.subheader("ğŸ“‹ ë§ˆì§€ë§‰ ë¶„ì„ ê²°ê³¼ (ê°ì„± ë¶„ë¥˜ ë°ì´í„° - English Column)")
                st.dataframe(sentiment_df.head(100), use_container_width=True)
             else:
                st.info("ğŸ‘† ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ê°ì„± ë¶„ì„ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
    
    # íƒ­ 4: ì‹œê°„ íŠ¸ë Œë“œ
    with tabs[3]:
        st.header("ğŸ“ˆ ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„")
        interval = st.radio("ì‹œê°„ ê°„ê²©", ["D (ì¼)", "W (ì£¼)", "M (ì›”)"], horizontal=True, key="time_interval")
        interval_code = interval.split()[0]
        
        if st.button("ğŸ” ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„", key="btn_time"):
            with st.spinner("ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„ ì¤‘..."):
                fig, trend_df = analyzer.time_trend(interval=interval_code)
                if fig:
                    st.pyplot(fig)
                    analysis_results['time_trend.csv'] = trend_df # ê²°ê³¼ ì €ì¥
                    st.session_state['trend_df'] = trend_df # ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸ì—ë„ ì €ì¥
                    
                    st.subheader("ğŸ“‹ íŠ¸ë Œë“œ ë°ì´í„° (English Column)")
                    st.dataframe(trend_df, use_container_width=True)
                    
                    csv = trend_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
                    st.download_button(
                        "ğŸ’¾ CSV ë‹¤ìš´ë¡œë“œ",
                        csv,
                        "time_trend.csv",
                        "text/csv",
                        key='download-trend-csv'
                    )
                else:
                    st.warning("published_at ì»¬ëŸ¼ì´ ì—†ì–´ ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
             if 'trend_df' in st.session_state:
                trend_df = st.session_state['trend_df']
                st.subheader("ğŸ“‹ ë§ˆì§€ë§‰ ë¶„ì„ ê²°ê³¼ (íŠ¸ë Œë“œ ë°ì´í„° - English Column)")
                st.dataframe(trend_df, use_container_width=True)
             else:
                st.info("ğŸ‘† ì‹œê°„ ê°„ê²©ì„ ì„ íƒí•˜ê³  ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ë¶„ì„í•˜ì„¸ìš”.")
    
    # íƒ­ 5: ë™ì‹œì¶œí˜„
    with tabs[4]:
        st.header("ğŸ”— í‚¤ì›Œë“œ ë™ì‹œì¶œí˜„ ë¶„ì„")
        cooc_n = st.slider("ë¶„ì„í•  í‚¤ì›Œë“œ ê°œìˆ˜", 5, 20, 15, key="cooc_n")
        
        if st.button("ğŸ” ë™ì‹œì¶œí˜„ ë¶„ì„", key="btn_cooc"):
            with st.spinner("ë™ì‹œì¶œí˜„ ë¶„ì„ ì¤‘..."):
                fig, cooc_matrix = analyzer.cooccurrence(top_n=cooc_n)
                st.pyplot(fig)
                analysis_results['cooccurrence_matrix.csv'] = cooc_matrix # ê²°ê³¼ ì €ì¥
                st.session_state['cooc_df'] = cooc_matrix # ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸ì—ë„ ì €ì¥
                
                st.subheader("ğŸ“‹ ë™ì‹œì¶œí˜„ ë§¤íŠ¸ë¦­ìŠ¤ (English Index/Column)")
                st.dataframe(cooc_matrix, use_container_width=True)
                
                csv = cooc_matrix.to_csv(encoding='utf-8-sig').encode('utf-8-sig')
                st.download_button(
                    "ğŸ’¾ CSV ë‹¤ìš´ë¡œë“œ",
                    csv,
                    "cooccurrence_matrix.csv",
                    "text/csv",
                    key='download-cooc-csv'
                )
        else:
             if 'cooc_df' in st.session_state:
                cooc_matrix = st.session_state['cooc_df']
                st.subheader("ğŸ“‹ ë§ˆì§€ë§‰ ë¶„ì„ ê²°ê³¼ (ë™ì‹œì¶œí˜„ ë§¤íŠ¸ë¦­ìŠ¤ - English Index/Column)")
                st.dataframe(cooc_matrix, use_container_width=True)
             else:
                st.info("ğŸ‘† í‚¤ì›Œë“œ ê°œìˆ˜ë¥¼ ì„ íƒí•˜ê³  ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ë¶„ì„í•˜ì„¸ìš”.")
    
    # íƒ­ 6: í† í”½ ë¹„êµ
    with tabs[5]:
        st.header("ğŸ¬ ì˜ìƒë³„ í† í”½ ë¹„êµ")
        
        if st.button("ğŸ” í† í”½ ë¹„êµ ë¶„ì„", key="btn_topic"):
            with st.spinner("í† í”½ ë¹„êµ ë¶„ì„ ì¤‘..."):
                fig, comparison_df = analyzer.topic_comparison()
                if fig:
                    st.pyplot(fig)
                    analysis_results['topic_comparison.csv'] = comparison_df # ê²°ê³¼ ì €ì¥
                    st.session_state['topic_df'] = comparison_df # ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸ì—ë„ ì €ì¥
                    
                    st.subheader("ğŸ“‹ í† í”½ ë¹„êµ ë°ì´í„° (English Column)")
                    st.dataframe(comparison_df, use_container_width=True)
                    
                    csv = comparison_df.to_csv(encoding='utf-8-sig').encode('utf-8-sig')
                    st.download_button(
                        "ğŸ’¾ CSV ë‹¤ìš´ë¡œë“œ",
                        csv,
                        "topic_comparison.csv",
                        "text/csv",
                        key='download-topic-csv'
                    )
                else:
                    st.warning("video_title ì»¬ëŸ¼ì´ ì—†ì–´ í† í”½ ë¹„êµ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
             if 'topic_df' in st.session_state:
                comparison_df = st.session_state['topic_df']
                st.subheader("ğŸ“‹ ë§ˆì§€ë§‰ ë¶„ì„ ê²°ê³¼ (í† í”½ ë¹„êµ ë°ì´í„° - English Column)")
                st.dataframe(comparison_df, use_container_width=True)
             else:
                st.info("ğŸ‘† ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ í† í”½ ë¹„êµë¥¼ ë¶„ì„í•˜ì„¸ìš”.")

    # íƒ­ 7: ì›ë³¸ ë°ì´í„°
    with tabs[6]:
        st.header("ğŸ“‹ ì›ë³¸ ë°ì´í„°")
        
        data_type = st.radio("ë°ì´í„° ìœ í˜• ì„ íƒ", ["ëŒ“ê¸€ ë°ì´í„°", "ì˜ìƒ ë°ì´í„°"], horizontal=True)
        
        if data_type == "ëŒ“ê¸€ ë°ì´í„°":
            st.subheader("ğŸ’¬ ëŒ“ê¸€ ë°ì´í„°")
            st.dataframe(comments_df, use_container_width=True, height=600)
            
            csv = comments_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
            st.download_button(
                "ğŸ’¾ ëŒ“ê¸€ ë°ì´í„° CSV ë‹¤ìš´ë¡œë“œ",
                csv,
                "comments_data.csv",
                "text/csv",
                key='download-comments-raw'
            )
        
        else:
            if videos_df is not None and not videos_df.empty:
                st.subheader("ğŸ¥ ì˜ìƒ ë°ì´í„°")
                st.dataframe(videos_df, use_container_width=True, height=600)
                
                csv = videos_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
                st.download_button(
                    "ğŸ’¾ ì˜ìƒ ë°ì´í„° CSV ë‹¤ìš´ë¡œë“œ",
                    csv,
                    "videos_data.csv",
                    "text/csv",
                    key='download-videos-raw'
                )
            else:
                st.warning("ì˜ìƒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # ========================================
    # AI ìë™ ë³´ê³ ì„œ ìƒì„± ì„¹ì…˜ (OpenAI API í˜¸ì¶œ)
    # ========================================
    
    st.markdown("---")
    st.header("ğŸ“„ Market Insight Report Generator (OpenAI API ê¸°ë°˜)")
    st.write("ë¶„ì„ CSV íŒŒì¼ì— í¬í•¨ëœ í•µì‹¬ í‚¤ì›Œë“œì™€ í†µê³„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìš”ì•½ ë³´ê³ ì„œë¥¼ ìë™ ìƒì„±í•©ë‹ˆë‹¤. **(CSV íŒŒì¼ì„ `analysis_results` í´ë”ì— ë‹¤ìš´ë¡œë“œ í›„ ì‚¬ìš© ê°€ëŠ¥)**")

    # SAVE_DIRì—ì„œ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ì‹¤ì œ í™˜ê²½ ê°€ì •)
    try:
        available_files = [f for f in os.listdir(SAVE_DIR) if f.endswith(".csv")]
    except FileNotFoundError:
        available_files = []

    if not available_files:
        st.warning("ë¶„ì„ ê²°ê³¼ CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ìœ„ ë¶„ì„ íƒ­ì—ì„œ ë¶„ì„ì„ ì‹¤í–‰í•˜ê³  CSVë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì—¬ `analysis_results` í´ë”ì— ì €ì¥í•˜ì„¸ìš”.")
    else:
        selected_files = st.multiselect("ğŸ“‚ ë³´ê³ ì„œì— í¬í•¨í•  íŒŒì¼ ì„ íƒ", available_files, default=available_files)

        if st.button("ğŸ§  ë³´ê³ ì„œ ìƒì„±"):
            full_text = ""
            report_sentences = []  # ì—¬ëŸ¬ ë¬¸ì¥ ëª¨ì•„ì„œ ë³´ê³ ì„œ ë§Œë“¤ê¸°
            
            if not selected_files:
                st.error("ë³´ê³ ì„œ ìƒì„±ì„ ìœ„í•´ íŒŒì¼ì„ 1ê°œ ì´ìƒ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.")
                return
                
            if not OPENAI_API_KEY:
                st.error("OpenAI API Keyê°€ .env íŒŒì¼ ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
                return

            # --- 1. ì›ë³¸ ëŒ“ê¸€ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ (ìƒ˜í”Œë§ì„ ìœ„í•´) ---
            raw_comments_df = st.session_state.get('comments_df')
            if raw_comments_df is None or raw_comments_df.empty:
                 st.error("ëŒ“ê¸€ ì›ë³¸ ë°ì´í„°ê°€ ì„¸ì…˜ì— ì—†ìŠµë‹ˆë‹¤. ë°ì´í„° ìˆ˜ì§‘ ë˜ëŠ” ë¡œë“œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                 return
            
            # Sentiment ë¶„ì„ì„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì—¬ í•„ìš”í•œ ì»¬ëŸ¼(Sentiment, PositiveCount, NegativeCount)ì„ ì¶”ê°€
            temp_analyzer = YouTubeCommentAnalyzer(raw_comments_df)
            
            if 'like_count' in raw_comments_df.columns:
                 # ë³µì‚¬ë³¸ì— 'LikeCount' ì¶”ê°€ (ì›ë³¸ì„ ê±´ë“œë¦¬ì§€ ì•Šê¸° ìœ„í•´)
                 raw_comments_df['LikeCount'] = raw_comments_df['like_count']
            
            # ì—¬ê¸°ì„œ sentiment_classified_df_fullëŠ” ì›ë³¸ comments_dfì˜ ë³µì‚¬ë³¸ì— ë¶„ì„ ê²°ê³¼ê°€ ì¶”ê°€ëœ DataFrameì…ë‹ˆë‹¤.
            _, _, sentiment_classified_df_full = temp_analyzer.sentiment_keywords() 


            with st.spinner("OpenAI GPT ëª¨ë¸ì´ ë³´ê³ ì„œë¥¼ ìƒì„± ì¤‘..."):
                for f in selected_files:
                    file_path = os.path.join(SAVE_DIR, f)
                    try:
                        df = pd.read_csv(file_path, encoding="utf-8-sig")

                        keywords = f"File: {f}. "
                        
                        if 'Frequency' in df.columns: # Keyword Frequency file
                            top_keyword = df.iloc[0]['Keyword']
                            top_count = df.iloc[0]['Frequency']
                            # English prompt structure
                            keywords += f"Top keyword is '{top_keyword}' with count {top_count}. Total unique keywords: {len(df)}. "
                        
                        elif 'Sentiment' in df.columns: # Sentiment Analysis file
                            sentiment_counts = df['Sentiment'].value_counts()
                            # UIì—ì„œ í•œê¸€ 'ê¸ì •'/'ë¶€ì •'ì„ ì‚¬ìš©í•˜ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                            pos = sentiment_counts.get('ê¸ì •', 0)
                            neg = sentiment_counts.get('ë¶€ì •', 0)
                            total = len(df)
                            pos_ratio = pos / total * 100 if total > 0 else 0
                            
                            # --- Use full classified DF for robust sampling ---
                            positive_samples = sentiment_classified_df_full[
                                (sentiment_classified_df_full['Sentiment'] == 'ê¸ì •') & 
                                (sentiment_classified_df_full['LikeCount'] > 0) 
                            ].sort_values(by='LikeCount', ascending=False)['Text'].head(3).tolist()
                            
                            if positive_samples:
                                # T5 input needs English structure
                                # Korean comments are passed as part of the English prompt for context
                                # Preprocess the text to remove symbols that might confuse the T5 model
                                clean_samples = [temp_analyzer.preprocess_text(s) for s in positive_samples]
                                sample_text = "Sample positive comments: " + " | ".join(clean_samples)
                                keywords += sample_text + " "
                            # --- END NEW SAMPLING LOGIC ---

                            # English prompt structure (stats must be after sample text for flow)
                            keywords += f"Total comments {total}. Positive comments: {pos} ({pos_ratio:.1f}%). Negative comments: {neg}. The overall sentiment is mostly Positive. "
                        
                        elif 'CommentCount' in df.columns: # Time Trend file
                             df['Date'] = pd.to_datetime(df['Date'])
                             max_comments_date = df.loc[df['CommentCount'].idxmax(), 'Date'].strftime('%Y-%m-%d')
                             max_comments_count = df['CommentCount'].max()
                             # English prompt structure
                             keywords += f"Peak comment count {max_comments_count} occurred on {max_comments_date}. Average comments per period is {df['CommentCount'].mean():.1f}. "
                        
                        elif 'Keyword1' in df.columns and 'Keyword2' in df.columns: # Cooccurrence Matrix
                            # ë™ì‹œì¶œí˜„ ë§¤íŠ¸ë¦­ìŠ¤ ì²˜ë¦¬ëŠ” ë³µì¡í•˜ë¯€ë¡œ ë‹¨ìˆœí™”ëœ í‚¤ì›Œë“œë¥¼ ì „ë‹¬
                            keywords += f"Co-occurrence matrix data. Top-left value (self-cooccurrence) is {str(df.iloc[0, 0])}. Analyzing relationships between {len(df)} keywords. "
                        
                        elif 'Keyword1' in df.columns: # Topic Comparison file
                            # FIX: Cooccurrence Matrix ì˜¤ë¥˜ ìˆ˜ì • í›„ Topic Comparison ì²˜ë¦¬ ë¡œì§ ë‹¤ì‹œ í™•ì¸
                            # Topic Comparison íŒŒì¼ì€ ì¸ë±ìŠ¤ê°€ ì˜ìƒ ì œëª©ì´ê³  ì»¬ëŸ¼ì´ Keyword1, Keyword2... ì„.
                            top_video_topic = df.index[0]
                            # FIX: Explicitly convert list elements to string before joining
                            key_terms = [str(x) for x in df.iloc[0].dropna().tolist()]
                            keywords += f"The top video topic is '{top_video_topic}' with key terms: {', '.join(key_terms)}. "
                        
                        # --- NEW LOGIC FOR RAW COMMENT DATA (comments_data.csv) ---
                        elif f == "comments_data.csv" and 'text' in df.columns and 'like_count' in df.columns:
                            total_comments = len(df)
                            avg_likes = df['like_count'].mean()
                            
                            # Get top 3 comments by like count from the CSV file itself
                            top_comments = df.sort_values(by='like_count', ascending=False)['text'].head(3).tolist()
                            
                            # Preprocess the text before sending to T5
                            clean_samples = [temp_analyzer.preprocess_text(s) for s in top_comments]
                            top_comment_text = " | ".join(clean_samples)
                            
                            keywords += f"Raw comment data summary. Total records: {total_comments}. Average likes per comment: {avg_likes:.1f}. Top comments by like count: {top_comment_text}. "
                        # --- END NEW LOGIC ---
                        
                        else:
                            # General/Other file
                            keywords += f"Dataset rows: {len(df)}. Columns: {', '.join(df.columns)}. Data statistics available. "
                        
                        full_text += keywords  # ì „ì²´ í‚¤ì›Œë“œ ëª¨ìŒ
                        
                        # ë¬¸ì¥ ìƒì„± (OpenAI API í˜¸ì¶œ)
                        sentence = generate_openai_report(keywords, OPENAI_API_KEY) 
                        report_sentences.append(f"**{f} Insight:** {sentence}") # Change header to English
                        
                    except Exception as e:
                        # íŒŒì¼ êµ¬ì¡° ì˜¤ë¥˜ ì‹œ ì—ëŸ¬ ë©”ì‹œì§€ì— íŒŒì¼ëª… í¬í•¨
                        st.error(f"íŒŒì¼ {f} ì²˜ë¦¬ ì˜¤ë¥˜: CSV íŒŒì¼ êµ¬ì¡° í™•ì¸ í•„ìš”. {str(e)}")
                        continue

            if report_sentences:
                # í•©ì³ì§„ ë³´ê³ ì„œ
                summary = "\n\n".join(report_sentences)
                
                # ìµœì¢… ë³´ê³ ì„œ ì œëª©ë„ ì˜ì–´ë¡œ ë³€ê²½
                final_report = f"""
# YouTube Analysis Auto-Generated Report
## Generated At: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

{summary}
"""
                
                st.subheader("ğŸ“ˆ AI ìë™ ìƒì„± ë³´ê³ ì„œ ì´ˆì•ˆ")
                st.text_area("ìš”ì•½ ê²°ê³¼", final_report, height=400)
                
                st.download_button(
                    "ğŸ’¾ ìš”ì•½ ë³´ê³ ì„œ ì €ì¥",
                    final_report.encode("utf-8-sig"),
                    "Market_Insight_Report.txt",
                    "text/plain"
                )
            else:
                st.error("ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨. íŒŒì¼ ì„ íƒ ë° êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")


if __name__ == "__main__":
    main()
