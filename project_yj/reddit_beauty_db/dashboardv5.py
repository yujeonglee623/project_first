import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from datetime import datetime
import re
from wordcloud import WordCloud
import praw
from dotenv import load_dotenv
import os
import requests 
import io 
from prawcore.exceptions import ResponseException, RequestException

# ========================================
# Streamlit ê¸°ë³¸ ì„¤ì • ë° OpenAI ì„¤ì •
# ========================================

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="Reddit ë¶„ì„ ëŒ€ì‹œë³´ë“œ",
    page_icon="ğŸ”´",
    layout="wide"
)

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False

# ë³´ê³ ì„œ ì €ì¥ì„ ìœ„í•œ ë””ë ‰í† ë¦¬ ì„¤ì • ë° ìë™ ìƒì„±
SAVE_DIR = "analysis_results"
os.makedirs(SAVE_DIR, exist_ok=True)


def generate_openai_report(keywords, user_prompt, api_key, model_name="gpt-4o"):
    """ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ì™€ ë¶„ì„ ë°ì´í„°ë¥¼ ì¡°í•©í•˜ì—¬ OpenAI APIë¥¼ ì´ìš©í•œ ì¼ë°˜ ë³´ê³ ì„œ ë¬¸ì¥ ìƒì„± (ì˜ë¬¸)"""
    
    if not api_key:
        return "Error: OpenAI API Key is missing. Please set the OPENAI_API_KEY in the .env file."

    # System Prompt: AIì˜ ì—­í• ê³¼ ì›í•˜ëŠ” ì¶œë ¥ í˜•ì‹ì„ ëª…í™•íˆ ì •ì˜
    system_prompt = (
        "You are a professional Social Media Market Analyst. "
        "Your task is to analyze the provided raw data summary or statistical analysis "
        "and generate a comprehensive, insightful, and professional English summary based *strictly* on the user's focus prompt. "
        "The summary should be concise (around 5 detailed sentences) and must address the user's specific questions/focus. "
        "If Korean content is provided, translate and interpret it within the context of the user's request. "
        "Do not use markdown headers or lists. Just provide the summary text."
    )
    
    # User Prompt: ì‚¬ìš©ì ì…ë ¥ê³¼ ë¶„ì„ ë°ì´í„°(keywords)ë¥¼ í•©ì³ì„œ ì „ë‹¬
    full_user_prompt = (
        f"**User Focus:** {user_prompt}\n\n"
        f"**Analysis Data for Context:** {keywords}\n\n"
        "Generate the insightful report summary in English, focusing on the User Focus above."
    )

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}",
    }

    # API Payload
    payload = {
        "model": model_name,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": full_user_prompt}
        ],
        "max_tokens": 400,
        "temperature": 0.3, 
    }

    try:
        response = requests.post("https://api.openai.com/v1/chat/completions", headers=headers, json=payload, timeout=40)
        response.raise_for_status()
        result = response.json()
        summary = result['choices'][0]['message']['content'].strip()
        return summary
    except Exception as e:
        return f"API Error occurred: {e}. Check API key or rate limits."


def generate_executive_report(keywords, user_prompt, api_key, model_name="gpt-4o"):
    """ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ì™€ ë¶„ì„ ë°ì´í„°ë¥¼ ì¡°í•©í•˜ì—¬ OpenAI APIë¥¼ ì´ìš©í•œ ì„ì›ì§„ ë³´ê³ ì„œ ìƒì„± (êµ­ë¬¸+ì˜ë¬¸ ë¶„ë¦¬)"""
    
    if not api_key:
        return "Error: OpenAI API Key is missing."

    # System Prompt: AIì˜ ì—­í• ê³¼ ì›í•˜ëŠ” ì¶œë ¥ í˜•ì‹ì„ ëª…í™•íˆ ì •ì˜ (êµ­ë¬¸/ì˜ë¬¸ ë¶„ë¦¬ ìš”ì²­)
    system_prompt = (
        "You are a Senior Executive Market Analyst. "
        "Your task is to analyze the provided data and generate a highly summarized, actionable **Executive Report** based on the user's focus. "
        "The response MUST contain two clearly separated sections: **English Summary** (5-7 sentences) and its **Korean Summary** (5-7 sentences). "
        "The output format MUST strictly follow the structure: 'English Summary: [English Text] Korean Summary: [Korean Text]'. "
        "Focus on key insights, strategic implications, and high-level trends. "
    )
    
    # User Prompt: ì‚¬ìš©ì ì…ë ¥ê³¼ ë¶„ì„ ë°ì´í„°(keywords)ë¥¼ í•©ì³ì„œ ì „ë‹¬
    full_user_prompt = (
        f"**User Focus for Executive Report:** {user_prompt}\n\n"
        f"**Analysis Data for Context:** {keywords}\n\n"
        "Generate the Executive Report in the required dual-language format."
    )

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}",
    }

    # API Payload (í† í°ì„ ì¡°ê¸ˆ ë” ëŠ˜ë ¤ êµ­ë¬¸/ì˜ë¬¸ ëª¨ë‘ ìƒì„± ê°€ëŠ¥í•˜ë„ë¡ í•¨)
    payload = {
        "model": model_name,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": full_user_prompt}
        ],
        "max_tokens": 800, # ë‘ ì–¸ì–´ë¥¼ ëª¨ë‘ ë‹´ê¸° ìœ„í•´ í† í° ì¦ê°€
        "temperature": 0.3, 
    }

    try:
        response = requests.post("https://api.openai.com/v1/chat/completions", headers=headers, json=payload, timeout=50)
        response.raise_for_status()
        result = response.json()
        summary = result['choices'][0]['message']['content'].strip()
        
        # ê²°ê³¼ íŒŒì‹±
        eng_match = re.search(r'English Summary:\s*(.*?)\s*Korean Summary:', summary, re.DOTALL)
        kor_match = re.search(r'Korean Summary:\s*(.*)', summary, re.DOTALL)

        english_summary = eng_match.group(1).strip() if eng_match else "Parsing Error: English Summary not found."
        korean_summary = kor_match.group(1).strip() if kor_match else "Parsing Error: Korean Summary not found."

        return english_summary, korean_summary

    except Exception as e:
        error_msg = f"API Error occurred: {e}. Check API key or rate limits."
        return error_msg, error_msg

# ========================================
# Reddit ë¶„ì„ í´ë˜ìŠ¤
# ========================================
class RedditAnalyzer:
    """Reddit ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self, posts_df, comments_df=None):
        self.posts_df = posts_df.copy()
        self.comments_df = comments_df.copy() if comments_df is not None and not comments_df.empty else None
        
        # ë‚ ì§œ ì»¬ëŸ¼ ë³€í™˜ 
        # API ìˆ˜ì§‘ ì‹œ created_utcê°€ UTC íƒ€ì„ìŠ¤íƒ¬í”„(ì´ˆ)ë¡œ ë“¤ì–´ì™€ì•¼ í•˜ì§€ë§Œ, 
        # ì´ë¯¸ datetime ê°ì²´ë¡œ ë³€í™˜ëœ ê²½ìš°ë¥¼ ìœ„í•´ ì˜¤ë¥˜ ì²˜ë¦¬ ì¶”ê°€
        if 'created_utc' in self.posts_df.columns:
            # ë¬¸ìì—´ í˜•íƒœì¼ ê²½ìš° pd.to_datetimeìœ¼ë¡œ ë³€í™˜ ì‹œë„
            if self.posts_df['created_utc'].dtype == 'object':
                 self.posts_df['created_utc'] = pd.to_datetime(self.posts_df['created_utc'], errors='coerce') 
            # ì •ìˆ˜í˜•íƒœì¼ ê²½ìš° UTC(ì´ˆ)ë¡œ ê°„ì£¼í•˜ê³  ë³€í™˜ ì‹œë„
            elif self.posts_df['created_utc'].dtype in ['int64', 'float64']:
                 self.posts_df['created_utc'] = pd.to_datetime(self.posts_df['created_utc'], unit='s', errors='coerce') 

        if self.comments_df is not None and 'created_utc' in self.comments_df.columns:
             # ë¬¸ìì—´ í˜•íƒœì¼ ê²½ìš° pd.to_datetimeìœ¼ë¡œ ë³€í™˜ ì‹œë„
            if self.comments_df['created_utc'].dtype == 'object':
                 self.comments_df['created_utc'] = pd.to_datetime(self.comments_df['created_utc'], errors='coerce') 
            # ì •ìˆ˜í˜•íƒœì¼ ê²½ìš° UTC(ì´ˆ)ë¡œ ê°„ì£¼í•˜ê³  ë³€í™˜ ì‹œë„
            elif self.comments_df['created_utc'].dtype in ['int64', 'float64']:
                 self.comments_df['created_utc'] = pd.to_datetime(self.comments_df['created_utc'], unit='s', errors='coerce')
    
    
    def preprocess_text(self, text):
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        if pd.isna(text) or text is None:
            return ""
        
        text = str(text).lower()
        text = re.sub(r'http\S+|www\S+', '', text)
        text = re.sub(r'[^ê°€-í£a-z0-9\s]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    
    def extract_keywords(self, text_series, top_n=50):
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        all_text = ' '.join(text_series.fillna('').apply(self.preprocess_text))
        words = all_text.split()
        words = [w for w in words if len(w) >= 2]
        
        stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
                     'of', 'is', 'are', 'was', 'were', 'been', 'be', 'have', 'has', 'had',
                     'i', 'me', 'my', 'you', 'your', 'it', 'its', 'not', 'no', 'yes', 'we',
                     'ê·¸', 'ì´', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë“¤', 'ë°', 'ë˜í•œ', 'í•˜ë‹¤', 'ìˆë‹¤', 'ë˜ë‹¤',
                     'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ê·¸ëŸ°', 'ì´ëŸ°', 'ì €ëŸ°', 'removed', 'deleted'}
        
        words = [w for w in words if w not in stopwords]
        word_freq = Counter(words)
        
        return word_freq.most_common(top_n)
    
    
    def wordcloud(self, text_series, width=1200, height=800):
        """ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±"""
        all_text = ' '.join(text_series.fillna('').apply(self.preprocess_text))
        
        try:
             font_path = 'C:/Windows/Fonts/malgun.ttf' 
        except:
             font_path = None

        wordcloud = WordCloud(
            font_path=font_path,
            width=width,
            height=height,
            background_color='white',
            max_words=100,
            relative_scaling=0.3,
            colormap='viridis'
        ).generate(all_text)
        
        fig, ax = plt.subplots(figsize=(15, 10))
        ax.imshow(wordcloud, interpolation='bilinear')
        ax.axis('off')
        ax.set_title('Reddit í…ìŠ¤íŠ¸ ì›Œë“œí´ë¼ìš°ë“œ', fontsize=20, pad=20)
        plt.tight_layout()
        
        return fig
    
    
    def keyword_frequency(self, text_series, top_n=20):
        """í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„"""
        keywords = self.extract_keywords(text_series, top_n=top_n)
        
        if not keywords:
             return None, pd.DataFrame(columns=['í‚¤ì›Œë“œ', 'ë¹ˆë„'])

        words, counts = zip(*keywords)
        
        fig, ax = plt.subplots(figsize=(12, 8))
        ax.barh(range(len(words)), counts, color='orangered')
        ax.set_yticks(range(len(words)))
        ax.set_yticklabels(words)
        ax.set_xlabel('ë¹ˆë„', fontsize=12)
        ax.set_title(f'ìƒìœ„ {top_n}ê°œ í‚¤ì›Œë“œ ë¹ˆë„', fontsize=16, pad=20)
        ax.invert_yaxis()
        plt.tight_layout()
        
        freq_df = pd.DataFrame(keywords, columns=['í‚¤ì›Œë“œ', 'ë¹ˆë„'])
        
        return fig, freq_df
    
    
    def sentiment_analysis(self, text_series, data_df):
        """ê°ì„± ë¶„ì„"""
        positive_words = {
            'good', 'great', 'excellent', 'amazing', 'wonderful', 'fantastic',
            'best', 'love', 'awesome', 'perfect', 'nice', 'happy', 'thank',
            'ì¢‹ë‹¤', 'ìµœê³ ', 'ëŒ€ë°•', 'ì˜ˆì˜ë‹¤', 'ë©‹ì§€ë‹¤', 'ì™„ë²½', 'ê°ì‚¬', 'í–‰ë³µ', 'ì¢‹ì•„ìš”'
        }
        
        negative_words = {
            'bad', 'worst', 'terrible', 'awful', 'horrible', 'hate',
            'poor', 'disappointing', 'useless', 'waste', 'crap',
            'ì‹«ë‹¤', 'ë³„ë¡œ', 'ë‚˜ì˜ë‹¤', 'ìµœì•…', 'í˜•í¸ì—†ë‹¤', 'ì‹¤ë§', 'ë³„ë¡œë„¤'
        }
        
        def calculate_sentiment(text):
            text = self.preprocess_text(text)
            words = text.split()
            
            pos_count = sum(1 for w in words if w in positive_words)
            neg_count = sum(1 for w in words if w in negative_words)
            
            if pos_count > neg_count:
                return 'ê¸ì •'
            elif pos_count < neg_count:
                return 'ë¶€ì •'
            else:
                return 'ì¤‘ë¦½'
        
        sentiments = text_series.fillna('').apply(calculate_sentiment)
        
        df_copy = data_df.copy().reset_index(drop=True)
        df_copy['Sentiment'] = sentiments
        sentiment_counts = sentiments.value_counts()
        
        if sentiment_counts.empty:
            return None, pd.Series(), df_copy[['Sentiment']]

        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        
        colors = ['#90EE90', '#FFB6C1', '#D3D3D3']
        
        order = ['ê¸ì •', 'ë¶€ì •', 'ì¤‘ë¦½']
        ordered_counts = sentiment_counts.reindex(order, fill_value=0)
        ordered_counts = ordered_counts[ordered_counts > 0]
        ordered_colors = [c for s, c in zip(order, colors) if s in ordered_counts.index]

        axes[0].pie(ordered_counts.values, labels=ordered_counts.index, 
                      autopct='%1.1f%%', colors=ordered_colors, startangle=90)
        axes[0].set_title('ê°ì„± ë¶„í¬', fontsize=14, pad=20)
        
        axes[1].bar(ordered_counts.index, ordered_counts.values, color=ordered_colors)
        axes[1].set_xlabel('ê°ì„±', fontsize=12)
        axes[1].set_ylabel('ê°œìˆ˜', fontsize=12)
        axes[1].set_title('ê°ì„±ë³„ ê°œìˆ˜', fontsize=14, pad=20)
        
        plt.tight_layout()
        
        sentiment_df = df_copy.rename(columns={'title': 'ì œëª©', 'selftext': 'ë³¸ë¬¸', 'body': 'ë³¸ë¬¸_ë˜ëŠ”_ë‚´ìš©', 'score': 'ì ìˆ˜'})
        
        if text_series.name == 'title':
            sentiment_df = sentiment_df[['Sentiment', 'ì œëª©', 'ì ìˆ˜']]
        elif text_series.name == 'selftext':
            sentiment_df = sentiment_df[['Sentiment', 'ë³¸ë¬¸', 'ì ìˆ˜']]
        else: # ëŒ“ê¸€ ë³¸ë¬¸
            sentiment_df = sentiment_df[['Sentiment', 'ë³¸ë¬¸_ë˜ëŠ”_ë‚´ìš©', 'ì ìˆ˜']]
            
        return fig, sentiment_counts, sentiment_df
    
    
    def time_trend(self, df, date_col='created_utc', interval='D'):
        """ì‹œê°„ëŒ€ë³„ íŠ¸ë Œë“œ ë¶„ì„"""
        if date_col not in df.columns:
            return None, None
        
        # 1. ë‚ ì§œ ì»¬ëŸ¼ì„ ì—ëŸ¬ë¥¼ ë¬´ì‹œí•˜ê³  datetime íƒ€ì…ìœ¼ë¡œ ê°•ì œ ë³€í™˜ (CSV ì—…ë¡œë“œ ëŒ€ë¹„)
        df_valid = df.copy()
        df_valid[date_col] = pd.to_datetime(df_valid[date_col], errors='coerce') 

        # 2. ìœ íš¨í•œ ë‚ ì§œê°€ ìˆëŠ” í–‰ë§Œ ì‚¬ìš©
        df_valid = df_valid.dropna(subset=[date_col])
        if df_valid.empty:
            st.warning("ìœ íš¨í•œ ë‚ ì§œ ë°ì´í„°ê°€ ì—†ì–´ ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None, None
        
        # 3. ë‚ ì§œ ì»¬ëŸ¼ì„ ì¸ë±ìŠ¤ë¡œ ì„¤ì • (ì´ë•Œ DatetimeIndexê°€ ìƒì„±ë¨)
        df_sorted = df_valid.reset_index(drop=True).set_index(date_col).sort_index()

        time_counts = df_sorted.resample(interval).size()
        
        # 'score' ì»¬ëŸ¼ì´ df_sortedì— ìˆëŠ”ì§€ í™•ì¸
        if 'score' in df_sorted.columns:
            time_scores = df_sorted['score'].resample(interval).sum()
        else:
            time_scores = None
        
        fig = None
        
        if time_scores is not None:
            fig, axes = plt.subplots(2, 1, figsize=(14, 10))
            
            axes[0].plot(time_counts.index, time_counts.values, marker='o', linewidth=2, color='orangered')
            axes[0].set_xlabel('ë‚ ì§œ', fontsize=12)
            axes[0].set_ylabel('ê²Œì‹œë¬¼/ëŒ“ê¸€ ìˆ˜', fontsize=12)
            axes[0].set_title('ì‹œê°„ëŒ€ë³„ ê²Œì‹œë¬¼/ëŒ“ê¸€ ìˆ˜ ì¶”ì´', fontsize=14, pad=20)
            axes[0].grid(True, alpha=0.3)
            
            axes[1].plot(time_scores.index, time_scores.values, marker='o', 
                          color='coral', linewidth=2)
            axes[1].set_xlabel('ë‚ ì§œ', fontsize=12)
            axes[1].set_ylabel('ì ìˆ˜ í•©ê³„', fontsize=12)
            axes[1].set_title('ì‹œê°„ëŒ€ë³„ ì ìˆ˜ ì¶”ì´', fontsize=14, pad=20)
            axes[1].grid(True, alpha=0.3)
            
            trend_df = pd.DataFrame({
                'ë‚ ì§œ': time_counts.index,
                'ê°œìˆ˜': time_counts.values,
                'ì ìˆ˜': time_scores.values
            })
        else:
            fig, ax = plt.subplots(figsize=(14, 6))
            
            ax.plot(time_counts.index, time_counts.values, marker='o', linewidth=2, color='orangered')
            ax.set_xlabel('ë‚ ì§œ', fontsize=12)
            ax.set_ylabel('ê²Œì‹œë¬¼/ëŒ“ê¸€ ìˆ˜', fontsize=12)
            ax.set_title('ì‹œê°„ëŒ€ë³„ ê²Œì‹œë¬¼/ëŒ“ê¸€ ìˆ˜ ì¶”ì´', fontsize=14, pad=20)
            ax.grid(True, alpha=0.3)
            
            trend_df = pd.DataFrame({
                'ë‚ ì§œ': time_counts.index,
                'ê°œìˆ˜': time_counts.values
            })
        
        plt.tight_layout()
        
        return fig, trend_df
    
    
    def subreddit_comparison(self):
        """ì„œë¸Œë ˆë”§ë³„ ë¹„êµ ë¶„ì„"""
        if 'subreddit' not in self.posts_df.columns or self.posts_df['subreddit'].nunique() < 2:
            return None, None
        
        subreddit_stats = self.posts_df.groupby('subreddit').agg({
            'score': ['mean', 'sum', 'count'],
            'num_comments': 'mean'
        }).round(2)
        
        subreddit_stats.columns = ['í‰ê· _ì ìˆ˜', 'ì´_ì ìˆ˜', 'ê²Œì‹œë¬¼_ìˆ˜', 'í‰ê· _ëŒ“ê¸€ìˆ˜']
        subreddit_stats.index.name = 'ì„œë¸Œë ˆë”§'
        subreddit_stats = subreddit_stats.sort_values('ì´_ì ìˆ˜', ascending=False)
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        axes[0, 0].bar(subreddit_stats.index, subreddit_stats['ê²Œì‹œë¬¼_ìˆ˜'], color='orangered')
        axes[0, 0].set_title('ì„œë¸Œë ˆë”§ë³„ ê²Œì‹œë¬¼ ìˆ˜', fontsize=14)
        axes[0, 0].set_ylabel('ê²Œì‹œë¬¼ ìˆ˜')
        axes[0, 0].tick_params(axis='x', rotation=45)
        
        axes[0, 1].bar(subreddit_stats.index, subreddit_stats['ì´_ì ìˆ˜'], color='coral')
        axes[0, 1].set_title('ì„œë¸Œë ˆë”§ë³„ ì´ ì ìˆ˜', fontsize=14)
        axes[0, 1].set_ylabel('ì´ ì ìˆ˜')
        axes[0, 1].tick_params(axis='x', rotation=45)
        
        axes[1, 0].bar(subreddit_stats.index, subreddit_stats['í‰ê· _ì ìˆ˜'], color='tomato')
        axes[1, 0].set_title('ì„œë¸Œë ˆë”§ë³„ í‰ê·  ì ìˆ˜', fontsize=14)
        axes[1, 0].set_ylabel('í‰ê·  ì ìˆ˜')
        axes[1, 0].tick_params(axis='x', rotation=45)
        
        axes[1, 1].bar(subreddit_stats.index, subreddit_stats['í‰ê· _ëŒ“ê¸€ìˆ˜'], color='lightsalmon')
        axes[1, 1].set_title('ì„œë¸Œë ˆë”§ë³„ í‰ê·  ëŒ“ê¸€ ìˆ˜', fontsize=14)
        axes[1, 1].set_ylabel('í‰ê·  ëŒ“ê¸€ ìˆ˜')
        axes[1, 1].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        
        return fig, subreddit_stats


def search_and_collect_reddit_data(subreddit_names, search_query, post_limit, sort_by, time_filter, collect_comments, comment_limit):
    # (API ìˆ˜ì§‘ í•¨ìˆ˜)
    load_dotenv()
    
    # User-Agentë¥¼ .env íŒŒì¼ì—ì„œ ê°€ì ¸ì˜¤ì§€ ëª»í•  ê²½ìš° ê¸°ë³¸ê°’ ì„¤ì •
    client_id = os.getenv("REDDIT_CLIENT_ID")
    client_secret = os.getenv("REDDIT_CLIENT_SECRET")
    # PRAWëŠ” client_idì™€ secretì„ ì‚¬ìš©í•˜ëŠ” ì¸ì¦(Application Only) ì‹œì—ë„ User-Agentë¥¼ ìš”êµ¬í•¨
    user_agent = os.getenv("REDDIT_USER_AGENT", "RedditAnalyzer_Streamlit_v1.0 (by Python)") 
    
    # ì‚¬ìš©ì ì´ë¦„ê³¼ ë¹„ë°€ë²ˆí˜¸ë¥¼ ì¶”ê°€ë¡œ ë¡œë“œ (User/Password ì¸ì¦ ì‹œ í•„ìš”)
    username = os.getenv("REDDIT_USERNAME")
    password = os.getenv("REDDIT_PASSWORD")
    
    if not client_id or not client_secret:
        st.error("Reddit API ìê²©ì¦ëª…ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        st.info("í•„ìš”í•œ í™˜ê²½ ë³€ìˆ˜: REDDIT_CLIENT_ID, REDDIT_CLIENT_SECRET, REDDIT_USERNAME(ì„ íƒ), REDDIT_PASSWORD(ì„ íƒ)")
        return None, None
    
    try:
        if username and password:
            # User/Password ì¸ì¦ (ê°€ì¥ ì¼ë°˜ì ì¸ ìŠ¤í¬ë¦½íŠ¸ ì ‘ê·¼ ë°©ì‹)
            reddit = praw.Reddit(
                client_id=client_id,
                client_secret=client_secret,
                user_agent=user_agent,
                username=username,
                password=password
            )
        else:
            # Application Only ì¸ì¦ (ì½ê¸° ì „ìš©, ì¼ë¶€ ê¸°ëŠ¥ ì œí•œë  ìˆ˜ ìˆìŒ)
            reddit = praw.Reddit(
                client_id=client_id,
                client_secret=client_secret,
                user_agent=user_agent
            )

    except Exception as e:
        st.error(f"Reddit API ì—°ê²° ì˜¤ë¥˜: {e}")
        return None, None
    
    all_posts = []
    all_comments = []
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    subreddit_list = [s.strip() for s in subreddit_names.split(',') if s.strip()]
    total_subreddits = len(subreddit_list)
    
    if total_subreddits == 0:
        st.warning("ìˆ˜ì§‘í•  ì„œë¸Œë ˆë”§ ì´ë¦„ì´ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None, None

    for idx, subreddit_name in enumerate(subreddit_list):
        status_text.text(f"ì„œë¸Œë ˆë”§ {idx+1}/{total_subreddits}: r/{subreddit_name} ìˆ˜ì§‘ ì¤‘...")
        
        try:
            try:
                subreddit = reddit.subreddit(subreddit_name)
                # ì„œë¸Œë ˆë”§ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ì„ ìœ„í•´ titleì— ì ‘ê·¼ ì‹œë„
                _ = subreddit.title 
            except ResponseException as e:
                st.warning(f"ì„œë¸Œë ˆë”§ r/{subreddit_name}ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìŠ¤í‚µí•©ë‹ˆë‹¤. ì˜¤ë¥˜: {e}")
                progress_bar.progress((idx + 1) / total_subreddits)
                continue

            # ê²€ìƒ‰ ì¿¼ë¦¬ê°€ ìˆëŠ” ê²½ìš° search() ì‚¬ìš©
            if search_query:
                posts = subreddit.search(search_query, sort=sort_by, time_filter=time_filter, limit=post_limit)
            # ê²€ìƒ‰ ì¿¼ë¦¬ê°€ ì—†ëŠ” ê²½ìš° ì •ë ¬ ë°©ì‹ì— ë”°ë¼ í•´ë‹¹ ì„œë¸Œë ˆë”§ì˜ ê¸€ ëª©ë¡ ê°€ì ¸ì˜´
            else:
                if sort_by == 'hot':
                    posts = subreddit.hot(limit=post_limit)
                elif sort_by == 'new':
                    posts = subreddit.new(limit=post_limit)
                elif sort_by == 'top':
                    posts = subreddit.top(time_filter=time_filter, limit=post_limit)
                elif sort_by == 'rising':
                    posts = subreddit.rising(limit=post_limit)
                else:
                    posts = subreddit.hot(limit=post_limit) # ê¸°ë³¸ê°’

            post_count = 0
            for post in posts:
                post_data = {
                    'post_id': post.id,
                    'subreddit': post.subreddit.display_name if hasattr(post.subreddit, 'display_name') else subreddit_name,
                    'title': post.title,
                    'selftext': post.selftext,
                    'author': str(post.author) if post.author else '[deleted]',
                    'score': post.score,
                    'upvote_ratio': post.upvote_ratio,
                    'num_comments': post.num_comments,
                    'created_utc': post.created_utc,
                    'url': post.url,
                    'permalink': f"https://reddit.com{post.permalink}"
                }
                all_posts.append(post_data)
                post_count += 1
                
                if collect_comments and post.num_comments > 0:
                    try:
                        # 'More Comments' ê°ì²´ë¥¼ ì œê±°í•˜ê³  ì‹¤ì œ ëŒ“ê¸€ë§Œ ê°€ì ¸ì˜´
                        post.comments.replace_more(limit=0)
                        comments = post.comments.list()[:comment_limit]
                        
                        for comment in comments:
                            if hasattr(comment, 'body') and comment.body is not None:
                                comment_data = {
                                    'comment_id': comment.id,
                                    'post_id': post.id,
                                    'subreddit': post.subreddit.display_name if hasattr(post.subreddit, 'display_name') else subreddit_name,
                                    'author': str(comment.author) if comment.author else '[deleted]',
                                    'body': comment.body,
                                    'score': comment.score,
                                    'created_utc': comment.created_utc,
                                    'post_title': post.title
                                }
                                all_comments.append(comment_data)
                    except Exception as e:
                        st.warning(f"ê²Œì‹œë¬¼ {post.id} ëŒ“ê¸€ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            
        except Exception as e:
            st.error(f"ì„œë¸Œë ˆë”§ r/{subreddit_name} ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        
        progress_bar.progress((idx + 1) / total_subreddits)
    
    progress_bar.empty()
    status_text.empty()
    
    posts_df = pd.DataFrame(all_posts)
    comments_df = pd.DataFrame(all_comments) if all_comments else None
    
    return posts_df, comments_df


# ========================================
# Streamlit ë©”ì¸ ì•±
# ========================================

def main():
    st.title("ğŸ”´ Reddit ë¶„ì„ ëŒ€ì‹œë³´ë“œ")
    st.markdown("---")
    
    load_dotenv()
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

    # ì‚¬ì´ë“œë°” - ë°ì´í„° ìˆ˜ì§‘/ì—…ë¡œë“œ
    st.sidebar.header("ğŸ“‚ ë°ì´í„° ì†ŒìŠ¤")
    data_source = st.sidebar.radio(
        "ë°ì´í„° ì…ë ¥ ë°©ì‹ ì„ íƒ",
        ["APIë¡œ ì‹¤ì‹œê°„ ìˆ˜ì§‘", "CSV íŒŒì¼ ì—…ë¡œë“œ"]
    )
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ë° ë¡œë“œ
    if 'posts_df' not in st.session_state:
        st.session_state['posts_df'] = pd.DataFrame(columns=['post_id', 'title', 'subreddit', 'score', 'num_comments'])
    if 'comments_df' not in st.session_state:
        st.session_state['comments_df'] = None 
    
    posts_df = st.session_state['posts_df']
    comments_df = st.session_state['comments_df']
    
    if data_source == "APIë¡œ ì‹¤ì‹œê°„ ìˆ˜ì§‘":
        st.sidebar.subheader("ğŸ” ê²€ìƒ‰ ì„¤ì •")
        subreddit_names = st.sidebar.text_input(
            "ì„œë¸Œë ˆë”§ (ì‰¼í‘œë¡œ êµ¬ë¶„)",
            value="kbeauty,AsianBeauty",
            help="ì˜ˆ: kbeauty,AsianBeauty,SkincareAddiction"
        )
        search_query = st.sidebar.text_input(
            "ê²€ìƒ‰ì–´ (ì„ íƒ)",
            value="",
            help="ë¹„ì›Œë‘ë©´ ì„œë¸Œë ˆë”§ì˜ 'sort_by'ì— í•´ë‹¹í•˜ëŠ” ê²Œì‹œë¬¼ ìˆ˜ì§‘"
        )
        post_limit = st.sidebar.slider("ê²Œì‹œë¬¼ ê°œìˆ˜", 10, 500, 100)
        
        sort_by = st.sidebar.selectbox(
            "ì •ë ¬ ë°©ì‹",
            ["hot", "new", "top", "rising"],
            format_func=lambda x: {
                "hot": "ì¸ê¸°ìˆœ",
                "new": "ìµœì‹ ìˆœ",
                "top": "ìµœê³  í‰ì ",
                "rising": "ê¸‰ìƒìŠ¹"
            }[x]
        )
        
        time_filter = st.sidebar.selectbox(
            "ê¸°ê°„ í•„í„° (top/searchë§Œ ì ìš©)",
            ["all", "day", "week", "month", "year"],
            format_func=lambda x: {
                "all": "ì „ì²´",
                "day": "ì˜¤ëŠ˜",
                "week": "ì´ë²ˆ ì£¼",
                "month": "ì´ë²ˆ ë‹¬",
                "year": "ì˜¬í•´"
            }[x]
        )
        
        collect_comments = st.sidebar.checkbox("ëŒ“ê¸€ë„ ìˆ˜ì§‘", value=True)
        comment_limit = st.sidebar.slider("ê²Œì‹œë¬¼ë‹¹ ëŒ“ê¸€ ìˆ˜", 10, 200, 50) if collect_comments else 0
        
        if st.sidebar.button("ğŸš€ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘"):
            with st.spinner("Reddit ë°ì´í„° ìˆ˜ì§‘ ì¤‘..."):
                posts_df_new, comments_df_new = search_and_collect_reddit_data(
                    subreddit_names, search_query, post_limit, 
                    sort_by, time_filter, collect_comments, comment_limit
                )
            
            if posts_df_new is not None and not posts_df_new.empty:
                st.success(f"âœ… ê²Œì‹œë¬¼ **{len(posts_df_new):,}**ê°œ ìˆ˜ì§‘ ì™„ë£Œ!")

                # ========================================
                # ğŸ”´ [ìˆ˜ì • ë°˜ì˜] íƒ€ì„ìŠ¤íƒ¬í”„ CSV ì €ì¥ ë¡œì§ ì¶”ê°€
                # ========================================
                
                # í˜„ì¬ ì‹œê°ì„ YYYYMMDD_HHMMSS í˜•ì‹ìœ¼ë¡œ í¬ë§·
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                
                # 1. ê²Œì‹œë¬¼ ë°ì´í„° ì €ì¥ (í•„ìˆ˜)
                posts_file_name = f"posts_data_{timestamp}.csv"
                posts_file_path = os.path.join(SAVE_DIR, posts_file_name)
                posts_df_new.to_csv(posts_file_path, index=False, encoding='utf-8-sig')
                st.success(f"ğŸ’¾ ê²Œì‹œë¬¼ ë°ì´í„°ê°€ **{posts_file_name}**ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                
                # ğŸ”´ ë””ë²„ê¹… ë¼ì¸ ì¶”ê°€
                current_time_str = datetime.now().strftime("%Y%m%d_%H%M%S")
                st.write(f"ë””ë²„ê¹…: ìƒì„±ëœ íƒ€ì„ìŠ¤íƒ¬í”„ ë¬¸ìì—´: {current_time_str}") # Streamlit í™”ë©´ì— ì¶œë ¥
                
                timestamp = current_time_str # ìƒì„±ëœ ë¬¸ìì—´ì„ ì‚¬ìš©
                # ... (ë‚˜ë¨¸ì§€ ì €ì¥ ë¡œì§ ìœ ì§€)

                # 2. ëŒ“ê¸€ ë°ì´í„° ì €ì¥ (ì„ íƒ)
                if comments_df_new is not None and not comments_df_new.empty:
                    st.success(f"âœ… ëŒ“ê¸€ **{len(comments_df_new):,}**ê°œ ìˆ˜ì§‘ ì™„ë£Œ!")
                    comments_file_name = f"comments_data_{timestamp}.csv"
                    comments_file_path = os.path.join(SAVE_DIR, comments_file_name)
                    comments_df_new.to_csv(comments_file_path, index=False, encoding='utf-8-sig')
                    st.success(f"ğŸ’¾ ëŒ“ê¸€ ë°ì´í„°ê°€ **{comments_file_name}**ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                
                # ========================================
                
                st.session_state['posts_df'] = posts_df_new
                st.session_state['comments_df'] = comments_df_new
                st.rerun()
            else:
                st.warning("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    else:  # CSV íŒŒì¼ ì—…ë¡œë“œ
        st.sidebar.subheader("ğŸ“¤ íŒŒì¼ ì—…ë¡œë“œ")
        posts_file = st.sidebar.file_uploader("ê²Œì‹œë¬¼ CSV íŒŒì¼", type=['csv'], key="posts_upload")
        comments_file = st.sidebar.file_uploader("ëŒ“ê¸€ CSV íŒŒì¼ (ì„ íƒ)", type=['csv'], key="comments_upload")
        
        if posts_file:
            posts_df_loaded = pd.read_csv(posts_file)
            st.session_state['posts_df'] = posts_df_loaded
            st.sidebar.success(f"âœ… ê²Œì‹œë¬¼ **{len(posts_df_loaded):,}**ê°œ ë¡œë“œ")
        
        if comments_file:
            comments_df_loaded = pd.read_csv(comments_file)
            st.session_state['comments_df'] = comments_df_loaded
            st.sidebar.success(f"âœ… ëŒ“ê¸€ **{len(comments_df_loaded):,}**ê°œ ë¡œë“œ")
        
        if posts_file or comments_file:
            st.rerun()

    
    posts_df = st.session_state['posts_df']
    comments_df = st.session_state['comments_df']

    if posts_df.empty:
        st.info("ğŸ‘† ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê±°ë‚˜ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        return 

    # ê¸°ë³¸ í†µê³„ (ìƒëµ: ê¸°ì¡´ ì½”ë“œ ìœ ì§€)
    st.header("ğŸ“ˆ ê¸°ë³¸ í†µê³„")
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric("ì´ ê²Œì‹œë¬¼ ìˆ˜", f"{len(posts_df):,}")
    with col2:
        st.metric("í‰ê·  ì ìˆ˜", f"{posts_df['score'].mean():.1f}")
    with col3:
        st.metric("í‰ê·  ëŒ“ê¸€ ìˆ˜", f"{posts_df['num_comments'].mean():.1f}")
    with col4:
        if comments_df is not None:
            st.metric("ì´ ëŒ“ê¸€ ìˆ˜", f"{len(comments_df):,}")

    st.markdown("---")

    # íƒ­ìœ¼ë¡œ ë¶„ì„ ëª¨ë“œ êµ¬ë¶„ (ìƒëµ: ê¸°ì¡´ ì½”ë“œ ìœ ì§€)
    tabs = st.tabs([
    "â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œ",
    "ğŸ“Š í‚¤ì›Œë“œ ë¹ˆë„",
    "ğŸ˜ŠğŸ˜¢ ê°ì„± ë¶„ì„",
    "ğŸ“ˆ ì‹œê°„ íŠ¸ë Œë“œ",
    "ğŸ¯ ì„œë¸Œë ˆë”§ ë¹„êµ",
    "ğŸ“‹ ì›ë³¸ ë°ì´í„°",
    "ğŸ“„ ë³´ê³ ì„œ ìƒì„±",
    "ğŸ’¼ ì„ì›ì§„ ë³´ê³ ì„œ"
    ])

    analyzer = RedditAnalyzer(posts_df, comments_df)
    
    # í…ìŠ¤íŠ¸ ë¶„ì„ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë°ì´í„°í”„ë ˆì„ í™•ì¸
    text_sources_available = ["ê²Œì‹œë¬¼ ì œëª©"]
    if 'selftext' in posts_df.columns and not posts_df['selftext'].isnull().all():
        text_sources_available.append("ê²Œì‹œë¬¼ ë³¸ë¬¸")
    if comments_df is not None and 'body' in comments_df.columns and not comments_df['body'].isnull().all():
        text_sources_available.append("ëŒ“ê¸€")


    # íƒ­ 1~8ì€ ê¸°ì¡´ ë¡œì§ ìœ ì§€
    
    with tabs[0]: # ì›Œë“œí´ë¼ìš°ë“œ
        st.header("â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œ")
        
        if not text_sources_available:
            st.warning("í…ìŠ¤íŠ¸ ë°ì´í„°(ì œëª©, ë³¸ë¬¸, ëŒ“ê¸€)ê°€ ì—†ì–´ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            text_source = st.radio("í…ìŠ¤íŠ¸ ì†ŒìŠ¤", text_sources_available, horizontal=True, key="wordcloud_source")

            if st.button("ğŸ” ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±", key="btn_wordcloud"):
                with st.spinner(f"{text_source} ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì¤‘..."):
                    if text_source == "ê²Œì‹œë¬¼ ì œëª©":
                        fig = analyzer.wordcloud(posts_df['title'])
                    elif text_source == "ê²Œì‹œë¬¼ ë³¸ë¬¸":
                        fig = analyzer.wordcloud(posts_df['selftext'])
                    else: fig = analyzer.wordcloud(comments_df['body'])
                    st.pyplot(fig)
            else: st.info("ğŸ‘† í…ìŠ¤íŠ¸ ì†ŒìŠ¤ë¥¼ ì„ íƒí•˜ê³  ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")

    with tabs[1]: # í‚¤ì›Œë“œ ë¹ˆë„
        st.header("ğŸ“Š í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„")
        
        if not text_sources_available: st.warning("í…ìŠ¤íŠ¸ ë°ì´í„°(ì œëª©, ë³¸ë¬¸, ëŒ“ê¸€)ê°€ ì—†ì–´ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            text_source = st.radio("í…ìŠ¤íŠ¸ ì†ŒìŠ¤", text_sources_available, horizontal=True, key="keyword_source")
            top_n = st.slider("í‘œì‹œí•  í‚¤ì›Œë“œ ê°œìˆ˜", 10, 50, 20, key="keyword_top_n")

            if st.button("ğŸ” í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„", key="btn_keyword"):
                with st.spinner(f"{text_source} í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„ ì¤‘..."):
                    if text_source == "ê²Œì‹œë¬¼ ì œëª©": fig, freq_df = analyzer.keyword_frequency(posts_df['title'], top_n=top_n)
                    elif text_source == "ê²Œì‹œë¬¼ ë³¸ë¬¸": fig, freq_df = analyzer.keyword_frequency(posts_df['selftext'], top_n=top_n)
                    else: fig, freq_df = analyzer.keyword_frequency(comments_df['body'], top_n=top_n)
                    
                    if fig:
                        st.pyplot(fig)
                        st.subheader("ğŸ“‹ í‚¤ì›Œë“œ ë°ì´í„°")
                        st.dataframe(freq_df, use_container_width=True)
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        csv_file_name = f"reddit_keyword_frequency_{timestamp}.csv"
                        freq_df.to_csv(os.path.join(SAVE_DIR, csv_file_name), index=False, encoding='utf-8-sig')
                        st.session_state['freq_df_report'] = freq_df 
                        csv_data = freq_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
                        st.download_button("ğŸ’¾ CSV ë‹¤ìš´ë¡œë“œ", csv_data, csv_file_name, "text/csv", key='download-keyword-csv')
                        st.success(f"ë¶„ì„ ê²°ê³¼ê°€ ì„œë²„ í´ë” `{SAVE_DIR}`ì— **{csv_file_name}**ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    else: st.warning("ë¶„ì„í•  ìœ íš¨í•œ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.info("ğŸ‘† í…ìŠ¤íŠ¸ ì†ŒìŠ¤ë¥¼ ì„ íƒí•˜ê³  ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")
                if 'freq_df_report' in st.session_state:
                    st.subheader("ğŸ“‹ ë§ˆì§€ë§‰ ë¶„ì„ ê²°ê³¼ (í‚¤ì›Œë“œ ë°ì´í„°)")
                    st.dataframe(st.session_state['freq_df_report'], use_container_width=True)


    with tabs[2]: # ê°ì„± ë¶„ì„
        st.header("ğŸ˜ŠğŸ˜¢ ê°ì„± ë¶„ì„")
        
        if not text_sources_available: st.warning("í…ìŠ¤íŠ¸ ë°ì´í„°(ì œëª©, ë³¸ë¬¸, ëŒ“ê¸€)ê°€ ì—†ì–´ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            text_source = st.radio("í…ìŠ¤íŠ¸ ì†ŒìŠ¤", text_sources_available, horizontal=True, key="sentiment_source")

            if st.button("ğŸ” ê°ì„± ë¶„ì„ ì‹¤í–‰", key="btn_sentiment"):
                with st.spinner(f"{text_source} ê°ì„± ë¶„ì„ ì¤‘..."):
                    if text_source == "ê²Œì‹œë¬¼ ì œëª©": fig, sentiment_counts, sentiment_df = analyzer.sentiment_analysis(posts_df['title'], posts_df)
                    elif text_source == "ê²Œì‹œë¬¼ ë³¸ë¬¸": fig, sentiment_counts, sentiment_df = analyzer.sentiment_analysis(posts_df['selftext'], posts_df)
                    else: fig, sentiment_counts, sentiment_df = analyzer.sentiment_analysis(comments_df['body'], comments_df)
                    
                    if fig:
                        st.pyplot(fig)
                        st.subheader("ğŸ“Š ê°ì„± ìš”ì•½")
                        col1_s, col2_s, col3_s = st.columns(3)
                        for idx, sentiment in enumerate(['ê¸ì •', 'ë¶€ì •', 'ì¤‘ë¦½']):
                             count = sentiment_counts.get(sentiment, 0)
                             with [col1_s, col2_s, col3_s][idx]: st.metric(sentiment, f"{count:,}ê°œ")
                                 
                        st.subheader("ğŸ“‹ ê°ì„± ë¶„ë¥˜ ë°ì´í„° (ìƒìœ„ 100ê°œ)")
                        st.dataframe(sentiment_df.head(100), use_container_width=True)
                        
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        csv_file_name = f"reddit_sentiment_analysis_{timestamp}.csv"
                        sentiment_df.to_csv(os.path.join(SAVE_DIR, csv_file_name), index=False, encoding='utf-8-sig')
                        st.session_state['sentiment_df_report'] = sentiment_df 
                        csv_data = sentiment_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
                        st.download_button("ğŸ’¾ CSV ë‹¤ìš´ë¡œë“œ", csv_data, csv_file_name, "text/csv", key='download-sentiment-csv')
                        st.success(f"ë¶„ì„ ê²°ê³¼ê°€ ì„œë²„ í´ë” `{SAVE_DIR}`ì— **{csv_file_name}**ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    else: st.warning("ê°ì„± ë¶„ì„ì„ ìˆ˜í–‰í•  í…ìŠ¤íŠ¸ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.info("ğŸ‘† í…ìŠ¤íŠ¸ ì†ŒìŠ¤ë¥¼ ì„ íƒí•˜ê³  ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")
                if 'sentiment_df_report' in st.session_state:
                    st.subheader("ğŸ“‹ ë§ˆì§€ë§‰ ë¶„ì„ ê²°ê³¼ (ê°ì„± ë¶„ë¥˜ ë°ì´í„° - ìƒìœ„ 100ê°œ)")
                    st.dataframe(st.session_state['sentiment_df_report'].head(100), use_container_width=True)


    with tabs[3]: # ì‹œê°„ íŠ¸ë Œë“œ
        st.header("ğŸ“ˆ ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„")
        data_source_trend_options = ["ê²Œì‹œë¬¼"]
        if comments_df is not None: data_source_trend_options.append("ëŒ“ê¸€")
        data_source_trend = st.radio("ë°ì´í„° ì†ŒìŠ¤", data_source_trend_options, horizontal=True)
        interval = st.radio("ì‹œê°„ ê°„ê²©", ["D (ì¼)", "W (ì£¼)", "M (ì›”)"], horizontal=True, key="time_interval")
        interval_code = interval.split()[0]

        if st.button("ğŸ” ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„", key="btn_time"):
            with st.spinner(f"{data_source_trend} ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„ ì¤‘..."):
                if data_source_trend == "ê²Œì‹œë¬¼": fig, trend_df = analyzer.time_trend(posts_df, interval=interval_code)
                else: fig, trend_df = analyzer.time_trend(comments_df, interval=interval_code)
                
                if fig:
                    st.pyplot(fig)
                    st.subheader("ğŸ“‹ íŠ¸ë Œë“œ ë°ì´í„°")
                    st.dataframe(trend_df, use_container_width=True)
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    csv_file_name = f"reddit_time_trend_{timestamp}.csv"
                    trend_df.to_csv(os.path.join(SAVE_DIR, csv_file_name), index=False, encoding='utf-8-sig')
                    st.session_state['trend_df_report'] = trend_df 
                    csv_data = trend_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
                    st.download_button("ğŸ’¾ CSV ë‹¤ìš´ë¡œë“œ", csv_data, csv_file_name, "text/csv", key='download-trend-csv')
                    st.success(f"ë¶„ì„ ê²°ê³¼ê°€ ì„œë²„ í´ë” `{SAVE_DIR}`ì— **{csv_file_name}**ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                else: st.warning("ë‚ ì§œ ì •ë³´ê°€ ì—†ê±°ë‚˜ ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ì–´ ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.info("ğŸ‘† ë°ì´í„° ì†ŒìŠ¤ì™€ ì‹œê°„ ê°„ê²©ì„ ì„ íƒí•˜ê³  ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")
            if 'trend_df_report' in st.session_state:
                st.subheader("ğŸ“‹ ë§ˆì§€ë§‰ ë¶„ì„ ê²°ê³¼ (íŠ¸ë Œë“œ ë°ì´í„°)")
                st.dataframe(st.session_state['trend_df_report'], use_container_width=True)


    with tabs[4]: # ì„œë¸Œë ˆë”§ ë¹„êµ
        st.header("ğŸ¯ ì„œë¸Œë ˆë”§ ë¹„êµ ë¶„ì„")

        if st.button("ğŸ” ì„œë¸Œë ˆë”§ ë¹„êµ ë¶„ì„", key="btn_subreddit"):
            with st.spinner("ì„œë¸Œë ˆë”§ ë¹„êµ ë¶„ì„ ì¤‘..."):
                fig, comparison_df = analyzer.subreddit_comparison()
                if fig:
                    st.pyplot(fig)
                    st.subheader("ğŸ“‹ ì„œë¸Œë ˆë”§ í†µê³„")
                    st.dataframe(comparison_df, use_container_width=True)
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    csv_file_name = f"reddit_subreddit_comparison_{timestamp}.csv"
                    comparison_df.to_csv(os.path.join(SAVE_DIR, csv_file_name), encoding='utf-8-sig') 
                    st.session_state['comparison_df_report'] = comparison_df 
                    csv_data = comparison_df.to_csv(encoding='utf-8-sig').encode('utf-8-sig')
                    st.download_button("ğŸ’¾ CSV ë‹¤ìš´ë¡œë“œ", csv_data, csv_file_name, "text/csv", key='download-subreddit-csv')
                    st.success(f"ë¶„ì„ ê²°ê³¼ê°€ ì„œë²„ í´ë” `{SAVE_DIR}`ì— **{csv_file_name}**ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                else: st.warning("ì„œë¸Œë ˆë”§ ì •ë³´ê°€ ì—†ê±°ë‚˜ ë¹„êµí•  ì„œë¸Œë ˆë”§ì´ 2ê°œ ë¯¸ë§Œì…ë‹ˆë‹¤.")
        else:
            st.info("ğŸ‘† ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ì„œë¸Œë ˆë”§ë³„ í†µê³„ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            if 'comparison_df_report' in st.session_state:
                st.subheader("ğŸ“‹ ë§ˆì§€ë§‰ ë¶„ì„ ê²°ê³¼ (ì„œë¸Œë ˆë”§ í†µê³„)")
                st.dataframe(st.session_state['comparison_df_report'], use_container_width=True)


    with tabs[5]: # ì›ë³¸ ë°ì´í„°
        st.header("ğŸ“‹ ì›ë³¸ ë°ì´í„°")
        data_type_options = ["ê²Œì‹œë¬¼ ë°ì´í„°"]
        if comments_df is not None: data_type_options.append("ëŒ“ê¸€ ë°ì´í„°")
        data_type = st.radio("ë°ì´í„° ìœ í˜• ì„ íƒ", data_type_options, horizontal=True)

        if data_type == "ê²Œì‹œë¬¼ ë°ì´í„°":
            st.subheader("ğŸ“ ê²Œì‹œë¬¼ ë°ì´í„°")
            display_columns = st.multiselect("í‘œì‹œí•  ì»¬ëŸ¼ ì„ íƒ", posts_df.columns.tolist(),
                default=['title', 'subreddit', 'score', 'num_comments', 'author'][:min(5, len(posts_df.columns))], key='posts_cols_select')
            
            if display_columns: st.dataframe(posts_df[display_columns], use_container_width=True, height=600)
            else: st.dataframe(posts_df, use_container_width=True, height=600)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            csv_file_name = f"reddit_posts_data_{timestamp}.csv"
            posts_df.to_csv(os.path.join(SAVE_DIR, csv_file_name), index=False, encoding='utf-8-sig')
            st.success(f"ì›ë³¸ ë°ì´í„°ê°€ ì„œë²„ í´ë” `{SAVE_DIR}`ì— **{csv_file_name}**ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            csv_data = posts_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
            st.download_button("ğŸ’¾ ê²Œì‹œë¬¼ ë°ì´í„° CSV ë‹¤ìš´ë¡œë“œ", csv_data, csv_file_name, "text/csv", key='download-posts-raw')
        else:
            if comments_df is not None and not comments_df.empty:
                st.subheader("ğŸ’¬ ëŒ“ê¸€ ë°ì´í„°")
                display_columns = st.multiselect("í‘œì‹œí•  ì»¬ëŸ¼ ì„ íƒ", comments_df.columns.tolist(),
                    default=['body', 'subreddit', 'score', 'author', 'post_title'][:min(5, len(comments_df.columns))], key='comments_cols_select')
                
                if display_columns: st.dataframe(comments_df[display_columns], use_container_width=True, height=600)
                else: st.dataframe(comments_df, use_container_width=True, height=600)
                
                csv_file_name = "reddit_comments_data.csv"
                comments_df.to_csv(os.path.join(SAVE_DIR, csv_file_name), index=False, encoding='utf-8-sig')
                st.success(f"ì›ë³¸ ë°ì´í„°ê°€ ì„œë²„ í´ë” `{SAVE_DIR}`ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                csv_data = comments_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
                st.download_button("ğŸ’¾ ëŒ“ê¸€ ë°ì´í„° CSV ë‹¤ìš´ë¡œë“œ", csv_data, csv_file_name, "text/csv", key='download-comments-raw')
            else: st.warning("ëŒ“ê¸€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")


    with tabs[6]: # AI ìë™ ë³´ê³ ì„œ ìƒì„± (ì¼ë°˜)
        st.header("ğŸ“„ Market Insight Report Generator (OpenAI API ê¸°ë°˜)")
        st.write("ë¶„ì„ CSV íŒŒì¼ì˜ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ **ì‚¬ìš©ì ì§€ì • í”„ë¡¬í”„íŠ¸**ì— ë§ì¶˜ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        st.warning("âš ï¸ **OpenAI API Key**ê°€ `.env` íŒŒì¼ì— ì„¤ì •ë˜ì–´ ìˆì–´ì•¼ í•˜ë©°, ë³´ê³ ì„œì— í¬í•¨í•  CSV íŒŒì¼ì€ ë¨¼ì € ë¶„ì„ íƒ­ì—ì„œ ì‹¤í–‰ í›„ **`analysis_results` í´ë”ì— ì €ì¥**í•´ì•¼ í•©ë‹ˆë‹¤.")
        
        user_focus_prompt = st.text_area(
            "âœï¸ ë³´ê³ ì„œì˜ í•µì‹¬ ë¶„ì„ ì£¼ì œ ë° ì§ˆë¬¸ (Focus Prompt)",
            value="Analyze the overall sentiment and identify the key positive drivers across the selected datasets. What strategic recommendations can be derived from the trend data and subreddit comparison?",
            height=150, key="report_general_prompt"
        )
        
        try:
            available_files = [f for f in os.listdir(SAVE_DIR) if f.endswith(".csv")]
        except FileNotFoundError:
            available_files = []

        if not available_files:
            st.error("ë¶„ì„ ê²°ê³¼ CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë‹¤ë¥¸ ë¶„ì„ íƒ­ì—ì„œ ë¶„ì„ì„ ì‹¤í–‰í•˜ê³  CSVë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì—¬ `analysis_results` í´ë”ì— ì €ì¥í•˜ì„¸ìš”.")
        else:
            selected_files = st.multiselect("ğŸ“‚ ë³´ê³ ì„œì— í¬í•¨í•  íŒŒì¼ ì„ íƒ", available_files, default=available_files, key="report_general_files")

            if st.button("ğŸ§  ë³´ê³ ì„œ ìƒì„± ì‹¤í–‰", key="btn_generate_report"):
                report_sentences = [] 
                
                if not selected_files or not user_focus_prompt.strip() or not OPENAI_API_KEY:
                    st.error("ì…ë ¥ê°’ì„ í™•ì¸í•˜ì„¸ìš”.")
                    return
                
                temp_analyzer = RedditAnalyzer(posts_df, comments_df)

                with st.spinner("OpenAI GPT ëª¨ë¸ì´ ë³´ê³ ì„œë¥¼ ìƒì„± ì¤‘..."):
                    
                    for f in selected_files:
                        file_path = os.path.join(SAVE_DIR, f)
                        keywords = f"File: {f}. "
                        try:
                            df = pd.read_csv(file_path, encoding="utf-8-sig")

                            if 'í‚¤ì›Œë“œ' in df.columns and 'ë¹ˆë„' in df.columns: keywords += f"Top keyword is '{df.iloc[0]['í‚¤ì›Œë“œ']}' with count {df.iloc[0]['ë¹ˆë„']}. Total unique keywords: {len(df)}. "
                            elif 'Sentiment' in df.columns:
                                sentiment_counts = df['Sentiment'].value_counts()
                                pos = sentiment_counts.get('ê¸ì •', 0)
                                neg = sentiment_counts.get('ë¶€ì •', 0)
                                total = len(df)
                                pos_ratio = pos / total * 100 if total > 0 else 0
                                keywords += f"Total comments/posts {total}. Positive ratio: {pos_ratio:.1f}%. Negative comments: {neg}. The overall sentiment is mostly Positive. "
                            elif 'ê°œìˆ˜' in df.columns and 'ë‚ ì§œ' in df.columns:
                                df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])
                                max_count_date = df.loc[df['ê°œìˆ˜'].idxmax(), 'ë‚ ì§œ'].strftime('%Y-%m-%d')
                                max_count = df['ê°œìˆ˜'].max()
                                keywords += f"Peak count {max_count} occurred on {max_count_date}. Average count per period is {df['ê°œìˆ˜'].mean():.1f}. "
                            elif 'ì´_ì ìˆ˜' in df.columns and 'ì„œë¸Œë ˆë”§' in df.columns:
                                top_subreddit = df.loc[df['ì´_ì ìˆ˜'].idxmax(), 'ì„œë¸Œë ˆë”§']
                                top_score = df['ì´_ì ìˆ˜'].max()
                                avg_comments = df['í‰ê· _ëŒ“ê¸€ìˆ˜'].mean()
                                keywords += f"Top subreddit by total score is '{top_subreddit}' with score {top_score}. Average comments per post across all subreddits: {avg_comments:.1f}. "
                            elif ('title' in df.columns and 'score' in df.columns) or ('body' in df.columns and 'score' in df.columns):
                                total_records = len(df)
                                avg_score = df['score'].mean()
                                keywords += f"Raw data summary. Total records: {total_records}. Average score: {avg_score:.1f}. "
                            else: keywords += f"Dataset rows: {len(df)}. Columns: {', '.join(df.columns)}. Data statistics available. "
                            
                            sentence = generate_openai_report(keywords, user_focus_prompt, OPENAI_API_KEY) 
                            report_sentences.append(f"**{f} Insight:** {sentence}")
                        except Exception as e:
                            st.error(f"íŒŒì¼ {f} ì²˜ë¦¬ ì˜¤ë¥˜: CSV íŒŒì¼ êµ¬ì¡° í™•ì¸ í•„ìš”. {str(e)}")
                            continue

                if report_sentences:
                    summary = "\n\n".join(report_sentences)
                    final_report = f"""
# Reddit Analysis Auto-Generated Report
## Generated At: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
## User Focus Prompt: {user_focus_prompt}

{summary}
"""
                    st.subheader("ğŸ“ˆ AI ìë™ ìƒì„± ë³´ê³ ì„œ ì´ˆì•ˆ")
                    st.text_area("ìš”ì•½ ê²°ê³¼", final_report, height=400)
                    st.download_button("ğŸ’¾ ìš”ì•½ ë³´ê³ ì„œ ì €ì¥", final_report.encode("utf-8-sig"), "Market_Insight_Report_Reddit.txt", "text/plain")
                else: st.error("ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨. íŒŒì¼ ì„ íƒ ë° êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")


    with tabs[7]: # ì„ì›ì§„ ë³´ê³ ì„œ
        st.header("ğŸ’¼ ì„ì›ì§„ ë³´ê³ ì„œ (Executive Summary)")
        st.write("í•µì‹¬ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ **êµ­ë¬¸ ë° ì˜ë¬¸**ìœ¼ë¡œ ë¶„ë¦¬ëœ, ì„ì›ì§„ ì œì¶œìš©ìœ¼ë¡œ ì í•©í•œ ìš”ì•½ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        st.warning("âš ï¸ ì´ ë³´ê³ ì„œ ìƒì„±ì„ ìœ„í•´ì„œëŠ” **OpenAI API Key**ê°€ í•„ìˆ˜ì´ë©°, ë‹¤ë¥¸ íƒ­ì—ì„œ CSV íŒŒì¼ì„ **`analysis_results`** í´ë”ì— ì €ì¥í•´ì•¼ í•©ë‹ˆë‹¤.")
        
        user_exec_prompt = st.text_area(
            "âœï¸ ì„ì›ì§„ ë³´ê³ ì„œì˜ í•µì‹¬ ë¶„ì„ ì£¼ì œ ë° ì§ˆë¬¸",
            value="Identify the 3 most critical market insights and propose concise strategic actions for brand positioning based on the competitive analysis.",
            height=100, key="report_exec_prompt"
        )
        
        try:
            available_files = [f for f in os.listdir(SAVE_DIR) if f.endswith(".csv")]
        except FileNotFoundError:
            available_files = []

        if not available_files:
            st.error("ë¶„ì„ ê²°ê³¼ CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë¶„ì„ íƒ­ì—ì„œ íŒŒì¼ì„ ì €ì¥í•˜ì„¸ìš”.")
        else:
            selected_exec_files = st.multiselect("ğŸ“‚ ë³´ê³ ì„œì— í¬í•¨í•  íŒŒì¼ ì„ íƒ", available_files, default=available_files, key="report_exec_files")

            if st.button("ğŸ§  êµ­/ì˜ë¬¸ ì„ì›ì§„ ë³´ê³ ì„œ ìƒì„±", key="btn_generate_exec_report"):
                
                if not selected_exec_files or not user_exec_prompt.strip() or not OPENAI_API_KEY:
                    st.error("ì…ë ¥ê°’ì„ í™•ì¸í•˜ì„¸ìš”.")
                    return

                temp_analyzer = RedditAnalyzer(posts_df, comments_df)
                full_keywords_for_exec = ""
                
                with st.spinner("OpenAI GPT ëª¨ë¸ì´ êµ­/ì˜ë¬¸ ë³´ê³ ì„œë¥¼ ìƒì„± ì¤‘..."):
                    
                    # 1. ëª¨ë“  ì„ íƒëœ íŒŒì¼ì˜ ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ í‚¤ì›Œë“œ ë¬¸ìì—´ë¡œ ì¡°í•© (ë°ì´í„° ì¶”ì¶œ ë¡œì§ ì¬ì‚¬ìš©)
                    for f in selected_exec_files:
                        file_path = os.path.join(SAVE_DIR, f)
                        keywords_chunk = f"File: {f}. "
                        try:
                            df = pd.read_csv(file_path, encoding="utf-8-sig")

                            if 'í‚¤ì›Œë“œ' in df.columns and 'ë¹ˆë„' in df.columns: keywords_chunk += f"Top keyword is '{df.iloc[0]['í‚¤ì›Œë“œ']}' with count {df.iloc[0]['ë¹ˆë„']}. Total unique keywords: {len(df)}. "
                            elif 'Sentiment' in df.columns:
                                sentiment_counts = df['Sentiment'].value_counts()
                                pos_ratio = sentiment_counts.get('ê¸ì •', 0) / len(df) * 100 if len(df) > 0 else 0
                                keywords_chunk += f"Total comments/posts {len(df)}. Positive ratio: {pos_ratio:.1f}%. "
                            elif 'ê°œìˆ˜' in df.columns and 'ë‚ ì§œ' in df.columns:
                                df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])
                                max_count = df['ê°œìˆ˜'].max()
                                keywords_chunk += f"Peak count {max_count}. Average count per period is {df['ê°œìˆ˜'].mean():.1f}. "
                            elif 'ì´_ì ìˆ˜' in df.columns and 'ì„œë¸Œë ˆë”§' in df.columns:
                                top_subreddit = df.loc[df['ì´_ì ìˆ˜'].idxmax(), 'ì„œë¸Œë ˆë”§']
                                top_score = df['ì´_ì ìˆ˜'].max()
                                keywords_chunk += f"Top subreddit is '{top_subreddit}' with score {top_score}. "
                            elif ('title' in df.columns and 'score' in df.columns) or ('body' in df.columns and 'score' in df.columns):
                                avg_score = df['score'].mean()
                                keywords_chunk += f"Raw data summary. Total records: {len(df)}. Average score: {avg_score:.1f}. "
                            else: keywords_chunk += f"Dataset rows: {len(df)}. Columns: {', '.join(df.columns)}. "
                            
                            full_keywords_for_exec += keywords_chunk
                        except Exception as e:
                            st.error(f"íŒŒì¼ {f} ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
                            continue
                            
                    # 2. í†µí•©ëœ í‚¤ì›Œë“œì™€ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì„ì›ì§„ ë³´ê³ ì„œ ìƒì„±
                    english_summary, korean_summary = generate_executive_report(full_keywords_for_exec, user_exec_prompt, OPENAI_API_KEY)
                    
                    # 3. ê²°ê³¼ ì¶œë ¥ ë° ë‹¤ìš´ë¡œë“œ
                    
                    # êµ­ë¬¸ ë³´ê³ ì„œ êµ¬ì„±
                    korean_final_report = f"""
# ğŸ”´ Reddit ì„ì›ì§„ ë³´ê³ ì„œ (êµ­ë¬¸)
## ì‘ì„±ì¼: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
## ë¶„ì„ ì£¼ì œ: {user_exec_prompt}
---
### í•µì‹¬ ìš”ì•½ (Korean Summary)
{korean_summary}

---
### ë¶„ì„ì— ì‚¬ìš©ëœ ë°ì´í„° íŒŒì¼
{', '.join(selected_exec_files)}
"""
                    # ì˜ë¬¸ ë³´ê³ ì„œ êµ¬ì„±
                    english_final_report = f"""
# ğŸ”´ Reddit Executive Summary (English)
## Generated At: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
## Focus Prompt: {user_exec_prompt}
---
### Executive Summary
{english_summary}

---
### Data Files Used
{', '.join(selected_exec_files)}
"""
                    
                    st.markdown("---")
                    st.subheader("ğŸ‡°ğŸ‡· êµ­ë¬¸ ë³´ê³ ì„œ ì´ˆì•ˆ")
                    st.text_area("êµ­ë¬¸ ìš”ì•½ ê²°ê³¼", korean_final_report, height=300)
                    
                    st.subheader("ğŸ‡ºğŸ‡¸ ì˜ë¬¸ ë³´ê³ ì„œ ì´ˆì•ˆ")
                    st.text_area("ì˜ë¬¸ ìš”ì•½ ê²°ê³¼", english_final_report, height=300)
                    
                    col_kor, col_eng = st.columns(2)
                    
                    with col_kor:
                        st.download_button(
                            "ğŸ’¾ êµ­ë¬¸ ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ (KR.txt)",
                            korean_final_report.encode("utf-8-sig"),
                            "Executive_Report_KR.txt",
                            "text/plain"
                        )
                    with col_eng:
                        st.download_button(
                            "ğŸ’¾ ì˜ë¬¸ ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ (EN.txt)",
                            english_final_report.encode("utf-8-sig"),
                            "Executive_Report_EN.txt",
                            "text/plain"
                        )
                    
                if "Parsing Error" in english_summary:
                    st.error("ë³´ê³ ì„œ íŒŒì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. OpenAI ì¶œë ¥ í˜•ì‹ì´ ì œëŒ€ë¡œ ì§€ì¼œì§€ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¼ë°˜ ë³´ê³ ì„œ íƒ­ì—ì„œ API ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”.")


if __name__ == "__main__":
    main()