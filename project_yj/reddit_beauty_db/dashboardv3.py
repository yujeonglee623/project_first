import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from datetime import datetime
import re
from wordcloud import WordCloud
import praw
from dotenv import load_dotenv
import os
from prawcore.exceptions import ResponseException, RequestException

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="Reddit ë¶„ì„ ëŒ€ì‹œë³´ë“œ",
    page_icon="ğŸ”´",
    layout="wide"
)

# í•œê¸€ í°íŠ¸ ì„¤ì •
# ì£¼ì˜: ì´ íŒŒì¼(malgun.ttf)ì´ ì‹¤í–‰ í™˜ê²½ì— ìˆì–´ì•¼ ì›Œë“œí´ë¼ìš°ë“œê°€ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.
plt.rcParams['font.family'] = 'Malgun Gothic'  # Windows
# plt.rcParams['font.family'] = 'AppleGothic'  # Mac
plt.rcParams['axes.unicode_minus'] = False


class RedditAnalyzer:
    """Reddit ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self, posts_df, comments_df=None):
        self.posts_df = posts_df.copy()
        # ë°ì´í„°í”„ë ˆì„ì´ ë¹„ì–´ìˆëŠ” ê²½ìš°ë¥¼ ë°©ì§€
        self.comments_df = comments_df.copy() if comments_df is not None and not comments_df.empty else None
        
        # ë‚ ì§œ ì»¬ëŸ¼ ë³€í™˜
        if 'created_utc' in self.posts_df.columns:
            self.posts_df['created_utc'] = pd.to_datetime(self.posts_df['created_utc'], unit='s', errors='coerce')
        if self.comments_df is not None and 'created_utc' in self.comments_df.columns:
            self.comments_df['created_utc'] = pd.to_datetime(self.comments_df['created_utc'], unit='s', errors='coerce')
    
    
    def preprocess_text(self, text):
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        if pd.isna(text) or text is None:
            return ""
        
        text = str(text).lower()
        text = re.sub(r'http\S+|www\S+', '', text)
        text = re.sub(r'[^ê°€-í£a-z0-9\s]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    
    def extract_keywords(self, text_series, min_length=2, top_n=50):
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # NaN/None ê°’ì„ ë¹ˆ ë¬¸ìì—´ë¡œ ëŒ€ì²´í•˜ì—¬ join ì‹œ ì˜¤ë¥˜ ë°©ì§€
        all_text = ' '.join(text_series.fillna('').apply(self.preprocess_text))
        words = all_text.split()
        words = [w for w in words if len(w) >= min_length]
        
        stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
                     'of', 'is', 'are', 'was', 'were', 'been', 'be', 'have', 'has', 'had',
                     'i', 'me', 'my', 'you', 'your', 'it', 'its', 'not', 'no', 'yes', 'we',
                     'ê·¸', 'ì´', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë“¤', 'ë°', 'ë˜í•œ', 'í•˜ë‹¤', 'ìˆë‹¤', 'ë˜ë‹¤',
                     'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ê·¸ëŸ°', 'ì´ëŸ°', 'ì €ëŸ°', 'removed', 'deleted'}
        
        words = [w for w in words if w not in stopwords]
        word_freq = Counter(words)
        
        return word_freq.most_common(top_n)
    
    
    def wordcloud(self, text_series, width=1200, height=800):
        """ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±"""
        # NaN/None ê°’ì„ ë¹ˆ ë¬¸ìì—´ë¡œ ëŒ€ì²´
        all_text = ' '.join(text_series.fillna('').apply(self.preprocess_text))
        
        # 'malgun.ttf' ê²½ë¡œê°€ í™˜ê²½ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì—ëŸ¬ ë°©ì§€ë¥¼ ìœ„í•´ font_path ìˆ˜ì • (í˜¹ì€ ì£¼ì„ ì²˜ë¦¬)
        try:
             font_path = 'C:/Windows/Fonts/malgun.ttf' # Windows ê¸°ë³¸ ê²½ë¡œ
        except:
             font_path = None

        wordcloud = WordCloud(
            font_path=font_path,
            width=width,
            height=height,
            background_color='white',
            max_words=100,
            relative_scaling=0.3,
            colormap='viridis'
        ).generate(all_text)
        
        fig, ax = plt.subplots(figsize=(15, 10))
        ax.imshow(wordcloud, interpolation='bilinear')
        ax.axis('off')
        ax.set_title('Reddit í…ìŠ¤íŠ¸ ì›Œë“œí´ë¼ìš°ë“œ', fontsize=20, pad=20)
        plt.tight_layout()
        
        return fig
    
    
    def keyword_frequency(self, text_series, top_n=20):
        """í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„"""
        keywords = self.extract_keywords(text_series, top_n=top_n)
        
        if not keywords:
            return None, pd.DataFrame(columns=['í‚¤ì›Œë“œ', 'ë¹ˆë„'])

        words, counts = zip(*keywords)
        
        fig, ax = plt.subplots(figsize=(12, 8))
        ax.barh(range(len(words)), counts, color='orangered')
        ax.set_yticks(range(len(words)))
        ax.set_yticklabels(words)
        ax.set_xlabel('ë¹ˆë„', fontsize=12)
        ax.set_title(f'ìƒìœ„ {top_n}ê°œ í‚¤ì›Œë“œ ë¹ˆë„', fontsize=16, pad=20)
        ax.invert_yaxis()
        plt.tight_layout()
        
        freq_df = pd.DataFrame(keywords, columns=['í‚¤ì›Œë“œ', 'ë¹ˆë„'])
        
        return fig, freq_df
    
    
    def sentiment_analysis(self, text_series):
        """ê°ì„± ë¶„ì„ (ë‹¨ìˆœ í‚¤ì›Œë“œ ë§¤ì¹­ ê¸°ë°˜)"""
        positive_words = {
            'good', 'great', 'excellent', 'amazing', 'wonderful', 'fantastic',
            'best', 'love', 'awesome', 'perfect', 'nice', 'happy', 'thank',
            'ì¢‹ë‹¤', 'ìµœê³ ', 'ëŒ€ë°•', 'ì˜ˆì˜ë‹¤', 'ë©‹ì§€ë‹¤', 'ì™„ë²½', 'ê°ì‚¬', 'í–‰ë³µ'
        }
        
        negative_words = {
            'bad', 'worst', 'terrible', 'awful', 'horrible', 'hate',
            'poor', 'disappointing', 'useless', 'waste', 'crap',
            'ì‹«ë‹¤', 'ë³„ë¡œ', 'ë‚˜ì˜ë‹¤', 'ìµœì•…', 'í˜•í¸ì—†ë‹¤', 'ì‹¤ë§'
        }
        
        def calculate_sentiment(text):
            text = self.preprocess_text(text)
            words = text.split()
            
            pos_count = sum(1 for w in words if w in positive_words)
            neg_count = sum(1 for w in words if w in negative_words)
            
            if pos_count > neg_count:
                return 'ê¸ì •'
            elif pos_count < neg_count:
                return 'ë¶€ì •'
            else:
                return 'ì¤‘ë¦½'
        
        sentiments = text_series.fillna('').apply(calculate_sentiment)
        sentiment_counts = sentiments.value_counts()
        
        if sentiment_counts.empty:
            return None, pd.Series()
            
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        
        colors = ['#90EE90', '#FFB6C1', '#D3D3D3']
        
        # ë°ì´í„° ì¸ë±ìŠ¤ ìˆœì„œë¥¼ ê¸ì •, ë¶€ì •, ì¤‘ë¦½ìœ¼ë¡œ í†µì¼ (ì—†ìœ¼ë©´ ì œì™¸)
        order = ['ê¸ì •', 'ë¶€ì •', 'ì¤‘ë¦½']
        ordered_counts = sentiment_counts.reindex(order, fill_value=0)
        ordered_counts = ordered_counts[ordered_counts > 0] # 0ê°œì¸ í•­ëª© ì œì™¸
        
        if ordered_counts.empty:
            return None, pd.Series()

        # ìƒ‰ìƒë„ ìˆœì„œì— ë§ê²Œ í•„í„°ë§
        ordered_colors = [c for s, c in zip(order, colors) if s in ordered_counts.index]
        
        axes[0].pie(ordered_counts.values, labels=ordered_counts.index, 
                    autopct='%1.1f%%', colors=ordered_colors, startangle=90)
        axes[0].set_title('ê°ì„± ë¶„í¬', fontsize=14, pad=20)
        
        axes[1].bar(ordered_counts.index, ordered_counts.values, color=ordered_colors)
        axes[1].set_xlabel('ê°ì„±', fontsize=12)
        axes[1].set_ylabel('ê°œìˆ˜', fontsize=12)
        axes[1].set_title('ê°ì„±ë³„ ê°œìˆ˜', fontsize=14, pad=20)
        
        plt.tight_layout()
        
        return fig, sentiment_counts
    
    
    def time_trend(self, df, date_col='created_utc', interval='D'):
        """ì‹œê°„ëŒ€ë³„ íŠ¸ë Œë“œ ë¶„ì„"""
        if date_col not in df.columns:
            return None, None
        
        # ë‚ ì§œ ì»¬ëŸ¼ì˜ ìœ íš¨ì„± í™•ì¸ ë° NaT ì œê±°
        df_valid = df.dropna(subset=[date_col])
        if df_valid.empty:
            return None, None
        
        # set_index ì „ì— ì¸ë±ìŠ¤ë¥¼ ë¦¬ì…‹í•˜ì—¬ 'SettingWithCopyWarning' ë°©ì§€ ë° ì¼ê´€ì„± í™•ë³´
        df_sorted = df_valid.reset_index(drop=True).set_index(date_col).sort_index()
        
        time_counts = df_sorted.resample(interval).size()
        
        # ëª¨ë“  êµ¬ê°„ì´ 0ê°œì¸ ê²½ìš° (ë°ì´í„°ê°€ ë„ˆë¬´ ì ê±°ë‚˜ í•œ ë‚ ì§œì— ì§‘ì¤‘ëœ ê²½ìš°)
        if time_counts.sum() == 0 and not df_sorted.empty:
             time_counts = pd.Series([len(df_sorted)], index=[df_sorted.index.min()], name='counts')
             if interval == 'D':
                interval_label = 'ì¼ë³„'
             elif interval == 'W':
                interval_label = 'ì£¼ë³„'
             else:
                interval_label = 'ì›”ë³„'
             st.info(f"ë°ì´í„°ê°€ {interval_label}ë¡œ ë¶„í• í•˜ê¸°ì— ì ê³ , í•˜ë‚˜ì˜ ì‹œì ì— ì§‘ì¤‘ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë‹¨ì¼ ì‹œì ì˜ ì´ê³„ë§Œ í‘œì‹œí•©ë‹ˆë‹¤.")
             # ì´ ê²½ìš° ê·¸ë˜í”„ë¥¼ ê·¸ë¦´ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ None ë°˜í™˜
             return None, pd.DataFrame({'ë‚ ì§œ': time_counts.index, 'ê°œìˆ˜': time_counts.values})


        if 'score' in df.columns:
            time_scores = df_sorted['score'].resample(interval).sum()
        else:
            time_scores = None
        
        if time_scores is not None:
            fig, axes = plt.subplots(2, 1, figsize=(14, 10))
            
            axes[0].plot(time_counts.index, time_counts.values, marker='o', linewidth=2, color='orangered')
            axes[0].set_xlabel('ë‚ ì§œ', fontsize=12)
            axes[0].set_ylabel('ê²Œì‹œë¬¼/ëŒ“ê¸€ ìˆ˜', fontsize=12)
            axes[0].set_title('ì‹œê°„ëŒ€ë³„ ê²Œì‹œë¬¼/ëŒ“ê¸€ ìˆ˜ ì¶”ì´', fontsize=14, pad=20)
            axes[0].grid(True, alpha=0.3)
            
            axes[1].plot(time_scores.index, time_scores.values, marker='o', 
                         color='coral', linewidth=2)
            axes[1].set_xlabel('ë‚ ì§œ', fontsize=12)
            axes[1].set_ylabel('ì ìˆ˜ í•©ê³„', fontsize=12)
            axes[1].set_title('ì‹œê°„ëŒ€ë³„ ì ìˆ˜ ì¶”ì´', fontsize=14, pad=20)
            axes[1].grid(True, alpha=0.3)
            
            trend_df = pd.DataFrame({
                'ë‚ ì§œ': time_counts.index,
                'ê°œìˆ˜': time_counts.values,
                'ì ìˆ˜': time_scores.values
            })
        else:
            fig, ax = plt.subplots(figsize=(14, 6))
            
            ax.plot(time_counts.index, time_counts.values, marker='o', linewidth=2, color='orangered')
            ax.set_xlabel('ë‚ ì§œ', fontsize=12)
            ax.set_ylabel('ê²Œì‹œë¬¼/ëŒ“ê¸€ ìˆ˜', fontsize=12)
            ax.set_title('ì‹œê°„ëŒ€ë³„ ê²Œì‹œë¬¼/ëŒ“ê¸€ ìˆ˜ ì¶”ì´', fontsize=14, pad=20)
            ax.grid(True, alpha=0.3)
            
            trend_df = pd.DataFrame({
                'ë‚ ì§œ': time_counts.index,
                'ê°œìˆ˜': time_counts.values
            })
        
        plt.tight_layout()
        
        return fig, trend_df
    
    
    def subreddit_comparison(self):
        """ì„œë¸Œë ˆë”§ë³„ ë¹„êµ ë¶„ì„"""
        if 'subreddit' not in self.posts_df.columns or self.posts_df['subreddit'].nunique() < 2:
            return None, None
        
        subreddit_stats = self.posts_df.groupby('subreddit').agg({
            'score': ['mean', 'sum', 'count'],
            'num_comments': 'mean'
        }).round(2)
        
        subreddit_stats.columns = ['í‰ê· _ì ìˆ˜', 'ì´_ì ìˆ˜', 'ê²Œì‹œë¬¼_ìˆ˜', 'í‰ê· _ëŒ“ê¸€ìˆ˜']
        subreddit_stats = subreddit_stats.sort_values('ì´_ì ìˆ˜', ascending=False)
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # ê²Œì‹œë¬¼ ìˆ˜
        axes[0, 0].bar(subreddit_stats.index, subreddit_stats['ê²Œì‹œë¬¼_ìˆ˜'], color='orangered')
        axes[0, 0].set_title('ì„œë¸Œë ˆë”§ë³„ ê²Œì‹œë¬¼ ìˆ˜', fontsize=14)
        axes[0, 0].set_ylabel('ê²Œì‹œë¬¼ ìˆ˜')
        axes[0, 0].tick_params(axis='x', rotation=45)
        
        # ì´ ì ìˆ˜
        axes[0, 1].bar(subreddit_stats.index, subreddit_stats['ì´_ì ìˆ˜'], color='coral')
        axes[0, 1].set_title('ì„œë¸Œë ˆë”§ë³„ ì´ ì ìˆ˜', fontsize=14)
        axes[0, 1].set_ylabel('ì´ ì ìˆ˜')
        axes[0, 1].tick_params(axis='x', rotation=45)
        
        # í‰ê·  ì ìˆ˜
        axes[1, 0].bar(subreddit_stats.index, subreddit_stats['í‰ê· _ì ìˆ˜'], color='tomato')
        axes[1, 0].set_title('ì„œë¸Œë ˆë”§ë³„ í‰ê·  ì ìˆ˜', fontsize=14)
        axes[1, 0].set_ylabel('í‰ê·  ì ìˆ˜')
        axes[1, 0].tick_params(axis='x', rotation=45)
        
        # í‰ê·  ëŒ“ê¸€ ìˆ˜
        axes[1, 1].bar(subreddit_stats.index, subreddit_stats['í‰ê· _ëŒ“ê¸€ìˆ˜'], color='lightsalmon')
        axes[1, 1].set_title('ì„œë¸Œë ˆë”§ë³„ í‰ê·  ëŒ“ê¸€ ìˆ˜', fontsize=14)
        axes[1, 1].set_ylabel('í‰ê·  ëŒ“ê¸€ ìˆ˜')
        axes[1, 1].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        
        return fig, subreddit_stats


def search_and_collect_reddit_data(subreddit_names, search_query, post_limit, sort_by, time_filter, collect_comments, comment_limit):
    """Reddit APIë¥¼ í†µí•œ ë°ì´í„° ìˆ˜ì§‘"""
    load_dotenv()
    
    client_id = os.getenv("REDDIT_CLIENT_ID")
    client_secret = os.getenv("REDDIT_CLIENT_SECRET")
    user_agent = os.getenv("REDDIT_USER_AGENT", "RedditAnalyzer/1.0")
    
    if not client_id or not client_secret:
        st.error("Reddit API ìê²©ì¦ëª…ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        st.info("í•„ìš”í•œ í™˜ê²½ ë³€ìˆ˜: REDDIT_CLIENT_ID, REDDIT_CLIENT_SECRET, REDDIT_USER_AGENT")
        return None, None
    
    try:
        reddit = praw.Reddit(
            client_id=client_id,
            client_secret=client_secret,
            user_agent=user_agent
        )
    except Exception as e:
        st.error(f"Reddit API ì—°ê²° ì˜¤ë¥˜: {e}")
        return None, None
    
    all_posts = []
    all_comments = []
    
    # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ì„œë¸Œë ˆë”§ ì´ë¦„ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    subreddit_list = [s.strip() for s in subreddit_names.split(',') if s.strip()]
    total_subreddits = len(subreddit_list)
    
    if total_subreddits == 0:
        st.warning("ìˆ˜ì§‘í•  ì„œë¸Œë ˆë”§ ì´ë¦„ì´ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None, None

    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for idx, subreddit_name in enumerate(subreddit_list):
        status_text.text(f"ì„œë¸Œë ˆë”§ {idx+1}/{total_subreddits}: r/{subreddit_name} ìˆ˜ì§‘ ì¤‘...")
        
        try:
            # ì„œë¸Œë ˆë”§ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (optional but good practice)
            try:
                subreddit = reddit.subreddit(subreddit_name)
                # ì„œë¸Œë ˆë”§ ê°ì²´ ìì²´ì— ì ‘ê·¼í•˜ì—¬ ì˜ˆì™¸ ë°œìƒì‹œí‚¤ê¸° (ì˜ˆ: subreddit.title)
                _ = subreddit.title
            except ResponseException as e:
                 st.warning(f"ì„œë¸Œë ˆë”§ r/{subreddit_name}ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ì¡´ì¬í•˜ì§€ ì•Šê±°ë‚˜ ë¹„ê³µê°œ). ìŠ¤í‚µí•©ë‹ˆë‹¤. ì˜¤ë¥˜: {e}")
                 progress_bar.progress((idx + 1) / total_subreddits)
                 continue


            # ê²Œì‹œë¬¼ ê²€ìƒ‰/ìˆ˜ì§‘
            if search_query:
                posts = subreddit.search(search_query, sort=sort_by, time_filter=time_filter, limit=post_limit)
            else:
                if sort_by == 'hot':
                    posts = subreddit.hot(limit=post_limit)
                elif sort_by == 'new':
                    posts = subreddit.new(limit=post_limit)
                elif sort_by == 'top':
                    posts = subreddit.top(time_filter=time_filter, limit=post_limit)
                elif sort_by == 'rising':
                    posts = subreddit.rising(limit=post_limit)
                else:
                    posts = subreddit.hot(limit=post_limit)
            
            # ê²Œì‹œë¬¼ ë°ì´í„° ìˆ˜ì§‘
            for post in posts:
                post_data = {
                    'post_id': post.id,
                    'subreddit': post.subreddit.display_name if hasattr(post.subreddit, 'display_name') else subreddit_name,
                    'title': post.title,
                    'selftext': post.selftext,
                    'author': str(post.author) if post.author else '[deleted]',
                    'score': post.score,
                    'upvote_ratio': post.upvote_ratio,
                    'num_comments': post.num_comments,
                    'created_utc': post.created_utc,
                    'url': post.url,
                    'permalink': f"https://reddit.com{post.permalink}"
                }
                all_posts.append(post_data)
                
                # ëŒ“ê¸€ ìˆ˜ì§‘
                if collect_comments and post.num_comments > 0:
                    try:
                        # PRAWì˜ 'replace_more'ëŠ” ì¶”ê°€ ëŒ“ê¸€ì„ ë¡œë“œí•˜ëŠ” ê¸°ëŠ¥. limit=0ì€ 'MoreComments' ê°ì²´ë¥¼ ë¡œë“œí•˜ì§€ ì•ŠìŒ
                        post.comments.replace_more(limit=0) 
                        comments = post.comments.list()[:comment_limit]
                        
                        for comment in comments:
                            if hasattr(comment, 'body') and comment.body is not None:
                                comment_data = {
                                    'comment_id': comment.id,
                                    'post_id': post.id,
                                    'subreddit': post.subreddit.display_name if hasattr(post.subreddit, 'display_name') else subreddit_name,
                                    'author': str(comment.author) if comment.author else '[deleted]',
                                    'body': comment.body,
                                    'score': comment.score,
                                    'created_utc': comment.created_utc,
                                    'post_title': post.title
                                }
                                all_comments.append(comment_data)
                    except Exception as e:
                        # ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘ ë°œìƒí•˜ëŠ” ë„¤íŠ¸ì›Œí¬ ë˜ëŠ” ê¸°íƒ€ ì˜¤ë¥˜ ì²˜ë¦¬
                        st.warning(f"ê²Œì‹œë¬¼ {post.id} ëŒ“ê¸€ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        
        except Exception as e:
            # ì„œë¸Œë ˆë”§ ì ‘ê·¼ ë“± ì£¼ìš” ì˜¤ë¥˜ ì²˜ë¦¬
            st.error(f"ì„œë¸Œë ˆë”§ r/{subreddit_name} ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        
        progress_bar.progress((idx + 1) / total_subreddits)
    
    progress_bar.empty()
    status_text.empty()
    
    posts_df = pd.DataFrame(all_posts)
    comments_df = pd.DataFrame(all_comments) if all_comments else None
    
    return posts_df, comments_df


# ========================================
# Streamlit ë©”ì¸ ì•±
# ========================================

def main():
    st.title("ğŸ”´ Reddit ë¶„ì„ ëŒ€ì‹œë³´ë“œ")
    st.markdown("---")
    
    # ì‚¬ì´ë“œë°” - ë°ì´í„° ìˆ˜ì§‘/ì—…ë¡œë“œ
    st.sidebar.header("ğŸ“‚ ë°ì´í„° ì†ŒìŠ¤")
    data_source = st.sidebar.radio(
        "ë°ì´í„° ì…ë ¥ ë°©ì‹ ì„ íƒ",
        ["APIë¡œ ì‹¤ì‹œê°„ ìˆ˜ì§‘", "CSV íŒŒì¼ ì—…ë¡œë“œ"]
    )
    
    # ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸ ì´ˆê¸°í™” ë° ë¡œë“œ
    if 'posts_df' not in st.session_state:
         st.session_state['posts_df'] = pd.DataFrame(columns=['post_id', 'title', 'subreddit', 'score', 'num_comments'])
    if 'comments_df' not in st.session_state:
         st.session_state['comments_df'] = None # pd.DataFrame(columns=['comment_id', 'body'])

    posts_df = st.session_state['posts_df']
    comments_df = st.session_state['comments_df']
    
    
    if data_source == "APIë¡œ ì‹¤ì‹œê°„ ìˆ˜ì§‘":
        st.sidebar.subheader("ğŸ” ê²€ìƒ‰ ì„¤ì •")
        subreddit_names = st.sidebar.text_input(
            "ì„œë¸Œë ˆë”§ (ì‰¼í‘œë¡œ êµ¬ë¶„)",
            value="kbeauty,AsianBeauty",
            help="ì˜ˆ: kbeauty,AsianBeauty,SkincareAddiction"
        )
        search_query = st.sidebar.text_input(
            "ê²€ìƒ‰ì–´ (ì„ íƒ)",
            value="",
            help="ë¹„ì›Œë‘ë©´ ì„œë¸Œë ˆë”§ì˜ 'sort_by'ì— í•´ë‹¹í•˜ëŠ” ê²Œì‹œë¬¼ ìˆ˜ì§‘"
        )
        post_limit = st.sidebar.slider("ê²Œì‹œë¬¼ ê°œìˆ˜", 10, 500, 100)
        
        sort_by = st.sidebar.selectbox(
            "ì •ë ¬ ë°©ì‹",
            ["hot", "new", "top", "rising"],
            format_func=lambda x: {
                "hot": "ì¸ê¸°ìˆœ",
                "new": "ìµœì‹ ìˆœ",
                "top": "ìµœê³  í‰ì ",
                "rising": "ê¸‰ìƒìŠ¹"
            }[x]
        )
        
        time_filter = st.sidebar.selectbox(
            "ê¸°ê°„ í•„í„° (top/searchë§Œ ì ìš©)",
            ["all", "day", "week", "month", "year"],
            format_func=lambda x: {
                "all": "ì „ì²´",
                "day": "ì˜¤ëŠ˜",
                "week": "ì´ë²ˆ ì£¼",
                "month": "ì´ë²ˆ ë‹¬",
                "year": "ì˜¬í•´"
            }[x]
        )
        
        collect_comments = st.sidebar.checkbox("ëŒ“ê¸€ë„ ìˆ˜ì§‘", value=True)
        comment_limit = st.sidebar.slider("ê²Œì‹œë¬¼ë‹¹ ëŒ“ê¸€ ìˆ˜", 10, 200, 50) if collect_comments else 0
        
        if st.sidebar.button("ğŸš€ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘"):
            with st.spinner("Reddit ë°ì´í„° ìˆ˜ì§‘ ì¤‘..."):
                posts_df_new, comments_df_new = search_and_collect_reddit_data(
                    subreddit_names, search_query, post_limit, 
                    sort_by, time_filter, collect_comments, comment_limit
                )
            
            if posts_df_new is not None and not posts_df_new.empty:
                st.success(f"âœ… ê²Œì‹œë¬¼ **{len(posts_df_new):,}**ê°œ ìˆ˜ì§‘ ì™„ë£Œ!")
                if comments_df_new is not None:
                    st.success(f"âœ… ëŒ“ê¸€ **{len(comments_df_new):,}**ê°œ ìˆ˜ì§‘ ì™„ë£Œ!")
                
                # ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸ì— ì €ì¥ (ìƒˆ ë°ì´í„°ë¡œ ë®ì–´ì“°ê¸°)
                st.session_state['posts_df'] = posts_df_new
                st.session_state['comments_df'] = comments_df_new
            else:
                st.warning("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    else:  # CSV íŒŒì¼ ì—…ë¡œë“œ
        st.sidebar.subheader("ğŸ“¤ íŒŒì¼ ì—…ë¡œë“œ")
        posts_file = st.sidebar.file_uploader("ê²Œì‹œë¬¼ CSV íŒŒì¼", type=['csv'], key="posts_upload")
        comments_file = st.sidebar.file_uploader("ëŒ“ê¸€ CSV íŒŒì¼ (ì„ íƒ)", type=['csv'], key="comments_upload")
        
        if posts_file:
            posts_df_loaded = pd.read_csv(posts_file)
            st.session_state['posts_df'] = posts_df_loaded
            st.sidebar.success(f"âœ… ê²Œì‹œë¬¼ **{len(posts_df_loaded):,}**ê°œ ë¡œë“œ")
        
        if comments_file:
            comments_df_loaded = pd.read_csv(comments_file)
            st.session_state['comments_df'] = comments_df_loaded
            st.sidebar.success(f"âœ… ëŒ“ê¸€ **{len(comments_df_loaded):,}**ê°œ ë¡œë“œ")

    
    posts_df = st.session_state['posts_df']
    comments_df = st.session_state['comments_df']

    # ë°ì´í„°ê°€ ì—†ì„ ê²½ìš° ë¶„ì„ì„ ì§„í–‰í•˜ì§€ ì•ŠìŒ
    if posts_df.empty:
        st.warning("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. APIë¡œ ìˆ˜ì§‘í•˜ê±°ë‚˜ CSV íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        return # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ main í•¨ìˆ˜ ì¢…ë£Œ

    # ê¸°ë³¸ í†µê³„
    st.header("ğŸ“ˆ ê¸°ë³¸ í†µê³„")
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric("ì´ ê²Œì‹œë¬¼ ìˆ˜", f"{len(posts_df):,}")
    with col2:
        st.metric("í‰ê·  ì ìˆ˜", f"{posts_df['score'].mean():.1f}")
    with col3:
        st.metric("í‰ê·  ëŒ“ê¸€ ìˆ˜", f"{posts_df['num_comments'].mean():.1f}")
    with col4:
        # ğŸŒŸ ì²« ë²ˆì§¸ ë“¤ì—¬ì“°ê¸° ì˜¤ë¥˜ ìˆ˜ì • ë¶€ë¶„: with col4: ë‹¤ìŒ ë‚´ìš© ë“¤ì—¬ì“°ê¸°
        if comments_df is not None:
            st.metric("ì´ ëŒ“ê¸€ ìˆ˜", f"{len(comments_df):,}")

    st.markdown("---")

    # íƒ­ìœ¼ë¡œ ë¶„ì„ ëª¨ë“œ êµ¬ë¶„
    tabs = st.tabs([
    "â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œ",
    "ğŸ“Š í‚¤ì›Œë“œ ë¹ˆë„",
    "ğŸ˜ŠğŸ˜¢ ê°ì„± ë¶„ì„",
    "ğŸ“ˆ ì‹œê°„ íŠ¸ë Œë“œ",
    "ğŸ¯ ì„œë¸Œë ˆë”§ ë¹„êµ",
    "ğŸ“‹ ì›ë³¸ ë°ì´í„°"
    ])

    analyzer = RedditAnalyzer(posts_df, comments_df)
    
    # í…ìŠ¤íŠ¸ ë¶„ì„ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë°ì´í„°í”„ë ˆì„ í™•ì¸
    text_sources_available = ["ê²Œì‹œë¬¼ ì œëª©"]
    if 'selftext' in posts_df.columns and not posts_df['selftext'].isnull().all():
         text_sources_available.append("ê²Œì‹œë¬¼ ë³¸ë¬¸")
    if comments_df is not None and 'body' in comments_df.columns and not comments_df['body'].isnull().all():
         text_sources_available.append("ëŒ“ê¸€")


    # íƒ­ 1: ì›Œë“œí´ë¼ìš°ë“œ
    with tabs[0]:
        st.header("â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œ")
        
        if not text_sources_available:
             st.warning("í…ìŠ¤íŠ¸ ë°ì´í„°(ì œëª©, ë³¸ë¬¸, ëŒ“ê¸€)ê°€ ì—†ì–´ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            text_source = st.radio(
                "í…ìŠ¤íŠ¸ ì†ŒìŠ¤",
                text_sources_available,
                horizontal=True,
                key="wordcloud_source"
            )

            if st.button("ğŸ” ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±", key="btn_wordcloud"):
                with st.spinner(f"{text_source} ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì¤‘..."):
                    if text_source == "ê²Œì‹œë¬¼ ì œëª©":
                        fig = analyzer.wordcloud(posts_df['title'])
                    elif text_source == "ê²Œì‹œë¬¼ ë³¸ë¬¸":
                        fig = analyzer.wordcloud(posts_df['selftext'])
                    else:  # ëŒ“ê¸€
                        fig = analyzer.wordcloud(comments_df['body'])
                    st.pyplot(fig)
            else:
                st.info("ğŸ‘† í…ìŠ¤íŠ¸ ì†ŒìŠ¤ë¥¼ ì„ íƒí•˜ê³  ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")

    # íƒ­ 2: í‚¤ì›Œë“œ ë¹ˆë„
    with tabs[1]:
        st.header("ğŸ“Š í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„")
        
        if not text_sources_available:
             st.warning("í…ìŠ¤íŠ¸ ë°ì´í„°(ì œëª©, ë³¸ë¬¸, ëŒ“ê¸€)ê°€ ì—†ì–´ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            text_source = st.radio(
                "í…ìŠ¤íŠ¸ ì†ŒìŠ¤",
                text_sources_available,
                horizontal=True,
                key="keyword_source"
            )
            top_n = st.slider("í‘œì‹œí•  í‚¤ì›Œë“œ ê°œìˆ˜", 10, 50, 20, key="keyword_top_n")

            if st.button("ğŸ” í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„", key="btn_keyword"):
                with st.spinner(f"{text_source} í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„ ì¤‘..."):
                    if text_source == "ê²Œì‹œë¬¼ ì œëª©":
                        fig, freq_df = analyzer.keyword_frequency(posts_df['title'], top_n=top_n)
                    elif text_source == "ê²Œì‹œë¬¼ ë³¸ë¬¸":
                        fig, freq_df = analyzer.keyword_frequency(posts_df['selftext'], top_n=top_n)
                    else:
                        fig, freq_df = analyzer.keyword_frequency(comments_df['body'], top_n=top_n)
                    
                    if fig:
                        st.pyplot(fig)
                    
                        st.subheader("ğŸ“‹ í‚¤ì›Œë“œ ë°ì´í„°")
                        st.dataframe(freq_df, use_container_width=True)
                        
                        csv = freq_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
                        st.download_button(
                            "ğŸ’¾ CSV ë‹¤ìš´ë¡œë“œ",
                            csv,
                            "reddit_keyword_frequency.csv",
                            "text/csv",
                            key='download-keyword-csv'
                        )
                    else:
                        st.warning("ë¶„ì„í•  ìœ íš¨í•œ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.info("ğŸ‘† í…ìŠ¤íŠ¸ ì†ŒìŠ¤ë¥¼ ì„ íƒí•˜ê³  ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")

    # íƒ­ 3: ê°ì„± ë¶„ì„
    with tabs[2]:
        st.header("ğŸ˜ŠğŸ˜¢ ê°ì„± ë¶„ì„")
        
        if not text_sources_available:
             st.warning("í…ìŠ¤íŠ¸ ë°ì´í„°(ì œëª©, ë³¸ë¬¸, ëŒ“ê¸€)ê°€ ì—†ì–´ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            text_source = st.radio(
                "í…ìŠ¤íŠ¸ ì†ŒìŠ¤",
                text_sources_available,
                horizontal=True,
                key="sentiment_source"
            )

            if st.button("ğŸ” ê°ì„± ë¶„ì„ ì‹¤í–‰", key="btn_sentiment"):
                with st.spinner(f"{text_source} ê°ì„± ë¶„ì„ ì¤‘..."):
                    if text_source == "ê²Œì‹œë¬¼ ì œëª©":
                        fig, sentiment_counts = analyzer.sentiment_analysis(posts_df['title'])
                    elif text_source == "ê²Œì‹œë¬¼ ë³¸ë¬¸":
                        fig, sentiment_counts = analyzer.sentiment_analysis(posts_df['selftext'])
                    else:
                        fig, sentiment_counts = analyzer.sentiment_analysis(comments_df['body'])
                    
                    if fig:
                        st.pyplot(fig)
                        
                        st.subheader("ğŸ“Š ê°ì„± ìš”ì•½")
                        col1_s, col2_s, col3_s = st.columns(3)
                        
                        # ê¸ì •, ë¶€ì •, ì¤‘ë¦½ ìˆœì„œëŒ€ë¡œ í‘œì‹œ
                        for idx, sentiment in enumerate(['ê¸ì •', 'ë¶€ì •', 'ì¤‘ë¦½']):
                             count = sentiment_counts.get(sentiment, 0)
                             with [col1_s, col2_s, col3_s][idx]:
                                 st.metric(sentiment, f"{count:,}ê°œ")
                    else:
                        st.warning("ê°ì„± ë¶„ì„ì„ ìˆ˜í–‰í•  í…ìŠ¤íŠ¸ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.info("ğŸ‘† í…ìŠ¤íŠ¸ ì†ŒìŠ¤ë¥¼ ì„ íƒí•˜ê³  ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")

    # íƒ­ 4: ì‹œê°„ íŠ¸ë Œë“œ
    with tabs[3]:
        st.header("ğŸ“ˆ ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„")

        data_source_trend_options = ["ê²Œì‹œë¬¼"]
        if comments_df is not None:
             data_source_trend_options.append("ëŒ“ê¸€")

        data_source_trend = st.radio(
            "ë°ì´í„° ì†ŒìŠ¤",
            data_source_trend_options,
            horizontal=True
        )
        interval = st.radio("ì‹œê°„ ê°„ê²©", ["D (ì¼)", "W (ì£¼)", "M (ì›”)"], horizontal=True, key="time_interval")
        interval_code = interval.split()[0]

        if st.button("ğŸ” ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„", key="btn_time"):
            with st.spinner(f"{data_source_trend} ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„ ì¤‘..."):
                if data_source_trend == "ê²Œì‹œë¬¼":
                    fig, trend_df = analyzer.time_trend(posts_df, interval=interval_code)
                else:
                    fig, trend_df = analyzer.time_trend(comments_df, interval=interval_code)
                
                if fig:
                    st.pyplot(fig)
                    
                    st.subheader("ğŸ“‹ íŠ¸ë Œë“œ ë°ì´í„°")
                    st.dataframe(trend_df, use_container_width=True)
                    
                    csv = trend_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
                    st.download_button(
                        "ğŸ’¾ CSV ë‹¤ìš´ë¡œë“œ",
                        csv,
                        "reddit_time_trend.csv",
                        "text/csv",
                        key='download-trend-csv'
                    )
                else:
                    st.warning("ë‚ ì§œ ì •ë³´ê°€ ì—†ê±°ë‚˜ ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ì–´ ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.info("ğŸ‘† ë°ì´í„° ì†ŒìŠ¤ì™€ ì‹œê°„ ê°„ê²©ì„ ì„ íƒí•˜ê³  ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")

    # íƒ­ 5: ì„œë¸Œë ˆë”§ ë¹„êµ
    with tabs[4]:
        st.header("ğŸ¯ ì„œë¸Œë ˆë”§ ë¹„êµ ë¶„ì„")

        if st.button("ğŸ” ì„œë¸Œë ˆë”§ ë¹„êµ ë¶„ì„", key="btn_subreddit"):
            with st.spinner("ì„œë¸Œë ˆë”§ ë¹„êµ ë¶„ì„ ì¤‘..."):
                fig, comparison_df = analyzer.subreddit_comparison()
                if fig:
                    st.pyplot(fig)
                    
                    st.subheader("ğŸ“‹ ì„œë¸Œë ˆë”§ í†µê³„")
                    st.dataframe(comparison_df, use_container_width=True)
                    
                    # ì¸ë±ìŠ¤(ì„œë¸Œë ˆë”§ ì´ë¦„)ë¥¼ í¬í•¨í•˜ì—¬ CSV ì €ì¥
                    csv = comparison_df.to_csv(encoding='utf-8-sig').encode('utf-8-sig') 
                    st.download_button(
                        "ğŸ’¾ CSV ë‹¤ìš´ë¡œë“œ",
                        csv,
                        "reddit_subreddit_comparison.csv",
                        "text/csv",
                        key='download-subreddit-csv'
                    )
                else:
                    st.warning("ì„œë¸Œë ˆë”§ ì •ë³´ê°€ ì—†ê±°ë‚˜ ë¹„êµí•  ì„œë¸Œë ˆë”§ì´ 2ê°œ ë¯¸ë§Œì…ë‹ˆë‹¤.")
        else:
            st.info("ğŸ‘† ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ì„œë¸Œë ˆë”§ë³„ í†µê³„ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    # íƒ­ 6: ì›ë³¸ ë°ì´í„°
    with tabs[5]:
        st.header("ğŸ“‹ ì›ë³¸ ë°ì´í„°")

        data_type_options = ["ê²Œì‹œë¬¼ ë°ì´í„°"]
        if comments_df is not None:
             data_type_options.append("ëŒ“ê¸€ ë°ì´í„°")
             
        data_type = st.radio(
            "ë°ì´í„° ìœ í˜• ì„ íƒ",
            data_type_options,
            horizontal=True
        )

        if data_type == "ê²Œì‹œë¬¼ ë°ì´í„°":
            st.subheader("ğŸ“ ê²Œì‹œë¬¼ ë°ì´í„°")
            
            # ì»¬ëŸ¼ ì„ íƒ
            display_columns = st.multiselect(
                "í‘œì‹œí•  ì»¬ëŸ¼ ì„ íƒ",
                posts_df.columns.tolist(),
                default=['title', 'subreddit', 'score', 'num_comments', 'author'][:min(5, len(posts_df.columns))],
                key='posts_cols_select'
            )
            
            if display_columns:
                st.dataframe(posts_df[display_columns], use_container_width=True, height=600)
            else:
                st.dataframe(posts_df, use_container_width=True, height=600)
            
            csv = posts_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
            st.download_button(
                "ğŸ’¾ ê²Œì‹œë¬¼ ë°ì´í„° CSV ë‹¤ìš´ë¡œë“œ",
                csv,
                "reddit_posts_data.csv",
                "text/csv",
                key='download-posts-raw'
            )

        # ğŸŒŸ ë‘ ë²ˆì§¸ ë“¤ì—¬ì“°ê¸° ì˜¤ë¥˜ ìˆ˜ì • ë¶€ë¶„: else ë‹¤ìŒì— ì˜¤ëŠ” if/else ë¸”ë¡ ë“¤ì—¬ì“°ê¸°
        else:
            if comments_df is not None and not comments_df.empty:
                st.subheader("ğŸ’¬ ëŒ“ê¸€ ë°ì´í„°")
                
                # ì»¬ëŸ¼ ì„ íƒ
                display_columns = st.multiselect(
                    "í‘œì‹œí•  ì»¬ëŸ¼ ì„ íƒ",
                    comments_df.columns.tolist(),
                    default=['body', 'subreddit', 'score', 'author', 'post_title'][:min(5, len(comments_df.columns))],
                    key='comments_cols_select'
                )
                
                if display_columns:
                    st.dataframe(comments_df[display_columns], use_container_width=True, height=600)
                else:
                    st.dataframe(comments_df, use_container_width=True, height=600)
                
                csv = comments_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
                st.download_button(
                    "ğŸ’¾ ëŒ“ê¸€ ë°ì´í„° CSV ë‹¤ìš´ë¡œë“œ",
                    csv,
                    "reddit_comments_data.csv",
                    "text/csv",
                    key='download-comments-raw'
                )
            else:
                st.warning("ëŒ“ê¸€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()