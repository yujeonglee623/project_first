import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from datetime import datetime
import re
from wordcloud import WordCloud
import praw
from dotenv import load_dotenv
import os
from prawcore.exceptions import ResponseException, RequestException

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="Reddit ë¶„ì„ ëŒ€ì‹œë³´ë“œ",
    page_icon="ğŸ”´",
    layout="wide"
)

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'Malgun Gothic'  # Windows
# plt.rcParams['font.family'] = 'AppleGothic'  # Mac
plt.rcParams['axes.unicode_minus'] = False


class RedditAnalyzer:
    """Reddit ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self, posts_df, comments_df=None):
        self.posts_df = posts_df.copy()
        self.comments_df = comments_df.copy() if comments_df is not None else None
        
        # ë‚ ì§œ ì»¬ëŸ¼ ë³€í™˜
        if 'created_utc' in self.posts_df.columns:
            self.posts_df['created_utc'] = pd.to_datetime(self.posts_df['created_utc'], unit='s')
        if self.comments_df is not None and 'created_utc' in self.comments_df.columns:
            self.comments_df['created_utc'] = pd.to_datetime(self.comments_df['created_utc'], unit='s')
    
    
    def preprocess_text(self, text):
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        if pd.isna(text):
            return ""
        
        text = str(text).lower()
        text = re.sub(r'http\S+|www\S+', '', text)
        text = re.sub(r'[^ê°€-í£a-z0-9\s]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    
    def extract_keywords(self, text_series, min_length=2, top_n=50):
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        all_text = ' '.join(text_series.apply(self.preprocess_text))
        words = all_text.split()
        words = [w for w in words if len(w) >= min_length]
        
        stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
                    'of', 'is', 'are', 'was', 'were', 'been', 'be', 'have', 'has', 'had',
                    'ê·¸', 'ì´', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë“¤', 'ë°', 'ë˜í•œ', 'í•˜ë‹¤', 'ìˆë‹¤', 'ë˜ë‹¤',
                    'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ê·¸ëŸ°', 'ì´ëŸ°', 'ì €ëŸ°', 'removed', 'deleted'}
        
        words = [w for w in words if w not in stopwords]
        word_freq = Counter(words)
        
        return word_freq.most_common(top_n)
    
    
    def wordcloud(self, text_series, width=1200, height=800):
        """ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±"""
        all_text = ' '.join(text_series.apply(self.preprocess_text))
        
        wordcloud = WordCloud(
            font_path='malgun.ttf',
            width=width,
            height=height,
            background_color='white',
            max_words=100,
            relative_scaling=0.3,
            colormap='viridis'
        ).generate(all_text)
        
        fig, ax = plt.subplots(figsize=(15, 10))
        ax.imshow(wordcloud, interpolation='bilinear')
        ax.axis('off')
        ax.set_title('Reddit í…ìŠ¤íŠ¸ ì›Œë“œí´ë¼ìš°ë“œ', fontsize=20, pad=20)
        plt.tight_layout()
        
        return fig
    
    
    def keyword_frequency(self, text_series, top_n=20):
        """í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„"""
        keywords = self.extract_keywords(text_series, top_n=top_n)
        words, counts = zip(*keywords)
        
        fig, ax = plt.subplots(figsize=(12, 8))
        ax.barh(range(len(words)), counts, color='orangered')
        ax.set_yticks(range(len(words)))
        ax.set_yticklabels(words)
        ax.set_xlabel('ë¹ˆë„', fontsize=12)
        ax.set_title(f'ìƒìœ„ {top_n}ê°œ í‚¤ì›Œë“œ ë¹ˆë„', fontsize=16, pad=20)
        ax.invert_yaxis()
        plt.tight_layout()
        
        freq_df = pd.DataFrame(keywords, columns=['í‚¤ì›Œë“œ', 'ë¹ˆë„'])
        
        return fig, freq_df
    
    
    def sentiment_analysis(self, text_series):
        """ê°ì„± ë¶„ì„"""
        positive_words = {
            'good', 'great', 'excellent', 'amazing', 'wonderful', 'fantastic',
            'best', 'love', 'awesome', 'perfect', 'nice', 'happy', 'thank',
            'ì¢‹ë‹¤', 'ìµœê³ ', 'ëŒ€ë°•', 'ì˜ˆì˜ë‹¤', 'ë©‹ì§€ë‹¤', 'ì™„ë²½', 'ê°ì‚¬', 'í–‰ë³µ'
        }
        
        negative_words = {
            'bad', 'worst', 'terrible', 'awful', 'horrible', 'hate',
            'poor', 'disappointing', 'useless', 'waste', 'crap',
            'ì‹«ë‹¤', 'ë³„ë¡œ', 'ë‚˜ì˜ë‹¤', 'ìµœì•…', 'í˜•í¸ì—†ë‹¤', 'ì‹¤ë§'
        }
        
        def calculate_sentiment(text):
            text = self.preprocess_text(text)
            words = text.split()
            
            pos_count = sum(1 for w in words if w in positive_words)
            neg_count = sum(1 for w in words if w in negative_words)
            
            if pos_count > neg_count:
                return 'ê¸ì •'
            elif pos_count < neg_count:
                return 'ë¶€ì •'
            else:
                return 'ì¤‘ë¦½'
        
        sentiments = text_series.apply(calculate_sentiment)
        sentiment_counts = sentiments.value_counts()
        
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        
        colors = ['#90EE90', '#FFB6C1', '#D3D3D3']
        axes[0].pie(sentiment_counts.values, labels=sentiment_counts.index, 
                   autopct='%1.1f%%', colors=colors, startangle=90)
        axes[0].set_title('ê°ì„± ë¶„í¬', fontsize=14, pad=20)
        
        axes[1].bar(sentiment_counts.index, sentiment_counts.values, color=colors)
        axes[1].set_xlabel('ê°ì„±', fontsize=12)
        axes[1].set_ylabel('ê°œìˆ˜', fontsize=12)
        axes[1].set_title('ê°ì„±ë³„ ê°œìˆ˜', fontsize=14, pad=20)
        
        plt.tight_layout()
        
        return fig, sentiment_counts
    
    
    def time_trend(self, df, date_col='created_utc', interval='D'):
        """ì‹œê°„ëŒ€ë³„ íŠ¸ë Œë“œ ë¶„ì„"""
        if date_col not in df.columns:
            return None, None
        
        # ìˆ˜ì •: set_index í›„ sort_index() ì¶”ê°€í•˜ì—¬ ì¸ë±ìŠ¤ë¥¼ ì •ë ¬ (resampleì„ ìœ„í•œ monotonic index í™•ë³´)
        df_sorted = df.set_index(date_col).sort_index()
        time_counts = df_sorted.resample(interval).size()
        
        if 'score' in df.columns:
            time_scores = df_sorted['score'].resample(interval).sum()
        else:
            time_scores = None
        
        if time_scores is not None:
            fig, axes = plt.subplots(2, 1, figsize=(14, 10))
            
            axes[0].plot(time_counts.index, time_counts.values, marker='o', linewidth=2, color='orangered')
            axes[0].set_xlabel('ë‚ ì§œ', fontsize=12)
            axes[0].set_ylabel('ê²Œì‹œë¬¼/ëŒ“ê¸€ ìˆ˜', fontsize=12)
            axes[0].set_title('ì‹œê°„ëŒ€ë³„ ê²Œì‹œë¬¼/ëŒ“ê¸€ ìˆ˜ ì¶”ì´', fontsize=14, pad=20)
            axes[0].grid(True, alpha=0.3)
            
            axes[1].plot(time_scores.index, time_scores.values, marker='o', 
                        color='coral', linewidth=2)
            axes[1].set_xlabel('ë‚ ì§œ', fontsize=12)
            axes[1].set_ylabel('ì ìˆ˜ í•©ê³„', fontsize=12)
            axes[1].set_title('ì‹œê°„ëŒ€ë³„ ì ìˆ˜ ì¶”ì´', fontsize=14, pad=20)
            axes[1].grid(True, alpha=0.3)
            
            trend_df = pd.DataFrame({
                'ë‚ ì§œ': time_counts.index,
                'ê°œìˆ˜': time_counts.values,
                'ì ìˆ˜': time_scores.values
            })
        else:
            fig, ax = plt.subplots(figsize=(14, 6))
            
            ax.plot(time_counts.index, time_counts.values, marker='o', linewidth=2, color='orangered')
            ax.set_xlabel('ë‚ ì§œ', fontsize=12)
            ax.set_ylabel('ê²Œì‹œë¬¼/ëŒ“ê¸€ ìˆ˜', fontsize=12)
            ax.set_title('ì‹œê°„ëŒ€ë³„ ê²Œì‹œë¬¼/ëŒ“ê¸€ ìˆ˜ ì¶”ì´', fontsize=14, pad=20)
            ax.grid(True, alpha=0.3)
            
            trend_df = pd.DataFrame({
                'ë‚ ì§œ': time_counts.index,
                'ê°œìˆ˜': time_counts.values
            })
        
        plt.tight_layout()
        
        return fig, trend_df
    
    
    def subreddit_comparison(self):
        """ì„œë¸Œë ˆë”§ë³„ ë¹„êµ ë¶„ì„"""
        if 'subreddit' not in self.posts_df.columns:
            return None, None
        
        subreddit_stats = self.posts_df.groupby('subreddit').agg({
            'score': ['mean', 'sum', 'count'],
            'num_comments': 'mean'
        }).round(2)
        
        subreddit_stats.columns = ['í‰ê· _ì ìˆ˜', 'ì´_ì ìˆ˜', 'ê²Œì‹œë¬¼_ìˆ˜', 'í‰ê· _ëŒ“ê¸€ìˆ˜']
        subreddit_stats = subreddit_stats.sort_values('ì´_ì ìˆ˜', ascending=False)
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # ê²Œì‹œë¬¼ ìˆ˜
        axes[0, 0].bar(subreddit_stats.index, subreddit_stats['ê²Œì‹œë¬¼_ìˆ˜'], color='orangered')
        axes[0, 0].set_title('ì„œë¸Œë ˆë”§ë³„ ê²Œì‹œë¬¼ ìˆ˜', fontsize=14)
        axes[0, 0].set_ylabel('ê²Œì‹œë¬¼ ìˆ˜')
        axes[0, 0].tick_params(axis='x', rotation=45)
        
        # ì´ ì ìˆ˜
        axes[0, 1].bar(subreddit_stats.index, subreddit_stats['ì´_ì ìˆ˜'], color='coral')
        axes[0, 1].set_title('ì„œë¸Œë ˆë”§ë³„ ì´ ì ìˆ˜', fontsize=14)
        axes[0, 1].set_ylabel('ì´ ì ìˆ˜')
        axes[0, 1].tick_params(axis='x', rotation=45)
        
        # í‰ê·  ì ìˆ˜
        axes[1, 0].bar(subreddit_stats.index, subreddit_stats['í‰ê· _ì ìˆ˜'], color='tomato')
        axes[1, 0].set_title('ì„œë¸Œë ˆë”§ë³„ í‰ê·  ì ìˆ˜', fontsize=14)
        axes[1, 0].set_ylabel('í‰ê·  ì ìˆ˜')
        axes[1, 0].tick_params(axis='x', rotation=45)
        
        # í‰ê·  ëŒ“ê¸€ ìˆ˜
        axes[1, 1].bar(subreddit_stats.index, subreddit_stats['í‰ê· _ëŒ“ê¸€ìˆ˜'], color='lightsalmon')
        axes[1, 1].set_title('ì„œë¸Œë ˆë”§ë³„ í‰ê·  ëŒ“ê¸€ ìˆ˜', fontsize=14)
        axes[1, 1].set_ylabel('í‰ê·  ëŒ“ê¸€ ìˆ˜')
        axes[1, 1].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        
        return fig, subreddit_stats


def search_and_collect_reddit_data(subreddit_names, search_query, post_limit, sort_by, time_filter, collect_comments, comment_limit):
    """Reddit APIë¥¼ í†µí•œ ë°ì´í„° ìˆ˜ì§‘"""
    load_dotenv()
    
    client_id = os.getenv("REDDIT_CLIENT_ID")
    client_secret = os.getenv("REDDIT_CLIENT_SECRET")
    user_agent = os.getenv("REDDIT_USER_AGENT", "RedditAnalyzer/1.0")
    
    if not client_id or not client_secret:
        st.error("Reddit API ìê²©ì¦ëª…ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        st.info("í•„ìš”í•œ í™˜ê²½ ë³€ìˆ˜: REDDIT_CLIENT_ID, REDDIT_CLIENT_SECRET, REDDIT_USER_AGENT")
        return None, None
    
    try:
        reddit = praw.Reddit(
            client_id=client_id,
            client_secret=client_secret,
            user_agent=user_agent
        )
    except Exception as e:
        st.error(f"Reddit API ì—°ê²° ì˜¤ë¥˜: {e}")
        return None, None
    
    all_posts = []
    all_comments = []
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    subreddit_list = [s.strip() for s in subreddit_names.split(',')]
    total_subreddits = len(subreddit_list)
    
    for idx, subreddit_name in enumerate(subreddit_list):
        status_text.text(f"ì„œë¸Œë ˆë”§ {idx+1}/{total_subreddits}: r/{subreddit_name} ìˆ˜ì§‘ ì¤‘...")
        
        try:
            subreddit = reddit.subreddit(subreddit_name)
            
            # ê²Œì‹œë¬¼ ê²€ìƒ‰/ìˆ˜ì§‘
            if search_query:
                posts = subreddit.search(search_query, sort=sort_by, time_filter=time_filter, limit=post_limit)
            else:
                if sort_by == 'hot':
                    posts = subreddit.hot(limit=post_limit)
                elif sort_by == 'new':
                    posts = subreddit.new(limit=post_limit)
                elif sort_by == 'top':
                    posts = subreddit.top(time_filter=time_filter, limit=post_limit)
                elif sort_by == 'rising':
                    posts = subreddit.rising(limit=post_limit)
                else:
                    posts = subreddit.hot(limit=post_limit)
            
            for post in posts:
                post_data = {
                    'post_id': post.id,
                    'subreddit': post.subreddit.display_name,
                    'title': post.title,
                    'selftext': post.selftext,
                    'author': str(post.author),
                    'score': post.score,
                    'upvote_ratio': post.upvote_ratio,
                    'num_comments': post.num_comments,
                    'created_utc': post.created_utc,
                    'url': post.url,
                    'permalink': f"https://reddit.com{post.permalink}"
                }
                all_posts.append(post_data)
                
                # ëŒ“ê¸€ ìˆ˜ì§‘
                if collect_comments:
                    try:
                        post.comments.replace_more(limit=0)
                        comments = post.comments.list()[:comment_limit]
                        
                        for comment in comments:
                            if hasattr(comment, 'body'):
                                comment_data = {
                                    'comment_id': comment.id,
                                    'post_id': post.id,
                                    'subreddit': post.subreddit.display_name,
                                    'author': str(comment.author),
                                    'body': comment.body,
                                    'score': comment.score,
                                    'created_utc': comment.created_utc,
                                    'post_title': post.title
                                }
                                all_comments.append(comment_data)
                    except Exception as e:
                        st.warning(f"ê²Œì‹œë¬¼ {post.id} ëŒ“ê¸€ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        
        except Exception as e:
            st.error(f"ì„œë¸Œë ˆë”§ r/{subreddit_name} ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        
        progress_bar.progress((idx + 1) / total_subreddits)
    
    progress_bar.empty()
    status_text.empty()
    
    posts_df = pd.DataFrame(all_posts)
    comments_df = pd.DataFrame(all_comments) if all_comments else None
    
    return posts_df, comments_df


# ========================================
# Streamlit ë©”ì¸ ì•±
# ========================================

def main():
    st.title("ğŸ”´ Reddit ë¶„ì„ ëŒ€ì‹œë³´ë“œ")
    st.markdown("---")
    
    # ì‚¬ì´ë“œë°” - ë°ì´í„° ìˆ˜ì§‘/ì—…ë¡œë“œ
    st.sidebar.header("ğŸ“‚ ë°ì´í„° ì†ŒìŠ¤")
    data_source = st.sidebar.radio(
        "ë°ì´í„° ì…ë ¥ ë°©ì‹ ì„ íƒ",
        ["APIë¡œ ì‹¤ì‹œê°„ ìˆ˜ì§‘", "CSV íŒŒì¼ ì—…ë¡œë“œ"]
    )
    
    posts_df = None
    comments_df = None
    
    if data_source == "APIë¡œ ì‹¤ì‹œê°„ ìˆ˜ì§‘":
        st.sidebar.subheader("ğŸ” ê²€ìƒ‰ ì„¤ì •")
        subreddit_names = st.sidebar.text_input(
            "ì„œë¸Œë ˆë”§ (ì‰¼í‘œë¡œ êµ¬ë¶„)",
            value="kbeauty,AsianBeauty",
            help="ì˜ˆ: kbeauty,AsianBeauty,SkincareAddiction"
        )
        search_query = st.sidebar.text_input(
            "ê²€ìƒ‰ì–´ (ì„ íƒ)",
            value="",
            help="ë¹„ì›Œë‘ë©´ ì„œë¸Œë ˆë”§ì˜ ëª¨ë“  ê²Œì‹œë¬¼ ìˆ˜ì§‘"
        )
        post_limit = st.sidebar.slider("ê²Œì‹œë¬¼ ê°œìˆ˜", 10, 500, 100)
        
        sort_by = st.sidebar.selectbox(
            "ì •ë ¬ ë°©ì‹",
            ["hot", "new", "top", "rising"],
            format_func=lambda x: {
                "hot": "ì¸ê¸°ìˆœ",
                "new": "ìµœì‹ ìˆœ",
                "top": "ìµœê³  í‰ì ",
                "rising": "ê¸‰ìƒìŠ¹"
            }[x]
        )
        
        time_filter = st.sidebar.selectbox(
            "ê¸°ê°„ í•„í„° (top/searchë§Œ ì ìš©)",
            ["all", "day", "week", "month", "year"],
            format_func=lambda x: {
                "all": "ì „ì²´",
                "day": "ì˜¤ëŠ˜",
                "week": "ì´ë²ˆ ì£¼",
                "month": "ì´ë²ˆ ë‹¬",
                "year": "ì˜¬í•´"
            }[x]
        )
        
        collect_comments = st.sidebar.checkbox("ëŒ“ê¸€ë„ ìˆ˜ì§‘", value=True)
        comment_limit = st.sidebar.slider("ê²Œì‹œë¬¼ë‹¹ ëŒ“ê¸€ ìˆ˜", 10, 200, 50) if collect_comments else 0
        
        if st.sidebar.button("ğŸš€ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘"):
            with st.spinner("Reddit ë°ì´í„° ìˆ˜ì§‘ ì¤‘..."):
                posts_df, comments_df = search_and_collect_reddit_data(
                    subreddit_names, search_query, post_limit, 
                    sort_by, time_filter, collect_comments, comment_limit
                )
            
            if posts_df is not None and not posts_df.empty:
                st.success(f"âœ… ê²Œì‹œë¬¼ {len(posts_df)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ!")
                if comments_df is not None:
                    st.success(f"âœ… ëŒ“ê¸€ {len(comments_df)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ!")
                
                # ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸ì— ì €ì¥
                st.session_state['posts_df'] = posts_df
                st.session_state['comments_df'] = comments_df
            else:
                st.warning("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    else:  # CSV íŒŒì¼ ì—…ë¡œë“œ
        st.sidebar.subheader("ğŸ“¤ íŒŒì¼ ì—…ë¡œë“œ")
        posts_file = st.sidebar.file_uploader("ê²Œì‹œë¬¼ CSV íŒŒì¼", type=['csv'])
        comments_file = st.sidebar.file_uploader("ëŒ“ê¸€ CSV íŒŒì¼ (ì„ íƒ)", type=['csv'])
        
        if posts_file:
            posts_df = pd.read_csv(posts_file)
            st.session_state['posts_df'] = posts_df
            st.sidebar.success(f"âœ… ê²Œì‹œë¬¼ {len(posts_df)}ê°œ ë¡œë“œ")
        
        if comments_file:
            comments_df = pd.read_csv(comments_file)
            st.session_state['comments_df'] = comments_df
            st.sidebar.success(f"âœ… ëŒ“ê¸€ {len(comments_df)}ê°œ ë¡œë“œ")
    
    # ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸ì—ì„œ ë°ì´í„° ë¡œë“œ
    if 'posts_df' in st.session_state:
        posts_df = st.session_state['posts_df']
    if 'comments_df' in st.session_state:
        comments_df = st.session_state['comments_df']
    # ê¸°ë³¸ í†µê³„
    st.header("ğŸ“ˆ ê¸°ë³¸ í†µê³„")
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric("ì´ ê²Œì‹œë¬¼ ìˆ˜", f"{len(posts_df):,}")
    with col2:
        st.metric("í‰ê·  ì ìˆ˜", f"{posts_df['score'].mean():.1f}")
    with col3:
        st.metric("í‰ê·  ëŒ“ê¸€ ìˆ˜", f"{posts_df['num_comments'].mean():.1f}")
    with col4:
    if comments_df is not None:
        st.metric("ì´ ëŒ“ê¸€ ìˆ˜", f"{len(comments_df):,}")

    st.markdown("---")

    # íƒ­ìœ¼ë¡œ ë¶„ì„ ëª¨ë“œ êµ¬ë¶„
    tabs = st.tabs([
    "â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œ",
    "ğŸ“Š í‚¤ì›Œë“œ ë¹ˆë„",
    "ğŸ˜ŠğŸ˜¢ ê°ì„± ë¶„ì„",
    "ğŸ“ˆ ì‹œê°„ íŠ¸ë Œë“œ",
    "ğŸ¯ ì„œë¸Œë ˆë”§ ë¹„êµ",
    "ğŸ“‹ ì›ë³¸ ë°ì´í„°"
    ])

    analyzer = RedditAnalyzer(posts_df, comments_df)

    # íƒ­ 1: ì›Œë“œí´ë¼ìš°ë“œ
    with tabs[0]:
    st.header("â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œ")

    text_source = st.radio(
        "í…ìŠ¤íŠ¸ ì†ŒìŠ¤",
        ["ê²Œì‹œë¬¼ ì œëª©", "ê²Œì‹œë¬¼ ë³¸ë¬¸", "ëŒ“ê¸€"] if comments_df is not None else ["ê²Œì‹œë¬¼ ì œëª©", "ê²Œì‹œë¬¼ ë³¸ë¬¸"],
        horizontal=True
    )

    if st.button("ğŸ” ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±", key="btn_wordcloud"):
        with st.spinner("ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì¤‘..."):
            if text_source == "ê²Œì‹œë¬¼ ì œëª©":
                fig = analyzer.wordcloud(posts_df['title'])
            elif text_source == "ê²Œì‹œë¬¼ ë³¸ë¬¸":
                fig = analyzer.wordcloud(posts_df['selftext'])
            else:  # ëŒ“ê¸€
                fig = analyzer.wordcloud(comments_df['body'])
            st.pyplot(fig)
    else:
        st.info("ğŸ‘† í…ìŠ¤íŠ¸ ì†ŒìŠ¤ë¥¼ ì„ íƒí•˜ê³  ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")

    # íƒ­ 2: í‚¤ì›Œë“œ ë¹ˆë„
    with tabs[1]:
    st.header("ğŸ“Š í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„")

    text_source = st.radio(
        "í…ìŠ¤íŠ¸ ì†ŒìŠ¤",
        ["ê²Œì‹œë¬¼ ì œëª©", "ê²Œì‹œë¬¼ ë³¸ë¬¸", "ëŒ“ê¸€"] if comments_df is not None else ["ê²Œì‹œë¬¼ ì œëª©", "ê²Œì‹œë¬¼ ë³¸ë¬¸"],
        horizontal=True,
        key="keyword_source"
    )
    top_n = st.slider("í‘œì‹œí•  í‚¤ì›Œë“œ ê°œìˆ˜", 10, 50, 20, key="keyword_top_n")

    if st.button("ğŸ” í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„", key="btn_keyword"):
        with st.spinner("í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„ ì¤‘..."):
            if text_source == "ê²Œì‹œë¬¼ ì œëª©":
                fig, freq_df = analyzer.keyword_frequency(posts_df['title'], top_n=top_n)
            elif text_source == "ê²Œì‹œë¬¼ ë³¸ë¬¸":
                fig, freq_df = analyzer.keyword_frequency(posts_df['selftext'], top_n=top_n)
            else:
                fig, freq_df = analyzer.keyword_frequency(comments_df['body'], top_n=top_n)
            
            st.pyplot(fig)
            
            st.subheader("ğŸ“‹ í‚¤ì›Œë“œ ë°ì´í„°")
            st.dataframe(freq_df, use_container_width=True)
            
            csv = freq_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
            st.download_button(
                "ğŸ’¾ CSV ë‹¤ìš´ë¡œë“œ",
                csv,
                "reddit_keyword_frequency.csv",
                "text/csv",
                key='download-keyword-csv'
            )
    else:
        st.info("ğŸ‘† í…ìŠ¤íŠ¸ ì†ŒìŠ¤ë¥¼ ì„ íƒí•˜ê³  ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")

    # íƒ­ 3: ê°ì„± ë¶„ì„
    with tabs[2]:
    st.header("ğŸ˜ŠğŸ˜¢ ê°ì„± ë¶„ì„")

    text_source = st.radio(
        "í…ìŠ¤íŠ¸ ì†ŒìŠ¤",
        ["ê²Œì‹œë¬¼ ì œëª©", "ê²Œì‹œë¬¼ ë³¸ë¬¸", "ëŒ“ê¸€"] if comments_df is not None else ["ê²Œì‹œë¬¼ ì œëª©", "ê²Œì‹œë¬¼ ë³¸ë¬¸"],
        horizontal=True,
        key="sentiment_source"
    )

    if st.button("ğŸ” ê°ì„± ë¶„ì„ ì‹¤í–‰", key="btn_sentiment"):
        with st.spinner("ê°ì„± ë¶„ì„ ì¤‘..."):
            if text_source == "ê²Œì‹œë¬¼ ì œëª©":
                fig, sentiment_counts = analyzer.sentiment_analysis(posts_df['title'])
            elif text_source == "ê²Œì‹œë¬¼ ë³¸ë¬¸":
                fig, sentiment_counts = analyzer.sentiment_analysis(posts_df['selftext'])
            else:
                fig, sentiment_counts = analyzer.sentiment_analysis(comments_df['body'])
            
            st.pyplot(fig)
            
            col1, col2, col3 = st.columns(3)
            for idx, (sentiment, count) in enumerate(sentiment_counts.items()):
                with [col1, col2, col3][idx % 3]:
                    st.metric(sentiment, f"{count:,}ê°œ")
    else:
        st.info("ğŸ‘† í…ìŠ¤íŠ¸ ì†ŒìŠ¤ë¥¼ ì„ íƒí•˜ê³  ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")

    # íƒ­ 4: ì‹œê°„ íŠ¸ë Œë“œ
    with tabs[3]:
    st.header("ğŸ“ˆ ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„")

    data_source_trend = st.radio(
        "ë°ì´í„° ì†ŒìŠ¤",
        ["ê²Œì‹œë¬¼", "ëŒ“ê¸€"] if comments_df is not None else ["ê²Œì‹œë¬¼"],
        horizontal=True
    )
    interval = st.radio("ì‹œê°„ ê°„ê²©", ["D (ì¼)", "W (ì£¼)", "M (ì›”)"], horizontal=True, key="time_interval")
    interval_code = interval.split()[0]

    if st.button("ğŸ” ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„", key="btn_time"):
        with st.spinner("ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„ ì¤‘..."):
            if data_source_trend == "ê²Œì‹œë¬¼":
                fig, trend_df = analyzer.time_trend(posts_df, interval=interval_code)
            else:
                fig, trend_df = analyzer.time_trend(comments_df, interval=interval_code)
            
            if fig:
                st.pyplot(fig)
                
                st.subheader("ğŸ“‹ íŠ¸ë Œë“œ ë°ì´í„°")
                st.dataframe(trend_df, use_container_width=True)
                
                csv = trend_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
                st.download_button(
                    "ğŸ’¾ CSV ë‹¤ìš´ë¡œë“œ",
                    csv,
                    "reddit_time_trend.csv",
                    "text/csv",
                    key='download-trend-csv'
                )
            else:
                st.warning("ë‚ ì§œ ì •ë³´ê°€ ì—†ì–´ ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.info("ğŸ‘† ë°ì´í„° ì†ŒìŠ¤ì™€ ì‹œê°„ ê°„ê²©ì„ ì„ íƒí•˜ê³  ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")

    # íƒ­ 5: ì„œë¸Œë ˆë”§ ë¹„êµ
    with tabs[4]:
    st.header("ğŸ¯ ì„œë¸Œë ˆë”§ ë¹„êµ ë¶„ì„")

    if st.button("ğŸ” ì„œë¸Œë ˆë”§ ë¹„êµ ë¶„ì„", key="btn_subreddit"):
        with st.spinner("ì„œë¸Œë ˆë”§ ë¹„êµ ë¶„ì„ ì¤‘..."):
            fig, comparison_df = analyzer.subreddit_comparison()
            if fig:
                st.pyplot(fig)
                
                st.subheader("ğŸ“‹ ì„œë¸Œë ˆë”§ í†µê³„")
                st.dataframe(comparison_df, use_container_width=True)
                
                csv = comparison_df.to_csv(encoding='utf-8-sig').encode('utf-8-sig')
                st.download_button(
                    "ğŸ’¾ CSV ë‹¤ìš´ë¡œë“œ",
                    csv,
                    "reddit_subreddit_comparison.csv",
                    "text/csv",
                    key='download-subreddit-csv'
                )
            else:
                st.warning("ì„œë¸Œë ˆë”§ ì •ë³´ê°€ ì—†ì–´ ë¹„êµ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.info("ğŸ‘† ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ì„œë¸Œë ˆë”§ë³„ í†µê³„ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    # íƒ­ 6: ì›ë³¸ ë°ì´í„°
    with tabs[5]:
    st.header("ğŸ“‹ ì›ë³¸ ë°ì´í„°")

    data_type = st.radio(
        "ë°ì´í„° ìœ í˜• ì„ íƒ",
        ["ê²Œì‹œë¬¼ ë°ì´í„°", "ëŒ“ê¸€ ë°ì´í„°"] if comments_df is not None else ["ê²Œì‹œë¬¼ ë°ì´í„°"],
        horizontal=True
    )

    if data_type == "ê²Œì‹œë¬¼ ë°ì´í„°":
        st.subheader("ğŸ“ ê²Œì‹œë¬¼ ë°ì´í„°")
        
        # ì»¬ëŸ¼ ì„ íƒ
        display_columns = st.multiselect(
            "í‘œì‹œí•  ì»¬ëŸ¼ ì„ íƒ",
            posts_df.columns.tolist(),
            default=['title', 'subreddit', 'score', 'num_comments', 'author'][:min(5, len(posts_df.columns))]
        )
        
        if display_columns:
            st.dataframe(posts_df[display_columns], use_container_width=True, height=600)
        else:
            st.dataframe(posts_df, use_container_width=True, height=600)
        
        csv = posts_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
        st.download_button(
            "ğŸ’¾ ê²Œì‹œë¬¼ ë°ì´í„° CSV ë‹¤ìš´ë¡œë“œ",
            csv,
            "reddit_posts_data.csv",
            "text/csv",
            key='download-posts-raw'
        )

    else:
        if comments_df is not None and not comments_df.empty:
            st.subheader("ğŸ’¬ ëŒ“ê¸€ ë°ì´í„°")
            
            # ì»¬ëŸ¼ ì„ íƒ
            display_columns = st.multiselect(
                "í‘œì‹œí•  ì»¬ëŸ¼ ì„ íƒ",
                comments_df.columns.tolist(),
                default=['body', 'subreddit', 'score', 'author', 'post_title'][:min(5, len(comments_df.columns))]
            )
            
            if display_columns:
                st.dataframe(comments_df[display_columns], use_container_width=True, height=600)
            else:
                st.dataframe(comments_df, use_container_width=True, height=600)
            
            csv = comments_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
            st.download_button(
                "ğŸ’¾ ëŒ“ê¸€ ë°ì´í„° CSV ë‹¤ìš´ë¡œë“œ",
                csv,
                "reddit_comments_data.csv",
                "text/csv",
                key='download-comments-raw'
            )
        else:
            st.warning("ëŒ“ê¸€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")


    if __name__ == "__main__":
    main()