import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from datetime import datetime
import re
from wordcloud import WordCloud
import praw
from dotenv import load_dotenv
import os
import requests # OpenAI API í˜¸ì¶œì„ ìœ„í•´ ì¶”ê°€
from prawcore.exceptions import ResponseException, RequestException

# ========================================
# Streamlit ê¸°ë³¸ ì„¤ì • ë° OpenAI ì„¤ì •
# ========================================

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="Reddit ë¶„ì„ ëŒ€ì‹œë³´ë“œ",
    page_icon="ğŸ”´",
    layout="wide"
)

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'Malgun Gothic'  # Windows
# plt.rcParams['font.family'] = 'AppleGothic'  # Mac
plt.rcParams['axes.unicode_minus'] = False

# ë³´ê³ ì„œ ì €ì¥ì„ ìœ„í•œ ë””ë ‰í† ë¦¬ ì„¤ì •
SAVE_DIR = "analysis_results"
os.makedirs(SAVE_DIR, exist_ok=True)


def generate_openai_report(keywords, api_key, model_name="gpt-4o"):
    """OpenAI APIë¥¼ ì´ìš©í•œ ë³´ê³ ì„œ ë¬¸ì¥ ìƒì„± í•¨ìˆ˜ (GPT-4o ì‚¬ìš©)"""
    
    if not api_key:
        return "Error: OpenAI API Key is missing. Please set the OPENAI_API_KEY in the .env file."

    # System Prompt: AIì˜ ì—­í• ê³¼ ì›í•˜ëŠ” ì¶œë ¥ í˜•ì‹ì„ ëª…í™•íˆ ì •ì˜
    system_prompt = (
        "You are a professional Social Media Market Analyst. "
        "Your task is to analyze the provided raw data summary or statistical analysis "
        "and generate a comprehensive, insightful, and professional English summary (approximately 5 detailed sentences). "
        "The summary must cover multiple facets, including key trends, sentiment drivers, quantitative findings, and strategic implications. "
        "If Korean comments are provided in the raw data, translate and interpret them. "
        "Do not use markdown headers or lists. Just provide the summary text."
    )
    
    # User Prompt: ì‹¤ì œ CSVì—ì„œ ì¶”ì¶œí•œ ë°ì´í„°ë¥¼ ì „ë‹¬
    user_prompt = f"Analyze the following data: {keywords}"

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}",
    }

    # API Payload
    payload = {
        "model": model_name,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        "max_tokens": 400,
        "temperature": 0.3,
    }

    try:
        response = requests.post("https://api.openai.com/v1/chat/completions", headers=headers, json=payload, timeout=40)
        response.raise_for_status() # HTTP ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ ì²˜ë¦¬
        
        result = response.json()
        
        # ê²°ê³¼ ì¶”ì¶œ
        summary = result['choices'][0]['message']['content'].strip()
        return summary

    except requests.exceptions.HTTPError as errh:
        return f"HTTP Error: {errh}. Check if your API key is valid and the model name is correct. (Status: {response.status_code})"
    except requests.exceptions.ConnectionError as errc:
        return f"Error Connecting: {errc}"
    except requests.exceptions.Timeout as errt:
        return f"Timeout Error: {errt}"
    except requests.exceptions.RequestException as err:
        return f"An Unexpected Error: {err}"
    except Exception as e:
        return f"API Error occurred: {e}. Check response structure."


# ========================================
# Reddit ë¶„ì„ í´ë˜ìŠ¤
# ========================================

class RedditAnalyzer:
    """Reddit ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self, posts_df, comments_df=None):
        self.posts_df = posts_df.copy()
        self.comments_df = comments_df.copy() if comments_df is not None and not comments_df.empty else None
        
        # ë‚ ì§œ ì»¬ëŸ¼ ë³€í™˜
        if 'created_utc' in self.posts_df.columns:
            self.posts_df['created_utc'] = pd.to_datetime(self.posts_df['created_utc'], unit='s', errors='coerce')
        if self.comments_df is not None and 'created_utc' in self.comments_df.columns:
            self.comments_df['created_utc'] = pd.to_datetime(self.comments_df['created_utc'], unit='s', errors='coerce')
    
    
    def preprocess_text(self, text):
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        if pd.isna(text) or text is None:
            return ""
        
        text = str(text).lower()
        text = re.sub(r'http\S+|www\S+', '', text)
        text = re.sub(r'[^ê°€-í£a-z0-9\s]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    
    def extract_keywords(self, text_series, min_length=2, top_n=50):
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        all_text = ' '.join(text_series.fillna('').apply(self.preprocess_text))
        words = all_text.split()
        words = [w for w in words if len(w) >= min_length]
        
        stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
                     'of', 'is', 'are', 'was', 'were', 'been', 'be', 'have', 'has', 'had',
                     'i', 'me', 'my', 'you', 'your', 'it', 'its', 'not', 'no', 'yes', 'we',
                     'ê·¸', 'ì´', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë“¤', 'ë°', 'ë˜í•œ', 'í•˜ë‹¤', 'ìˆë‹¤', 'ë˜ë‹¤',
                     'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ê·¸ëŸ°', 'ì´ëŸ°', 'ì €ëŸ°', 'removed', 'deleted'}
        
        words = [w for w in words if w not in stopwords]
        word_freq = Counter(words)
        
        return word_freq.most_common(top_n)
    
    
    def wordcloud(self, text_series, width=1200, height=800):
        """ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±"""
        all_text = ' '.join(text_series.fillna('').apply(self.preprocess_text))
        
        try:
             font_path = 'C:/Windows/Fonts/malgun.ttf' 
        except:
             font_path = None

        wordcloud = WordCloud(
            font_path=font_path,
            width=width,
            height=height,
            background_color='white',
            max_words=100,
            relative_scaling=0.3,
            colormap='viridis'
        ).generate(all_text)
        
        fig, ax = plt.subplots(figsize=(15, 10))
        ax.imshow(wordcloud, interpolation='bilinear')
        ax.axis('off')
        ax.set_title('Reddit í…ìŠ¤íŠ¸ ì›Œë“œí´ë¼ìš°ë“œ', fontsize=20, pad=20)
        plt.tight_layout()
        
        return fig
    
    
    def keyword_frequency(self, text_series, top_n=20):
        """í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„"""
        keywords = self.extract_keywords(text_series, top_n=top_n)
        
        if not keywords:
             return None, pd.DataFrame(columns=['í‚¤ì›Œë“œ', 'ë¹ˆë„'])

        words, counts = zip(*keywords)
        
        fig, ax = plt.subplots(figsize=(12, 8))
        ax.barh(range(len(words)), counts, color='orangered')
        ax.set_yticks(range(len(words)))
        ax.set_yticklabels(words)
        ax.set_xlabel('ë¹ˆë„', fontsize=12)
        ax.set_title(f'ìƒìœ„ {top_n}ê°œ í‚¤ì›Œë“œ ë¹ˆë„', fontsize=16, pad=20)
        ax.invert_yaxis()
        plt.tight_layout()
        
        freq_df = pd.DataFrame(keywords, columns=['í‚¤ì›Œë“œ', 'ë¹ˆë„'])
        
        return fig, freq_df
    
    
    def sentiment_analysis(self, text_series, data_df): # data_df ì¶”ê°€
        """ê°ì„± ë¶„ì„"""
        positive_words = {
            'good', 'great', 'excellent', 'amazing', 'wonderful', 'fantastic',
            'best', 'love', 'awesome', 'perfect', 'nice', 'happy', 'thank',
            'ì¢‹ë‹¤', 'ìµœê³ ', 'ëŒ€ë°•', 'ì˜ˆì˜ë‹¤', 'ë©‹ì§€ë‹¤', 'ì™„ë²½', 'ê°ì‚¬', 'í–‰ë³µ', 'ì¢‹ì•„ìš”'
        }
        
        negative_words = {
            'bad', 'worst', 'terrible', 'awful', 'horrible', 'hate',
            'poor', 'disappointing', 'useless', 'waste', 'crap',
            'ì‹«ë‹¤', 'ë³„ë¡œ', 'ë‚˜ì˜ë‹¤', 'ìµœì•…', 'í˜•í¸ì—†ë‹¤', 'ì‹¤ë§', 'ë³„ë¡œë„¤'
        }
        
        def calculate_sentiment(text):
            text = self.preprocess_text(text)
            words = text.split()
            
            pos_count = sum(1 for w in words if w in positive_words)
            neg_count = sum(1 for w in words if w in negative_words)
            
            if pos_count > neg_count:
                return 'ê¸ì •'
            elif pos_count < neg_count:
                return 'ë¶€ì •'
            else:
                return 'ì¤‘ë¦½'
        
        sentiments = text_series.fillna('').apply(calculate_sentiment)
        
        # ì›ë³¸ ë°ì´í„°í”„ë ˆì„ì˜ ë³µì‚¬ë³¸ì— ê°ì„± ê²°ê³¼ë¥¼ ì¶”ê°€
        df_copy = data_df.copy().reset_index(drop=True)
        df_copy['Sentiment'] = sentiments
        sentiment_counts = sentiments.value_counts()
        
        if sentiment_counts.empty:
            return None, pd.Series(), df_copy[['Sentiment']]

        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        
        colors = ['#90EE90', '#FFB6C1', '#D3D3D3']
        
        # ë°ì´í„° ìˆœì„œ ì •ë¦¬
        order = ['ê¸ì •', 'ë¶€ì •', 'ì¤‘ë¦½']
        ordered_counts = sentiment_counts.reindex(order, fill_value=0)
        ordered_counts = ordered_counts[ordered_counts > 0]
        ordered_colors = [c for s, c in zip(order, colors) if s in ordered_counts.index]

        axes[0].pie(ordered_counts.values, labels=ordered_counts.index, 
                    autopct='%1.1f%%', colors=ordered_colors, startangle=90)
        axes[0].set_title('ê°ì„± ë¶„í¬', fontsize=14, pad=20)
        
        axes[1].bar(ordered_counts.index, ordered_counts.values, color=ordered_colors)
        axes[1].set_xlabel('ê°ì„±', fontsize=12)
        axes[1].set_ylabel('ê°œìˆ˜', fontsize=12)
        axes[1].set_title('ê°ì„±ë³„ ê°œìˆ˜', fontsize=14, pad=20)
        
        plt.tight_layout()
        
        # ë³´ê³ ì„œ ìƒì„±ì„ ìœ„í•´ í•„ìš”í•œ í•µì‹¬ ì»¬ëŸ¼ë§Œ ë‹´ì€ DataFrame ë°˜í™˜
        sentiment_df = df_copy.rename(columns={'title': 'ì œëª©', 'selftext': 'ë³¸ë¬¸', 'body': 'ë³¸ë¬¸_ë˜ëŠ”_ë‚´ìš©', 'score': 'ì ìˆ˜'})
        
        if text_series.name == 'title':
            sentiment_df = sentiment_df[['Sentiment', 'ì œëª©', 'ì ìˆ˜']]
        elif text_series.name == 'selftext':
            sentiment_df = sentiment_df[['Sentiment', 'ë³¸ë¬¸', 'ì ìˆ˜']]
        else: # ëŒ“ê¸€ ë³¸ë¬¸
            sentiment_df = sentiment_df[['Sentiment', 'ë³¸ë¬¸_ë˜ëŠ”_ë‚´ìš©', 'ì ìˆ˜']]
            
        return fig, sentiment_counts, sentiment_df
    
    
    def time_trend(self, df, date_col='created_utc', interval='D'):
        """ì‹œê°„ëŒ€ë³„ íŠ¸ë Œë“œ ë¶„ì„"""
        if date_col not in df.columns:
            return None, None
        
        df_valid = df.dropna(subset=[date_col])
        if df_valid.empty:
            return None, None
        
        df_sorted = df_valid.reset_index(drop=True).set_index(date_col).sort_index()
        time_counts = df_sorted.resample(interval).size()
        
        if 'score' in df.columns:
            time_scores = df_sorted['score'].resample(interval).sum()
        else:
            time_scores = None
        
        fig = None
        if time_scores is not None:
            fig, axes = plt.subplots(2, 1, figsize=(14, 10))
            
            axes[0].plot(time_counts.index, time_counts.values, marker='o', linewidth=2, color='orangered')
            axes[0].set_xlabel('ë‚ ì§œ', fontsize=12)
            axes[0].set_ylabel('ê²Œì‹œë¬¼/ëŒ“ê¸€ ìˆ˜', fontsize=12)
            axes[0].set_title('ì‹œê°„ëŒ€ë³„ ê²Œì‹œë¬¼/ëŒ“ê¸€ ìˆ˜ ì¶”ì´', fontsize=14, pad=20)
            axes[0].grid(True, alpha=0.3)
            
            axes[1].plot(time_scores.index, time_scores.values, marker='o', 
                         color='coral', linewidth=2)
            axes[1].set_xlabel('ë‚ ì§œ', fontsize=12)
            axes[1].set_ylabel('ì ìˆ˜ í•©ê³„', fontsize=12)
            axes[1].set_title('ì‹œê°„ëŒ€ë³„ ì ìˆ˜ ì¶”ì´', fontsize=14, pad=20)
            axes[1].grid(True, alpha=0.3)
            
            trend_df = pd.DataFrame({
                'ë‚ ì§œ': time_counts.index,
                'ê°œìˆ˜': time_counts.values,
                'ì ìˆ˜': time_scores.values
            })
        else:
            fig, ax = plt.subplots(figsize=(14, 6))
            
            ax.plot(time_counts.index, time_counts.values, marker='o', linewidth=2, color='orangered')
            ax.set_xlabel('ë‚ ì§œ', fontsize=12)
            ax.set_ylabel('ê²Œì‹œë¬¼/ëŒ“ê¸€ ìˆ˜', fontsize=12)
            ax.set_title('ì‹œê°„ëŒ€ë³„ ê²Œì‹œë¬¼/ëŒ“ê¸€ ìˆ˜ ì¶”ì´', fontsize=14, pad=20)
            ax.grid(True, alpha=0.3)
            
            trend_df = pd.DataFrame({
                'ë‚ ì§œ': time_counts.index,
                'ê°œìˆ˜': time_counts.values
            })
        
        plt.tight_layout()
        
        return fig, trend_df
    
    
    def subreddit_comparison(self):
        """ì„œë¸Œë ˆë”§ë³„ ë¹„êµ ë¶„ì„"""
        if 'subreddit' not in self.posts_df.columns or self.posts_df['subreddit'].nunique() < 2:
            return None, None
        
        subreddit_stats = self.posts_df.groupby('subreddit').agg({
            'score': ['mean', 'sum', 'count'],
            'num_comments': 'mean'
        }).round(2)
        
        subreddit_stats.columns = ['í‰ê· _ì ìˆ˜', 'ì´_ì ìˆ˜', 'ê²Œì‹œë¬¼_ìˆ˜', 'í‰ê· _ëŒ“ê¸€ìˆ˜']
        subreddit_stats.index.name = 'ì„œë¸Œë ˆë”§' # ì¸ë±ìŠ¤ ì´ë¦„ ì„¤ì •
        subreddit_stats = subreddit_stats.sort_values('ì´_ì ìˆ˜', ascending=False)
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # ê²Œì‹œë¬¼ ìˆ˜
        axes[0, 0].bar(subreddit_stats.index, subreddit_stats['ê²Œì‹œë¬¼_ìˆ˜'], color='orangered')
        axes[0, 0].set_title('ì„œë¸Œë ˆë”§ë³„ ê²Œì‹œë¬¼ ìˆ˜', fontsize=14)
        axes[0, 0].set_ylabel('ê²Œì‹œë¬¼ ìˆ˜')
        axes[0, 0].tick_params(axis='x', rotation=45)
        
        # ì´ ì ìˆ˜
        axes[0, 1].bar(subreddit_stats.index, subreddit_stats['ì´_ì ìˆ˜'], color='coral')
        axes[0, 1].set_title('ì„œë¸Œë ˆë”§ë³„ ì´ ì ìˆ˜', fontsize=14)
        axes[0, 1].set_ylabel('ì´ ì ìˆ˜')
        axes[0, 1].tick_params(axis='x', rotation=45)
        
        # í‰ê·  ì ìˆ˜
        axes[1, 0].bar(subreddit_stats.index, subreddit_stats['í‰ê· _ì ìˆ˜'], color='tomato')
        axes[1, 0].set_title('ì„œë¸Œë ˆë”§ë³„ í‰ê·  ì ìˆ˜', fontsize=14)
        axes[1, 0].set_ylabel('í‰ê·  ì ìˆ˜')
        axes[1, 0].tick_params(axis='x', rotation=45)
        
        # í‰ê·  ëŒ“ê¸€ ìˆ˜
        axes[1, 1].bar(subreddit_stats.index, subreddit_stats['í‰ê· _ëŒ“ê¸€ìˆ˜'], color='lightsalmon')
        axes[1, 1].set_title('ì„œë¸Œë ˆë”§ë³„ í‰ê·  ëŒ“ê¸€ ìˆ˜', fontsize=14)
        axes[1, 1].set_ylabel('í‰ê·  ëŒ“ê¸€ ìˆ˜')
        axes[1, 1].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        
        return fig, subreddit_stats


def search_and_collect_reddit_data(subreddit_names, search_query, post_limit, sort_by, time_filter, collect_comments, comment_limit):
    """Reddit APIë¥¼ í†µí•œ ë°ì´í„° ìˆ˜ì§‘"""
    load_dotenv()
    
    client_id = os.getenv("REDDIT_CLIENT_ID")
    client_secret = os.getenv("REDDIT_CLIENT_SECRET")
    user_agent = os.getenv("REDDIT_USER_AGENT", "RedditAnalyzer/1.0")
    
    if not client_id or not client_secret:
        st.error("Reddit API ìê²©ì¦ëª…ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        st.info("í•„ìš”í•œ í™˜ê²½ ë³€ìˆ˜: REDDIT_CLIENT_ID, REDDIT_CLIENT_SECRET, REDDIT_USER_AGENT")
        return None, None
    
    try:
        reddit = praw.Reddit(
            client_id=client_id,
            client_secret=client_secret,
            user_agent=user_agent
        )
    except Exception as e:
        st.error(f"Reddit API ì—°ê²° ì˜¤ë¥˜: {e}")
        return None, None
    
    all_posts = []
    all_comments = []
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    subreddit_list = [s.strip() for s in subreddit_names.split(',') if s.strip()]
    total_subreddits = len(subreddit_list)
    
    if total_subreddits == 0:
        st.warning("ìˆ˜ì§‘í•  ì„œë¸Œë ˆë”§ ì´ë¦„ì´ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None, None

    for idx, subreddit_name in enumerate(subreddit_list):
        status_text.text(f"ì„œë¸Œë ˆë”§ {idx+1}/{total_subreddits}: r/{subreddit_name} ìˆ˜ì§‘ ì¤‘...")
        
        try:
            try:
                subreddit = reddit.subreddit(subreddit_name)
                _ = subreddit.title # ì„œë¸Œë ˆë”§ ì¡´ì¬ í™•ì¸
            except ResponseException as e:
                 st.warning(f"ì„œë¸Œë ˆë”§ r/{subreddit_name}ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìŠ¤í‚µí•©ë‹ˆë‹¤. ì˜¤ë¥˜: {e}")
                 progress_bar.progress((idx + 1) / total_subreddits)
                 continue

            # ê²Œì‹œë¬¼ ê²€ìƒ‰/ìˆ˜ì§‘
            if search_query:
                posts = subreddit.search(search_query, sort=sort_by, time_filter=time_filter, limit=post_limit)
            else:
                if sort_by == 'hot':
                    posts = subreddit.hot(limit=post_limit)
                elif sort_by == 'new':
                    posts = subreddit.new(limit=post_limit)
                elif sort_by == 'top':
                    posts = subreddit.top(time_filter=time_filter, limit=post_limit)
                elif sort_by == 'rising':
                    posts = subreddit.rising(limit=post_limit)
                else:
                    posts = subreddit.hot(limit=post_limit)
            
            for post in posts:
                post_data = {
                    'post_id': post.id,
                    'subreddit': post.subreddit.display_name if hasattr(post.subreddit, 'display_name') else subreddit_name,
                    'title': post.title,
                    'selftext': post.selftext,
                    'author': str(post.author) if post.author else '[deleted]',
                    'score': post.score,
                    'upvote_ratio': post.upvote_ratio,
                    'num_comments': post.num_comments,
                    'created_utc': post.created_utc,
                    'url': post.url,
                    'permalink': f"https://reddit.com{post.permalink}"
                }
                all_posts.append(post_data)
                
                # ëŒ“ê¸€ ìˆ˜ì§‘
                if collect_comments and post.num_comments > 0:
                    try:
                        post.comments.replace_more(limit=0)
                        comments = post.comments.list()[:comment_limit]
                        
                        for comment in comments:
                            if hasattr(comment, 'body') and comment.body is not None:
                                comment_data = {
                                    'comment_id': comment.id,
                                    'post_id': post.id,
                                    'subreddit': post.subreddit.display_name if hasattr(post.subreddit, 'display_name') else subreddit_name,
                                    'author': str(comment.author) if comment.author else '[deleted]',
                                    'body': comment.body,
                                    'score': comment.score,
                                    'created_utc': comment.created_utc,
                                    'post_title': post.title
                                }
                                all_comments.append(comment_data)
                    except Exception as e:
                        st.warning(f"ê²Œì‹œë¬¼ {post.id} ëŒ“ê¸€ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        
        except Exception as e:
            st.error(f"ì„œë¸Œë ˆë”§ r/{subreddit_name} ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        
        progress_bar.progress((idx + 1) / total_subreddits)
    
    progress_bar.empty()
    status_text.empty()
    
    posts_df = pd.DataFrame(all_posts)
    comments_df = pd.DataFrame(all_comments) if all_comments else None
    
    return posts_df, comments_df


# ========================================
# Streamlit ë©”ì¸ ì•±
# ========================================

def main():
    st.title("ğŸ”´ Reddit ë¶„ì„ ëŒ€ì‹œë³´ë“œ")
    st.markdown("---")
    
    # OpenAI API Key ë¡œë“œ
    load_dotenv()
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

    # ì‚¬ì´ë“œë°” - ë°ì´í„° ìˆ˜ì§‘/ì—…ë¡œë“œ
    st.sidebar.header("ğŸ“‚ ë°ì´í„° ì†ŒìŠ¤")
    data_source = st.sidebar.radio(
        "ë°ì´í„° ì…ë ¥ ë°©ì‹ ì„ íƒ",
        ["APIë¡œ ì‹¤ì‹œê°„ ìˆ˜ì§‘", "CSV íŒŒì¼ ì—…ë¡œë“œ"]
    )
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ë° ë¡œë“œ
    if 'posts_df' not in st.session_state:
         st.session_state['posts_df'] = pd.DataFrame(columns=['post_id', 'title', 'subreddit', 'score', 'num_comments'])
    if 'comments_df' not in st.session_state:
         st.session_state['comments_df'] = None 
    
    posts_df = st.session_state['posts_df']
    comments_df = st.session_state['comments_df']
    
    if data_source == "APIë¡œ ì‹¤ì‹œê°„ ìˆ˜ì§‘":
        st.sidebar.subheader("ğŸ” ê²€ìƒ‰ ì„¤ì •")
        subreddit_names = st.sidebar.text_input(
            "ì„œë¸Œë ˆë”§ (ì‰¼í‘œë¡œ êµ¬ë¶„)",
            value="kbeauty,AsianBeauty",
            help="ì˜ˆ: kbeauty,AsianBeauty,SkincareAddiction"
        )
        search_query = st.sidebar.text_input(
            "ê²€ìƒ‰ì–´ (ì„ íƒ)",
            value="",
            help="ë¹„ì›Œë‘ë©´ ì„œë¸Œë ˆë”§ì˜ 'sort_by'ì— í•´ë‹¹í•˜ëŠ” ê²Œì‹œë¬¼ ìˆ˜ì§‘"
        )
        post_limit = st.sidebar.slider("ê²Œì‹œë¬¼ ê°œìˆ˜", 10, 500, 100)
        
        sort_by = st.sidebar.selectbox(
            "ì •ë ¬ ë°©ì‹",
            ["hot", "new", "top", "rising"],
            format_func=lambda x: {
                "hot": "ì¸ê¸°ìˆœ",
                "new": "ìµœì‹ ìˆœ",
                "top": "ìµœê³  í‰ì ",
                "rising": "ê¸‰ìƒìŠ¹"
            }[x]
        )
        
        time_filter = st.sidebar.selectbox(
            "ê¸°ê°„ í•„í„° (top/searchë§Œ ì ìš©)",
            ["all", "day", "week", "month", "year"],
            format_func=lambda x: {
                "all": "ì „ì²´",
                "day": "ì˜¤ëŠ˜",
                "week": "ì´ë²ˆ ì£¼",
                "month": "ì´ë²ˆ ë‹¬",
                "year": "ì˜¬í•´"
            }[x]
        )
        
        collect_comments = st.sidebar.checkbox("ëŒ“ê¸€ë„ ìˆ˜ì§‘", value=True)
        comment_limit = st.sidebar.slider("ê²Œì‹œë¬¼ë‹¹ ëŒ“ê¸€ ìˆ˜", 10, 200, 50) if collect_comments else 0
        
        if st.sidebar.button("ğŸš€ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘"):
            with st.spinner("Reddit ë°ì´í„° ìˆ˜ì§‘ ì¤‘..."):
                posts_df_new, comments_df_new = search_and_collect_reddit_data(
                    subreddit_names, search_query, post_limit, 
                    sort_by, time_filter, collect_comments, comment_limit
                )
            
            if posts_df_new is not None and not posts_df_new.empty:
                st.success(f"âœ… ê²Œì‹œë¬¼ **{len(posts_df_new):,}**ê°œ ìˆ˜ì§‘ ì™„ë£Œ!")
                if comments_df_new is not None:
                    st.success(f"âœ… ëŒ“ê¸€ **{len(comments_df_new):,}**ê°œ ìˆ˜ì§‘ ì™„ë£Œ!")
                
                # ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸ì— ì €ì¥ (ìƒˆ ë°ì´í„°ë¡œ ë®ì–´ì“°ê¸°)
                st.session_state['posts_df'] = posts_df_new
                st.session_state['comments_df'] = comments_df_new
                st.rerun()
            else:
                st.warning("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    else:  # CSV íŒŒì¼ ì—…ë¡œë“œ
        st.sidebar.subheader("ğŸ“¤ íŒŒì¼ ì—…ë¡œë“œ")
        posts_file = st.sidebar.file_uploader("ê²Œì‹œë¬¼ CSV íŒŒì¼", type=['csv'], key="posts_upload")
        comments_file = st.sidebar.file_uploader("ëŒ“ê¸€ CSV íŒŒì¼ (ì„ íƒ)", type=['csv'], key="comments_upload")
        
        if posts_file:
            posts_df_loaded = pd.read_csv(posts_file)
            st.session_state['posts_df'] = posts_df_loaded
            st.sidebar.success(f"âœ… ê²Œì‹œë¬¼ **{len(posts_df_loaded):,}**ê°œ ë¡œë“œ")
        
        if comments_file:
            comments_df_loaded = pd.read_csv(comments_file)
            st.session_state['comments_df'] = comments_df_loaded
            st.sidebar.success(f"âœ… ëŒ“ê¸€ **{len(comments_df_loaded):,}**ê°œ ë¡œë“œ")
        
        # íŒŒì¼ ì—…ë¡œë“œ í›„ ë°ì´í„° ë¡œë“œë¥¼ ìœ„í•´ rerurn
        if posts_file or comments_file:
            st.rerun()

    
    posts_df = st.session_state['posts_df']
    comments_df = st.session_state['comments_df']

    # ë°ì´í„°ê°€ ì—†ì„ ê²½ìš° ë¶„ì„ì„ ì§„í–‰í•˜ì§€ ì•ŠìŒ
    if posts_df.empty:
        st.info("ğŸ‘† ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê±°ë‚˜ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        return 

    # ê¸°ë³¸ í†µê³„
    st.header("ğŸ“ˆ ê¸°ë³¸ í†µê³„")
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric("ì´ ê²Œì‹œë¬¼ ìˆ˜", f"{len(posts_df):,}")
    with col2:
        st.metric("í‰ê·  ì ìˆ˜", f"{posts_df['score'].mean():.1f}")
    with col3:
        st.metric("í‰ê·  ëŒ“ê¸€ ìˆ˜", f"{posts_df['num_comments'].mean():.1f}")
    with col4:
        if comments_df is not None:
            st.metric("ì´ ëŒ“ê¸€ ìˆ˜", f"{len(comments_df):,}")

    st.markdown("---")

    # íƒ­ìœ¼ë¡œ ë¶„ì„ ëª¨ë“œ êµ¬ë¶„
    tabs = st.tabs([
    "â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œ",
    "ğŸ“Š í‚¤ì›Œë“œ ë¹ˆë„",
    "ğŸ˜ŠğŸ˜¢ ê°ì„± ë¶„ì„",
    "ğŸ“ˆ ì‹œê°„ íŠ¸ë Œë“œ",
    "ğŸ¯ ì„œë¸Œë ˆë”§ ë¹„êµ",
    "ğŸ“‹ ì›ë³¸ ë°ì´í„°",
    "ğŸ“„ ë³´ê³ ì„œ ìƒì„±" # ìƒˆ íƒ­ ì¶”ê°€
    ])

    analyzer = RedditAnalyzer(posts_df, comments_df)
    
    # í…ìŠ¤íŠ¸ ë¶„ì„ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë°ì´í„°í”„ë ˆì„ í™•ì¸
    text_sources_available = ["ê²Œì‹œë¬¼ ì œëª©"]
    if 'selftext' in posts_df.columns and not posts_df['selftext'].isnull().all():
         text_sources_available.append("ê²Œì‹œë¬¼ ë³¸ë¬¸")
    if comments_df is not None and 'body' in comments_df.columns and not comments_df['body'].isnull().all():
         text_sources_available.append("ëŒ“ê¸€")


    # íƒ­ 1: ì›Œë“œí´ë¼ìš°ë“œ
    with tabs[0]:
        st.header("â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œ")
        
        if not text_sources_available:
             st.warning("í…ìŠ¤íŠ¸ ë°ì´í„°(ì œëª©, ë³¸ë¬¸, ëŒ“ê¸€)ê°€ ì—†ì–´ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            text_source = st.radio(
                "í…ìŠ¤íŠ¸ ì†ŒìŠ¤",
                text_sources_available,
                horizontal=True,
                key="wordcloud_source"
            )

            if st.button("ğŸ” ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±", key="btn_wordcloud"):
                with st.spinner(f"{text_source} ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì¤‘..."):
                    if text_source == "ê²Œì‹œë¬¼ ì œëª©":
                        fig = analyzer.wordcloud(posts_df['title'])
                    elif text_source == "ê²Œì‹œë¬¼ ë³¸ë¬¸":
                        fig = analyzer.wordcloud(posts_df['selftext'])
                    else:  # ëŒ“ê¸€
                        fig = analyzer.wordcloud(comments_df['body'])
                    st.pyplot(fig)
            else:
                st.info("ğŸ‘† í…ìŠ¤íŠ¸ ì†ŒìŠ¤ë¥¼ ì„ íƒí•˜ê³  ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")

    # íƒ­ 2: í‚¤ì›Œë“œ ë¹ˆë„
    with tabs[1]:
        st.header("ğŸ“Š í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„")
        
        if not text_sources_available:
             st.warning("í…ìŠ¤íŠ¸ ë°ì´í„°(ì œëª©, ë³¸ë¬¸, ëŒ“ê¸€)ê°€ ì—†ì–´ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            text_source = st.radio(
                "í…ìŠ¤íŠ¸ ì†ŒìŠ¤",
                text_sources_available,
                horizontal=True,
                key="keyword_source"
            )
            top_n = st.slider("í‘œì‹œí•  í‚¤ì›Œë“œ ê°œìˆ˜", 10, 50, 20, key="keyword_top_n")

            if st.button("ğŸ” í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„", key="btn_keyword"):
                with st.spinner(f"{text_source} í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„ ì¤‘..."):
                    if text_source == "ê²Œì‹œë¬¼ ì œëª©":
                        fig, freq_df = analyzer.keyword_frequency(posts_df['title'], top_n=top_n)
                    elif text_source == "ê²Œì‹œë¬¼ ë³¸ë¬¸":
                        fig, freq_df = analyzer.keyword_frequency(posts_df['selftext'], top_n=top_n)
                    else:
                        fig, freq_df = analyzer.keyword_frequency(comments_df['body'], top_n=top_n)
                    
                    if fig:
                        st.pyplot(fig)
                    
                        st.subheader("ğŸ“‹ í‚¤ì›Œë“œ ë°ì´í„°")
                        st.dataframe(freq_df, use_container_width=True)
                        
                        csv = freq_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
                        st.download_button(
                            "ğŸ’¾ CSV ë‹¤ìš´ë¡œë“œ",
                            csv,
                            "reddit_keyword_frequency.csv",
                            "text/csv",
                            key='download-keyword-csv'
                        )
                        st.session_state['freq_df_report'] = freq_df # ë³´ê³ ì„œ ìƒì„±ì„ ìœ„í•´ ì €ì¥
                    else:
                        st.warning("ë¶„ì„í•  ìœ íš¨í•œ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.info("ğŸ‘† í…ìŠ¤íŠ¸ ì†ŒìŠ¤ë¥¼ ì„ íƒí•˜ê³  ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")
                if 'freq_df_report' in st.session_state:
                     st.subheader("ğŸ“‹ ë§ˆì§€ë§‰ ë¶„ì„ ê²°ê³¼ (í‚¤ì›Œë“œ ë°ì´í„°)")
                     st.dataframe(st.session_state['freq_df_report'], use_container_width=True)


    # íƒ­ 3: ê°ì„± ë¶„ì„
    with tabs[2]:
        st.header("ğŸ˜ŠğŸ˜¢ ê°ì„± ë¶„ì„")
        
        if not text_sources_available:
             st.warning("í…ìŠ¤íŠ¸ ë°ì´í„°(ì œëª©, ë³¸ë¬¸, ëŒ“ê¸€)ê°€ ì—†ì–´ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            text_source = st.radio(
                "í…ìŠ¤íŠ¸ ì†ŒìŠ¤",
                text_sources_available,
                horizontal=True,
                key="sentiment_source"
            )

            if st.button("ğŸ” ê°ì„± ë¶„ì„ ì‹¤í–‰", key="btn_sentiment"):
                with st.spinner(f"{text_source} ê°ì„± ë¶„ì„ ì¤‘..."):
                    if text_source == "ê²Œì‹œë¬¼ ì œëª©":
                        fig, sentiment_counts, sentiment_df = analyzer.sentiment_analysis(posts_df['title'], posts_df)
                    elif text_source == "ê²Œì‹œë¬¼ ë³¸ë¬¸":
                        fig, sentiment_counts, sentiment_df = analyzer.sentiment_analysis(posts_df['selftext'], posts_df)
                    else:
                        fig, sentiment_counts, sentiment_df = analyzer.sentiment_analysis(comments_df['body'], comments_df)
                    
                    if fig:
                        st.pyplot(fig)
                        
                        st.subheader("ğŸ“Š ê°ì„± ìš”ì•½")
                        col1_s, col2_s, col3_s = st.columns(3)
                        
                        # ê¸ì •, ë¶€ì •, ì¤‘ë¦½ ìˆœì„œëŒ€ë¡œ í‘œì‹œ
                        for idx, sentiment in enumerate(['ê¸ì •', 'ë¶€ì •', 'ì¤‘ë¦½']):
                             count = sentiment_counts.get(sentiment, 0)
                             with [col1_s, col2_s, col3_s][idx]:
                                 st.metric(sentiment, f"{count:,}ê°œ")
                                 
                        st.subheader("ğŸ“‹ ê°ì„± ë¶„ë¥˜ ë°ì´í„° (ìƒìœ„ 100ê°œ)")
                        st.dataframe(sentiment_df.head(100), use_container_width=True)
                        
                        csv = sentiment_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
                        st.download_button(
                            "ğŸ’¾ CSV ë‹¤ìš´ë¡œë“œ",
                            csv,
                            "reddit_sentiment_analysis.csv",
                            "text/csv",
                            key='download-sentiment-csv'
                        )
                        st.session_state['sentiment_df_report'] = sentiment_df # ë³´ê³ ì„œ ìƒì„±ì„ ìœ„í•´ ì €ì¥
                    else:
                        st.warning("ê°ì„± ë¶„ì„ì„ ìˆ˜í–‰í•  í…ìŠ¤íŠ¸ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.info("ğŸ‘† í…ìŠ¤íŠ¸ ì†ŒìŠ¤ë¥¼ ì„ íƒí•˜ê³  ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")
                if 'sentiment_df_report' in st.session_state:
                     st.subheader("ğŸ“‹ ë§ˆì§€ë§‰ ë¶„ì„ ê²°ê³¼ (ê°ì„± ë¶„ë¥˜ ë°ì´í„° - ìƒìœ„ 100ê°œ)")
                     st.dataframe(st.session_state['sentiment_df_report'].head(100), use_container_width=True)


    # íƒ­ 4: ì‹œê°„ íŠ¸ë Œë“œ
    with tabs[3]:
        st.header("ğŸ“ˆ ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„")

        data_source_trend_options = ["ê²Œì‹œë¬¼"]
        if comments_df is not None:
             data_source_trend_options.append("ëŒ“ê¸€")

        data_source_trend = st.radio(
            "ë°ì´í„° ì†ŒìŠ¤",
            data_source_trend_options,
            horizontal=True
        )
        interval = st.radio("ì‹œê°„ ê°„ê²©", ["D (ì¼)", "W (ì£¼)", "M (ì›”)"], horizontal=True, key="time_interval")
        interval_code = interval.split()[0]

        if st.button("ğŸ” ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„", key="btn_time"):
            with st.spinner(f"{data_source_trend} ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„ ì¤‘..."):
                if data_source_trend == "ê²Œì‹œë¬¼":
                    fig, trend_df = analyzer.time_trend(posts_df, interval=interval_code)
                else:
                    fig, trend_df = analyzer.time_trend(comments_df, interval=interval_code)
                
                if fig:
                    st.pyplot(fig)
                    
                    st.subheader("ğŸ“‹ íŠ¸ë Œë“œ ë°ì´í„°")
                    st.dataframe(trend_df, use_container_width=True)
                    
                    csv = trend_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
                    st.download_button(
                        "ğŸ’¾ CSV ë‹¤ìš´ë¡œë“œ",
                        csv,
                        "reddit_time_trend.csv",
                        "text/csv",
                        key='download-trend-csv'
                    )
                    st.session_state['trend_df_report'] = trend_df # ë³´ê³ ì„œ ìƒì„±ì„ ìœ„í•´ ì €ì¥
                else:
                    st.warning("ë‚ ì§œ ì •ë³´ê°€ ì—†ê±°ë‚˜ ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ì–´ ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.info("ğŸ‘† ë°ì´í„° ì†ŒìŠ¤ì™€ ì‹œê°„ ê°„ê²©ì„ ì„ íƒí•˜ê³  ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")
            if 'trend_df_report' in st.session_state:
                 st.subheader("ğŸ“‹ ë§ˆì§€ë§‰ ë¶„ì„ ê²°ê³¼ (íŠ¸ë Œë“œ ë°ì´í„°)")
                 st.dataframe(st.session_state['trend_df_report'], use_container_width=True)


    # íƒ­ 5: ì„œë¸Œë ˆë”§ ë¹„êµ
    with tabs[4]:
        st.header("ğŸ¯ ì„œë¸Œë ˆë”§ ë¹„êµ ë¶„ì„")

        if st.button("ğŸ” ì„œë¸Œë ˆë”§ ë¹„êµ ë¶„ì„", key="btn_subreddit"):
            with st.spinner("ì„œë¸Œë ˆë”§ ë¹„êµ ë¶„ì„ ì¤‘..."):
                fig, comparison_df = analyzer.subreddit_comparison()
                if fig:
                    st.pyplot(fig)
                    
                    st.subheader("ğŸ“‹ ì„œë¸Œë ˆë”§ í†µê³„")
                    st.dataframe(comparison_df, use_container_width=True)
                    
                    csv = comparison_df.to_csv(encoding='utf-8-sig').encode('utf-8-sig')
                    st.download_button(
                        "ğŸ’¾ CSV ë‹¤ìš´ë¡œë“œ",
                        csv,
                        "reddit_subreddit_comparison.csv",
                        "text/csv",
                        key='download-subreddit-csv'
                    )
                    st.session_state['comparison_df_report'] = comparison_df # ë³´ê³ ì„œ ìƒì„±ì„ ìœ„í•´ ì €ì¥
                else:
                    st.warning("ì„œë¸Œë ˆë”§ ì •ë³´ê°€ ì—†ê±°ë‚˜ ë¹„êµí•  ì„œë¸Œë ˆë”§ì´ 2ê°œ ë¯¸ë§Œì…ë‹ˆë‹¤.")
        else:
            st.info("ğŸ‘† ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ì„œë¸Œë ˆë”§ë³„ í†µê³„ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            if 'comparison_df_report' in st.session_state:
                 st.subheader("ğŸ“‹ ë§ˆì§€ë§‰ ë¶„ì„ ê²°ê³¼ (ì„œë¸Œë ˆë”§ í†µê³„)")
                 st.dataframe(st.session_state['comparison_df_report'], use_container_width=True)


    # íƒ­ 6: ì›ë³¸ ë°ì´í„°
    with tabs[5]:
        st.header("ğŸ“‹ ì›ë³¸ ë°ì´í„°")

        data_type = st.radio(
            "ë°ì´í„° ìœ í˜• ì„ íƒ",
            ["ê²Œì‹œë¬¼ ë°ì´í„°", "ëŒ“ê¸€ ë°ì´í„°"] if comments_df is not None else ["ê²Œì‹œë¬¼ ë°ì´í„°"],
            horizontal=True
        )

        if data_type == "ê²Œì‹œë¬¼ ë°ì´í„°":
            st.subheader("ğŸ“ ê²Œì‹œë¬¼ ë°ì´í„°")
            
            # ì»¬ëŸ¼ ì„ íƒ
            display_columns = st.multiselect(
                "í‘œì‹œí•  ì»¬ëŸ¼ ì„ íƒ",
                posts_df.columns.tolist(),
                default=['title', 'subreddit', 'score', 'num_comments', 'author'][:min(5, len(posts_df.columns))],
                key='posts_cols_select'
            )
            
            if display_columns:
                st.dataframe(posts_df[display_columns], use_container_width=True, height=600)
            else:
                st.dataframe(posts_df, use_container_width=True, height=600)
            
            csv = posts_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
            st.download_button(
                "ğŸ’¾ ê²Œì‹œë¬¼ ë°ì´í„° CSV ë‹¤ìš´ë¡œë“œ",
                csv,
                "reddit_posts_data.csv",
                "text/csv",
                key='download-posts-raw'
            )

        else:
            if comments_df is not None and not comments_df.empty:
                st.subheader("ğŸ’¬ ëŒ“ê¸€ ë°ì´í„°")
                
                # ì»¬ëŸ¼ ì„ íƒ
                display_columns = st.multiselect(
                    "í‘œì‹œí•  ì»¬ëŸ¼ ì„ íƒ",
                    comments_df.columns.tolist(),
                    default=['body', 'subreddit', 'score', 'author', 'post_title'][:min(5, len(comments_df.columns))],
                    key='comments_cols_select'
                )
                
                if display_columns:
                    st.dataframe(comments_df[display_columns], use_container_width=True, height=600)
                else:
                    st.dataframe(comments_df, use_container_width=True, height=600)
                
                csv = comments_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
                st.download_button(
                    "ğŸ’¾ ëŒ“ê¸€ ë°ì´í„° CSV ë‹¤ìš´ë¡œë“œ",
                    csv,
                    "reddit_comments_data.csv",
                    "text/csv",
                    key='download-comments-raw'
                )
            else:
                st.warning("ëŒ“ê¸€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")


    # íƒ­ 7: AI ìë™ ë³´ê³ ì„œ ìƒì„± ì„¹ì…˜ (OpenAI API í˜¸ì¶œ)
    with tabs[6]:
        st.header("ğŸ“„ Market Insight Report Generator (OpenAI API ê¸°ë°˜)")
        st.write("ë¶„ì„ CSV íŒŒì¼ì— í¬í•¨ëœ í•µì‹¬ í‚¤ì›Œë“œì™€ í†µê³„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìš”ì•½ ë³´ê³ ì„œë¥¼ ìë™ ìƒì„±í•©ë‹ˆë‹¤. **(âš ï¸ ë¶„ì„ íƒ­ì—ì„œ CSVë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì—¬ `analysis_results` í´ë”ì— ì €ì¥í•´ì•¼ í•©ë‹ˆë‹¤)**")

        # SAVE_DIRì—ì„œ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ì‹¤ì œ í™˜ê²½ ê°€ì •)
        try:
            available_files = [f for f in os.listdir(SAVE_DIR) if f.endswith(".csv")]
        except FileNotFoundError:
            available_files = []

        if not available_files:
            st.warning("ë¶„ì„ ê²°ê³¼ CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë‹¤ë¥¸ ë¶„ì„ íƒ­ì—ì„œ ë¶„ì„ì„ ì‹¤í–‰í•˜ê³  CSVë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì—¬ `analysis_results` í´ë”ì— ì €ì¥í•˜ì„¸ìš”.")
        else:
            selected_files = st.multiselect("ğŸ“‚ ë³´ê³ ì„œì— í¬í•¨í•  íŒŒì¼ ì„ íƒ", available_files, default=available_files)

            if st.button("ğŸ§  ë³´ê³ ì„œ ìƒì„±"):
                report_sentences = [] 
                
                if not selected_files:
                    st.error("ë³´ê³ ì„œ ìƒì„±ì„ ìœ„í•´ íŒŒì¼ì„ 1ê°œ ì´ìƒ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.")
                    return
                
                if not OPENAI_API_KEY:
                    st.error("OpenAI API Keyê°€ .env íŒŒì¼ ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
                    return
                
                # ì„ì‹œ ë¶„ì„ê¸° ìƒì„± (í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ë¥¼ ìœ„í•´)
                temp_analyzer = RedditAnalyzer(posts_df, comments_df)

                with st.spinner("OpenAI GPT ëª¨ë¸ì´ ë³´ê³ ì„œë¥¼ ìƒì„± ì¤‘..."):
                    for f in selected_files:
                        file_path = os.path.join(SAVE_DIR, f)
                        keywords = f"File: {f}. "
                        
                        try:
                            df = pd.read_csv(file_path, encoding="utf-8-sig")

                            # --- í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„ íŒŒì¼ (reddit_keyword_frequency.csv) ---
                            if 'í‚¤ì›Œë“œ' in df.columns and 'ë¹ˆë„' in df.columns:
                                top_keyword = df.iloc[0]['í‚¤ì›Œë“œ']
                                top_count = df.iloc[0]['ë¹ˆë„']
                                keywords += f"Top keyword is '{top_keyword}' with count {top_count}. Total unique keywords: {len(df)}. "

                            # --- ê°ì„± ë¶„ì„ íŒŒì¼ (reddit_sentiment_analysis.csv) ---
                            elif 'Sentiment' in df.columns: # 'Sentiment'ëŠ” ê°ì„± ë¶„ì„ í•¨ìˆ˜ì—ì„œ ìƒì„±í•œ ì»¬ëŸ¼ ì´ë¦„
                                sentiment_counts = df['Sentiment'].value_counts()
                                pos = sentiment_counts.get('ê¸ì •', 0)
                                neg = sentiment_counts.get('ë¶€ì •', 0)
                                total = len(df)
                                pos_ratio = pos / total * 100 if total > 0 else 0
                                
                                # ê¸ì • ëŒ“ê¸€ ìƒ˜í”Œ (ìƒìœ„ 3ê°œ, ì ìˆ˜(ì ìˆ˜) ê¸°ì¤€)
                                pos_df = df[df['Sentiment'] == 'ê¸ì •']
                                
                                # ëŒ“ê¸€ ë˜ëŠ” ê²Œì‹œë¬¼ ë³¸ë¬¸(text/body/ë³¸ë¬¸_ë˜ëŠ”_ë‚´ìš©)ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ìƒ˜í”Œë§
                                if 'ë³¸ë¬¸_ë˜ëŠ”_ë‚´ìš©' in pos_df.columns:
                                    text_col = 'ë³¸ë¬¸_ë˜ëŠ”_ë‚´ìš©'
                                elif 'ë³¸ë¬¸' in pos_df.columns:
                                    text_col = 'ë³¸ë¬¸'
                                elif 'ì œëª©' in pos_df.columns:
                                     text_col = 'ì œëª©'
                                else:
                                    text_col = None
                                    
                                if text_col is not None and 'ì ìˆ˜' in pos_df.columns:
                                    positive_samples = pos_df.sort_values(by='ì ìˆ˜', ascending=False)[text_col].head(3).tolist()
                                    if positive_samples:
                                        clean_samples = [temp_analyzer.preprocess_text(s) for s in positive_samples]
                                        sample_text = "Sample positive content (Korean): " + " | ".join(clean_samples)
                                        keywords += sample_text + " "

                                keywords += f"Total comments/posts {total}. Positive ratio: {pos_ratio:.1f}%. Negative comments: {neg}. The overall sentiment is mostly Positive. "
                                
                            # --- ì‹œê°„ íŠ¸ë Œë“œ íŒŒì¼ (reddit_time_trend.csv) ---
                            elif 'ê°œìˆ˜' in df.columns and 'ë‚ ì§œ' in df.columns:
                                df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])
                                max_count_date = df.loc[df['ê°œìˆ˜'].idxmax(), 'ë‚ ì§œ'].strftime('%Y-%m-%d')
                                max_count = df['ê°œìˆ˜'].max()
                                keywords += f"Peak count {max_count} occurred on {max_count_date}. Average count per period is {df['ê°œìˆ˜'].mean():.1f}. "
                                
                            # --- ì„œë¸Œë ˆë”§ ë¹„êµ íŒŒì¼ (reddit_subreddit_comparison.csv) ---
                            elif 'ì´_ì ìˆ˜' in df.columns and 'ì„œë¸Œë ˆë”§' in df.columns:
                                top_subreddit = df.loc[df['ì´_ì ìˆ˜'].idxmax(), 'ì„œë¸Œë ˆë”§']
                                top_score = df['ì´_ì ìˆ˜'].max()
                                avg_comments = df['í‰ê· _ëŒ“ê¸€ìˆ˜'].mean()
                                keywords += f"Top subreddit by total score is '{top_subreddit}' with score {top_score}. Average comments per post across all subreddits: {avg_comments:.1f}. "

                            # --- ì›ë³¸ ê²Œì‹œë¬¼/ëŒ“ê¸€ ë°ì´í„° (posts/comments_data.csv) ---
                            elif ('title' in df.columns and 'score' in df.columns) or ('body' in df.columns and 'score' in df.columns):
                                total_records = len(df)
                                avg_score = df['score'].mean()
                                
                                # ìƒìœ„ ì ìˆ˜ ëŒ“ê¸€/ê²Œì‹œë¬¼ ìƒ˜í”Œë§
                                top_content_col = 'title' if 'title' in df.columns else 'body'
                                top_content = df.sort_values(by='score', ascending=False)[top_content_col].head(3).tolist()
                                clean_samples = [temp_analyzer.preprocess_text(str(s)) for s in top_content]
                                top_content_text = " | ".join(clean_samples)
                                
                                keywords += f"Raw data summary. Total records: {total_records}. Average score: {avg_score:.1f}. Top content by score: {top_content_text}. "
                            
                            else:
                                keywords += f"Dataset rows: {len(df)}. Columns: {', '.join(df.columns)}. Data statistics available. "
                            
                            
                            # ë¬¸ì¥ ìƒì„± (OpenAI API í˜¸ì¶œ)
                            sentence = generate_openai_report(keywords, OPENAI_API_KEY) 
                            report_sentences.append(f"**{f} Insight:** {sentence}")

                        except Exception as e:
                            st.error(f"íŒŒì¼ {f} ì²˜ë¦¬ ì˜¤ë¥˜: CSV íŒŒì¼ êµ¬ì¡° í™•ì¸ í•„ìš”. {str(e)}")
                            continue

                if report_sentences:
                    summary = "\n\n".join(report_sentences)
                    
                    final_report = f"""
# Reddit Analysis Auto-Generated Report
## Generated At: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

{summary}
"""
                    st.subheader("ğŸ“ˆ AI ìë™ ìƒì„± ë³´ê³ ì„œ ì´ˆì•ˆ")
                    st.text_area("ìš”ì•½ ê²°ê³¼", final_report, height=400)
                    
                    st.download_button(
                        "ğŸ’¾ ìš”ì•½ ë³´ê³ ì„œ ì €ì¥",
                        final_report.encode("utf-8-sig"),
                        "Market_Insight_Report_Reddit.txt",
                        "text/plain"
                    )
                else:
                    st.error("ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨. íŒŒì¼ ì„ íƒ ë° êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")


if __name__ == "__main__":
    main()